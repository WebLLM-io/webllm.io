[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.1","content-config-digest","28e7bc87492477d9","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://webllm.io/docs\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"where\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4322,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"css-variables\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[null,[null,{\"themes\":[{\"name\":\"Night Owl No Italics\",\"type\":\"dark\",\"colors\":{\"focusBorder\":\"#122d42\",\"foreground\":\"#d6deeb\",\"disabledForeground\":\"#cccccc80\",\"descriptionForeground\":\"#d6deebb3\",\"errorForeground\":\"#ef5350\",\"icon.foreground\":\"#c5c5c5\",\"contrastActiveBorder\":null,\"contrastBorder\":\"#122d42\",\"textBlockQuote.background\":\"#7f7f7f1a\",\"textBlockQuote.border\":\"#007acc80\",\"textCodeBlock.background\":\"#4f4f4f\",\"textLink.activeForeground\":\"#3794ff\",\"textLink.foreground\":\"#3794ff\",\"textPreformat.foreground\":\"#d7ba7d\",\"textSeparator.foreground\":\"#ffffff2e\",\"editor.background\":\"#23262f\",\"editor.foreground\":\"#d6deeb\",\"editorLineNumber.foreground\":\"#4b6479\",\"editorLineNumber.activeForeground\":\"#c5e4fd\",\"editorActiveLineNumber.foreground\":\"#c6c6c6\",\"editor.selectionBackground\":\"#1d3b53\",\"editor.inactiveSelectionBackground\":\"#7e57c25a\",\"editor.selectionHighlightBackground\":\"#5f7e9779\",\"editorError.foreground\":\"#ef5350\",\"editorWarning.foreground\":\"#b39554\",\"editorInfo.foreground\":\"#3794ff\",\"editorHint.foreground\":\"#eeeeeeb2\",\"problemsErrorIcon.foreground\":\"#ef5350\",\"problemsWarningIcon.foreground\":\"#b39554\",\"problemsInfoIcon.foreground\":\"#3794ff\",\"editor.findMatchBackground\":\"#5f7e9779\",\"editor.findMatchHighlightBackground\":\"#1085bb5d\",\"editor.findRangeHighlightBackground\":\"#3a3d4166\",\"editorLink.activeForeground\":\"#4e94ce\",\"editorLightBulb.foreground\":\"#ffcc00\",\"editorLightBulbAutoFix.foreground\":\"#75beff\",\"diffEditor.insertedTextBackground\":\"#99b76d23\",\"diffEditor.insertedTextBorder\":\"#c5e47833\",\"diffEditor.removedTextBackground\":\"#ef535033\",\"diffEditor.removedTextBorder\":\"#ef53504d\",\"diffEditor.insertedLineBackground\":\"#9bb95533\",\"diffEditor.removedLineBackground\":\"#ff000033\",\"editorStickyScroll.background\":\"#011627\",\"editorStickyScrollHover.background\":\"#2a2d2e\",\"editorInlayHint.background\":\"#5f7e97cc\",\"editorInlayHint.foreground\":\"#ffffff\",\"editorInlayHint.typeBackground\":\"#5f7e97cc\",\"editorInlayHint.typeForeground\":\"#ffffff\",\"editorInlayHint.parameterBackground\":\"#5f7e97cc\",\"editorInlayHint.parameterForeground\":\"#ffffff\",\"editorPane.background\":\"#011627\",\"editorGroup.emptyBackground\":\"#011627\",\"editorGroup.focusedEmptyBorder\":null,\"editorGroupHeader.tabsBackground\":\"var(--sl-color-black)\",\"editorGroupHeader.tabsBorder\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"editorGroupHeader.noTabsBackground\":\"#011627\",\"editorGroupHeader.border\":null,\"editorGroup.border\":\"#011627\",\"editorGroup.dropBackground\":\"#7e57c273\",\"editorGroup.dropIntoPromptForeground\":\"#d6deeb\",\"editorGroup.dropIntoPromptBackground\":\"#021320\",\"editorGroup.dropIntoPromptBorder\":null,\"sideBySideEditor.horizontalBorder\":\"#011627\",\"sideBySideEditor.verticalBorder\":\"#011627\",\"scrollbar.shadow\":\"#010b14\",\"scrollbarSlider.background\":\"#ffffff17\",\"scrollbarSlider.hoverBackground\":\"#ffffff40\",\"scrollbarSlider.activeBackground\":\"#084d8180\",\"panel.background\":\"#011627\",\"panel.border\":\"#5f7e97\",\"panelTitle.activeBorder\":\"#5f7e97\",\"panelTitle.activeForeground\":\"#ffffffcc\",\"panelTitle.inactiveForeground\":\"#d6deeb80\",\"panelSectionHeader.background\":\"#80808051\",\"terminal.background\":\"#011627\",\"widget.shadow\":\"#011627\",\"editorWidget.background\":\"#021320\",\"editorWidget.foreground\":\"#d6deeb\",\"editorWidget.border\":\"#5f7e97\",\"quickInput.background\":\"#021320\",\"quickInput.foreground\":\"#d6deeb\",\"quickInputTitle.background\":\"#ffffff1a\",\"pickerGroup.foreground\":\"#d1aaff\",\"pickerGroup.border\":\"#011627\",\"editor.hoverHighlightBackground\":\"#7e57c25a\",\"editorHoverWidget.background\":\"#011627\",\"editorHoverWidget.foreground\":\"#d6deeb\",\"editorHoverWidget.border\":\"#5f7e97\",\"editorHoverWidget.statusBarBackground\":\"#011a2f\",\"titleBar.activeBackground\":\"var(--sl-color-black)\",\"titleBar.activeForeground\":\"var(--sl-color-text)\",\"titleBar.inactiveBackground\":\"#010e1a\",\"titleBar.inactiveForeground\":\"#eeefff99\",\"titleBar.border\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"toolbar.hoverBackground\":\"#5a5d5e50\",\"toolbar.activeBackground\":\"#63666750\",\"tab.activeBackground\":\"#0b2942\",\"tab.unfocusedActiveBackground\":\"#0b2942\",\"tab.inactiveBackground\":\"#01111d\",\"tab.unfocusedInactiveBackground\":\"#01111d\",\"tab.activeForeground\":\"var(--sl-color-text)\",\"tab.inactiveForeground\":\"#5f7e97\",\"tab.unfocusedActiveForeground\":\"#5f7e97\",\"tab.unfocusedInactiveForeground\":\"#5f7e97\",\"tab.hoverBackground\":null,\"tab.unfocusedHoverBackground\":null,\"tab.hoverForeground\":null,\"tab.unfocusedHoverForeground\":null,\"tab.border\":\"#272b3b\",\"tab.lastPinnedBorder\":\"#585858\",\"tab.activeBorder\":\"transparent\",\"tab.unfocusedActiveBorder\":\"#262a39\",\"tab.activeBorderTop\":\"var(--sl-color-accent-high)\",\"tab.unfocusedActiveBorderTop\":null,\"tab.hoverBorder\":null,\"tab.unfocusedHoverBorder\":null,\"tab.activeModifiedBorder\":\"#3399cc\",\"tab.inactiveModifiedBorder\":\"#3399cc80\",\"tab.unfocusedActiveModifiedBorder\":\"#3399cc80\",\"tab.unfocusedInactiveModifiedBorder\":\"#3399cc40\",\"badge.background\":\"#5f7e97\",\"badge.foreground\":\"#ffffff\",\"button.background\":\"#7e57c2cc\",\"button.foreground\":\"#ffffffcc\",\"button.border\":\"#122d42\",\"button.separator\":\"#ffffff52\",\"button.hoverBackground\":\"#7e57c2\",\"button.secondaryBackground\":\"#3a3d41\",\"button.secondaryForeground\":\"#ffffff\",\"button.secondaryHoverBackground\":\"#46494e\",\"dropdown.background\":\"#011627\",\"dropdown.foreground\":\"#ffffffcc\",\"dropdown.border\":\"#5f7e97\",\"list.activeSelectionBackground\":\"#234d708c\",\"list.activeSelectionForeground\":\"#ffffff\",\"tree.indentGuidesStroke\":\"#585858\",\"input.background\":\"#0b253a\",\"input.foreground\":\"#ffffffcc\",\"input.placeholderForeground\":\"#5f7e97\",\"inputOption.activeBorder\":\"#ffffffcc\",\"inputOption.hoverBackground\":\"#5a5d5e80\",\"inputOption.activeBackground\":\"#122d4266\",\"inputOption.activeForeground\":\"#ffffff\",\"inputValidation.infoBackground\":\"#00589ef2\",\"inputValidation.infoBorder\":\"#64b5f6\",\"inputValidation.warningBackground\":\"#675700f2\",\"inputValidation.warningBorder\":\"#ffca28\",\"inputValidation.errorBackground\":\"#ab0300f2\",\"inputValidation.errorBorder\":\"#ef5350\",\"keybindingLabel.background\":\"#8080802b\",\"keybindingLabel.foreground\":\"#cccccc\",\"keybindingLabel.border\":\"#33333399\",\"keybindingLabel.bottomBorder\":\"#44444499\",\"menu.foreground\":\"#ffffffcc\",\"menu.background\":\"#011627\",\"menu.selectionForeground\":\"#ffffff\",\"menu.selectionBackground\":\"#234d708c\",\"menu.separatorBackground\":\"#606060\",\"editor.snippetTabstopHighlightBackground\":\"#7c7c74c\",\"editor.snippetFinalTabstopHighlightBorder\":\"#525252\",\"terminal.ansiBlack\":\"#011627\",\"terminal.ansiRed\":\"#ef5350\",\"terminal.ansiGreen\":\"#22da6e\",\"terminal.ansiYellow\":\"#c5e478\",\"terminal.ansiBlue\":\"#82aaff\",\"terminal.ansiMagenta\":\"#c792ea\",\"terminal.ansiCyan\":\"#21c7a8\",\"terminal.ansiWhite\":\"#ffffff\",\"terminal.ansiBrightBlack\":\"#575656\",\"terminal.ansiBrightRed\":\"#ef5350\",\"terminal.ansiBrightGreen\":\"#22da6e\",\"terminal.ansiBrightYellow\":\"#ffeb95\",\"terminal.ansiBrightBlue\":\"#82aaff\",\"terminal.ansiBrightMagenta\":\"#c792ea\",\"terminal.ansiBrightCyan\":\"#7fdbca\",\"terminal.ansiBrightWhite\":\"#ffffff\",\"selection.background\":\"#4373c2\",\"input.border\":\"#5f7e97\",\"punctuation.definition.generic.begin.html\":\"#ef5350f2\",\"progress.background\":\"#7e57c2\",\"breadcrumb.foreground\":\"#a599e9\",\"breadcrumb.focusForeground\":\"#ffffff\",\"breadcrumb.activeSelectionForeground\":\"#ffffff\",\"breadcrumbPicker.background\":\"#001122\",\"list.invalidItemForeground\":\"#975f94\",\"list.dropBackground\":\"#011627\",\"list.focusBackground\":\"#010d18\",\"list.focusForeground\":\"#ffffff\",\"list.highlightForeground\":\"#ffffff\",\"list.hoverBackground\":\"#011627\",\"list.hoverForeground\":\"#ffffff\",\"list.inactiveSelectionBackground\":\"#0e293f\",\"list.inactiveSelectionForeground\":\"#5f7e97\",\"activityBar.background\":\"#011627\",\"activityBar.dropBackground\":\"#5f7e97\",\"activityBar.foreground\":\"#5f7e97\",\"activityBar.border\":\"#011627\",\"activityBarBadge.background\":\"#44596b\",\"activityBarBadge.foreground\":\"#ffffff\",\"sideBar.background\":\"#011627\",\"sideBar.foreground\":\"#89a4bb\",\"sideBar.border\":\"#011627\",\"sideBarTitle.foreground\":\"#5f7e97\",\"sideBarSectionHeader.background\":\"#011627\",\"sideBarSectionHeader.foreground\":\"#5f7e97\",\"editorCursor.foreground\":\"#80a4c2\",\"editor.wordHighlightBackground\":\"#f6bbe533\",\"editor.wordHighlightStrongBackground\":\"#e2a2f433\",\"editor.lineHighlightBackground\":\"#0003\",\"editor.rangeHighlightBackground\":\"#7e57c25a\",\"editorIndentGuide.background\":\"#5e81ce52\",\"editorIndentGuide.activeBackground\":\"#7e97ac\",\"editorRuler.foreground\":\"#5e81ce52\",\"editorCodeLens.foreground\":\"#5e82ceb4\",\"editorBracketMatch.background\":\"#5f7e974d\",\"editorOverviewRuler.currentContentForeground\":\"#7e57c2\",\"editorOverviewRuler.incomingContentForeground\":\"#7e57c2\",\"editorOverviewRuler.commonContentForeground\":\"#7e57c2\",\"editorGutter.background\":\"#011627\",\"editorGutter.modifiedBackground\":\"#e2b93d\",\"editorGutter.addedBackground\":\"#9ccc65\",\"editorGutter.deletedBackground\":\"#ef5350\",\"editorSuggestWidget.background\":\"#2c3043\",\"editorSuggestWidget.border\":\"#2b2f40\",\"editorSuggestWidget.foreground\":\"#d6deeb\",\"editorSuggestWidget.highlightForeground\":\"#ffffff\",\"editorSuggestWidget.selectedBackground\":\"#5f7e97\",\"debugExceptionWidget.background\":\"#011627\",\"debugExceptionWidget.border\":\"#5f7e97\",\"editorMarkerNavigation.background\":\"#0b2942\",\"editorMarkerNavigationError.background\":\"#ef5350\",\"editorMarkerNavigationWarning.background\":\"#ffca28\",\"peekView.border\":\"#5f7e97\",\"peekViewEditor.background\":\"#011627\",\"peekViewEditor.matchHighlightBackground\":\"#7e57c25a\",\"peekViewResult.background\":\"#011627\",\"peekViewResult.fileForeground\":\"#5f7e97\",\"peekViewResult.lineForeground\":\"#5f7e97\",\"peekViewResult.matchHighlightBackground\":\"#ffffffcc\",\"peekViewResult.selectionBackground\":\"#2e3250\",\"peekViewResult.selectionForeground\":\"#5f7e97\",\"peekViewTitle.background\":\"#011627\",\"peekViewTitleDescription.foreground\":\"#697098\",\"peekViewTitleLabel.foreground\":\"#5f7e97\",\"merge.currentHeaderBackground\":\"#5f7e97\",\"merge.incomingHeaderBackground\":\"#7e57c25a\",\"statusBar.background\":\"#011627\",\"statusBar.foreground\":\"#5f7e97\",\"statusBar.border\":\"#262a39\",\"statusBar.debuggingBackground\":\"#202431\",\"statusBar.debuggingBorder\":\"#1f2330\",\"statusBar.noFolderBackground\":\"#011627\",\"statusBar.noFolderBorder\":\"#25293a\",\"statusBarItem.activeBackground\":\"#202431\",\"statusBarItem.hoverBackground\":\"#202431\",\"statusBarItem.prominentBackground\":\"#202431\",\"statusBarItem.prominentHoverBackground\":\"#202431\",\"notifications.background\":\"#01111d\",\"notifications.border\":\"#262a39\",\"notificationCenter.border\":\"#262a39\",\"notificationToast.border\":\"#262a39\",\"notifications.foreground\":\"#ffffffcc\",\"notificationLink.foreground\":\"#80cbc4\",\"extensionButton.prominentForeground\":\"#ffffffcc\",\"extensionButton.prominentBackground\":\"#7e57c2cc\",\"extensionButton.prominentHoverBackground\":\"#7e57c2\",\"terminal.selectionBackground\":\"#1b90dd4d\",\"terminalCursor.background\":\"#234d70\",\"debugToolBar.background\":\"#011627\",\"welcomePage.buttonBackground\":\"#011627\",\"welcomePage.buttonHoverBackground\":\"#011627\",\"walkThrough.embeddedEditorBackground\":\"#011627\",\"gitDecoration.modifiedResourceForeground\":\"#a2bffc\",\"gitDecoration.deletedResourceForeground\":\"#ef535090\",\"gitDecoration.untrackedResourceForeground\":\"#c5e478ff\",\"gitDecoration.ignoredResourceForeground\":\"#395a75\",\"gitDecoration.conflictingResourceForeground\":\"#ffeb95cc\",\"source.elm\":\"#5f7e97\",\"string.quoted.single.js\":\"#ffffff\",\"meta.objectliteral.js\":\"#82aaff\"},\"fg\":\"#d6deeb\",\"bg\":\"#23262f\",\"semanticHighlighting\":false,\"settings\":[{\"name\":\"Changed\",\"scope\":[\"markup.changed\",\"meta.diff.header.git\",\"meta.diff.header.from-file\",\"meta.diff.header.to-file\"],\"settings\":{\"foreground\":\"#a2bffc\"}},{\"name\":\"Deleted\",\"scope\":[\"markup.deleted.diff\"],\"settings\":{\"foreground\":\"#f27775fe\"}},{\"name\":\"Inserted\",\"scope\":[\"markup.inserted.diff\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Global settings\",\"settings\":{\"background\":\"#011627\",\"foreground\":\"#d6deeb\"}},{\"name\":\"Comment\",\"scope\":[\"comment\"],\"settings\":{\"foreground\":\"#919f9f\",\"fontStyle\":\"\"}},{\"name\":\"String\",\"scope\":[\"string\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"String Quoted\",\"scope\":[\"string.quoted\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"Support Constant Math\",\"scope\":[\"support.constant.math\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Number\",\"scope\":[\"constant.numeric\",\"constant.character.numeric\"],\"settings\":{\"foreground\":\"#f78c6c\",\"fontStyle\":\"\"}},{\"name\":\"Built-in constant\",\"scope\":[\"constant.language\",\"punctuation.definition.constant\",\"variable.other.constant\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"User-defined constant\",\"scope\":[\"constant.character\",\"constant.other\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Constant Character Escape\",\"scope\":[\"constant.character.escape\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"RegExp String\",\"scope\":[\"string.regexp\",\"string.regexp keyword.other\"],\"settings\":{\"foreground\":\"#5ca7e4\"}},{\"name\":\"Comma in functions\",\"scope\":[\"meta.function punctuation.separator.comma\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"Variable\",\"scope\":[\"variable\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Keyword\",\"scope\":[\"punctuation.accessor\",\"keyword\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Storage\",\"scope\":[\"storage\",\"meta.var.expr\",\"meta.class meta.method.declaration meta.var.expr storage.type.js\",\"storage.type.property.js\",\"storage.type.property.ts\",\"storage.type.property.tsx\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type.function.arrow.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Class name\",\"scope\":[\"entity.name.class\",\"meta.class entity.name.type.class\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Inherited class\",\"scope\":[\"entity.other.inherited-class\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Function name\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Meta Tag\",\"scope\":[\"punctuation.definition.tag\",\"meta.tag\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"HTML Tag names\",\"scope\":[\"entity.name.tag\",\"meta.tag.other.html\",\"meta.tag.other.js\",\"meta.tag.other.tsx\",\"entity.name.tag.tsx\",\"entity.name.tag.js\",\"entity.name.tag\",\"meta.tag.js\",\"meta.tag.tsx\",\"meta.tag.html\"],\"settings\":{\"foreground\":\"#caece6\",\"fontStyle\":\"\"}},{\"name\":\"Tag attribute\",\"scope\":[\"entity.other.attribute-name\"],\"settings\":{\"fontStyle\":\"\",\"foreground\":\"#c5e478\"}},{\"name\":\"Entity Name Tag Custom\",\"scope\":[\"entity.name.tag.custom\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Library (function & constant)\",\"scope\":[\"support.function\",\"support.constant\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Support Constant Property Value meta\",\"scope\":[\"support.constant.meta.property-value\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Library class/type\",\"scope\":[\"support.type\",\"support.class\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Support Variable DOM\",\"scope\":[\"support.variable.dom\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Invalid\",\"scope\":[\"invalid\"],\"settings\":{\"background\":\"#ff2c83\",\"foreground\":\"#ffffff\"}},{\"name\":\"Invalid deprecated\",\"scope\":[\"invalid.deprecated\"],\"settings\":{\"foreground\":\"#ffffff\",\"background\":\"#d3423e\"}},{\"name\":\"Keyword Operator\",\"scope\":[\"keyword.operator\"],\"settings\":{\"foreground\":\"#7fdbca\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Relational\",\"scope\":[\"keyword.operator.relational\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Assignment\",\"scope\":[\"keyword.operator.assignment\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Arithmetic\",\"scope\":[\"keyword.operator.arithmetic\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Bitwise\",\"scope\":[\"keyword.operator.bitwise\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Increment\",\"scope\":[\"keyword.operator.increment\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Ternary\",\"scope\":[\"keyword.operator.ternary\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Double-Slashed Comment\",\"scope\":[\"comment.line.double-slash\"],\"settings\":{\"foreground\":\"#919f9f\"}},{\"name\":\"Object\",\"scope\":[\"object\"],\"settings\":{\"foreground\":\"#cdebf7\"}},{\"name\":\"Null\",\"scope\":[\"constant.language.null\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Meta Brace\",\"scope\":[\"meta.brace\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Meta Delimiter Period\",\"scope\":[\"meta.delimiter.period\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Punctuation Definition String\",\"scope\":[\"punctuation.definition.string\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Punctuation Definition String Markdown\",\"scope\":[\"punctuation.definition.string.begin.markdown\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Boolean\",\"scope\":[\"constant.language.boolean\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Object Comma\",\"scope\":[\"object.comma\"],\"settings\":{\"foreground\":\"#ffffff\"}},{\"name\":\"Variable Parameter Function\",\"scope\":[\"variable.parameter.function\"],\"settings\":{\"foreground\":\"#7fdbca\",\"fontStyle\":\"\"}},{\"name\":\"Support Type Property Name & entity name tags\",\"scope\":[\"support.type.vendor.property-name\",\"support.constant.vendor.property-value\",\"support.type.property-name\",\"meta.property-list entity.name.tag\"],\"settings\":{\"foreground\":\"#80cbc4\",\"fontStyle\":\"\"}},{\"name\":\"Entity Name tag reference in stylesheets\",\"scope\":[\"meta.property-list entity.name.tag.reference\"],\"settings\":{\"foreground\":\"#57eaf1\"}},{\"name\":\"Constant Other Color RGB Value Punctuation Definition Constant\",\"scope\":[\"constant.other.color.rgb-value punctuation.definition.constant\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Constant Other Color\",\"scope\":[\"constant.other.color\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Keyword Other Unit\",\"scope\":[\"keyword.other.unit\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Meta Selector\",\"scope\":[\"meta.selector\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Entity Other Attribute Name Id\",\"scope\":[\"entity.other.attribute-name.id\"],\"settings\":{\"foreground\":\"#fad430\"}},{\"name\":\"Meta Property Name\",\"scope\":[\"meta.property-name\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Doctypes\",\"scope\":[\"entity.name.tag.doctype\",\"meta.tag.sgml.doctype\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Punctuation Definition Parameters\",\"scope\":[\"punctuation.definition.parameters\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Keyword Control Operator\",\"scope\":[\"keyword.control.operator\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Keyword Operator Logical\",\"scope\":[\"keyword.operator.logical\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Variable Instances\",\"scope\":[\"variable.instance\",\"variable.other.instance\",\"variable.readwrite.instance\",\"variable.other.readwrite.instance\",\"variable.other.property\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Variable Property Other object property\",\"scope\":[\"variable.other.object.property\"],\"settings\":{\"foreground\":\"#faf39f\",\"fontStyle\":\"\"}},{\"name\":\"Variable Property Other object\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Entity Name Function\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#82aaff\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Comparison, returns, imports, and Keyword Operator Ruby\",\"scope\":[\"keyword.control.conditional.js\",\"keyword.operator.comparison\",\"keyword.control.flow.js\",\"keyword.control.flow.ts\",\"keyword.control.flow.tsx\",\"keyword.control.ruby\",\"keyword.control.def.ruby\",\"keyword.control.loop.js\",\"keyword.control.loop.ts\",\"keyword.control.import.js\",\"keyword.control.import.ts\",\"keyword.control.import.tsx\",\"keyword.control.from.js\",\"keyword.control.from.ts\",\"keyword.control.from.tsx\",\"keyword.control.conditional.js\",\"keyword.control.conditional.ts\",\"keyword.control.switch.js\",\"keyword.control.switch.ts\",\"keyword.operator.instanceof.js\",\"keyword.operator.expression.instanceof.ts\",\"keyword.operator.expression.instanceof.tsx\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Support Constant, `new` keyword, Special Method Keyword, `debugger`, other keywords\",\"scope\":[\"support.constant\",\"keyword.other.special-method\",\"keyword.other.new\",\"keyword.other.debugger\",\"keyword.control\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Support Function\",\"scope\":[\"support.function\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Invalid Broken\",\"scope\":[\"invalid.broken\"],\"settings\":{\"foreground\":\"#989da0\",\"background\":\"#F78C6C\"}},{\"name\":\"Invalid Unimplemented\",\"scope\":[\"invalid.unimplemented\"],\"settings\":{\"background\":\"#8BD649\",\"foreground\":\"#ffffff\"}},{\"name\":\"Invalid Illegal\",\"scope\":[\"invalid.illegal\"],\"settings\":{\"foreground\":\"#ffffff\",\"background\":\"#ec5f67\"}},{\"name\":\"Language Variable\",\"scope\":[\"variable.language\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Support Variable Property\",\"scope\":[\"support.variable.property\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Variable Function\",\"scope\":[\"variable.function\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Variable Interpolation\",\"scope\":[\"variable.interpolation\"],\"settings\":{\"foreground\":\"#ef787f\"}},{\"name\":\"Meta Function Call\",\"scope\":[\"meta.function-call\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Punctuation Section Embedded\",\"scope\":[\"punctuation.section.embedded\"],\"settings\":{\"foreground\":\"#e2817f\"}},{\"name\":\"Punctuation Tweaks\",\"scope\":[\"punctuation.terminator.expression\",\"punctuation.definition.arguments\",\"punctuation.definition.array\",\"punctuation.section.array\",\"meta.array\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"More Punctuation Tweaks\",\"scope\":[\"punctuation.definition.list.begin\",\"punctuation.definition.list.end\",\"punctuation.separator.arguments\",\"punctuation.definition.list\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Template Strings\",\"scope\":[\"string.template meta.template.expression\"],\"settings\":{\"foreground\":\"#e2817f\"}},{\"name\":\"Backtics(``) in Template Strings\",\"scope\":[\"string.template punctuation.definition.string\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Italics\",\"scope\":[\"italic\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"italic\"}},{\"name\":\"Bold\",\"scope\":[\"bold\"],\"settings\":{\"foreground\":\"#c5e478\",\"fontStyle\":\"bold\"}},{\"name\":\"Quote\",\"scope\":[\"quote\"],\"settings\":{\"foreground\":\"#969bb7\",\"fontStyle\":\"\"}},{\"name\":\"Raw Code\",\"scope\":[\"raw\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"CoffeScript Variable Assignment\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#31e1eb\"}},{\"name\":\"CoffeScript Parameter Function\",\"scope\":[\"variable.parameter.function.coffee\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"CoffeeScript Assignments\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"C# Readwrite Variables\",\"scope\":[\"variable.other.readwrite.cs\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C# Classes & Storage types\",\"scope\":[\"entity.name.type.class.cs\",\"storage.type.cs\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"C# Namespaces\",\"scope\":[\"entity.name.type.namespace.cs\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"C# Unquoted String Zone\",\"scope\":[\"string.unquoted.preprocessor.message.cs\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C# Region\",\"scope\":[\"punctuation.separator.hash.cs\",\"keyword.preprocessor.region.cs\",\"keyword.preprocessor.endregion.cs\"],\"settings\":{\"foreground\":\"#ffcb8b\",\"fontStyle\":\"bold\"}},{\"name\":\"C# Other Variables\",\"scope\":[\"variable.other.object.cs\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"C# Enum\",\"scope\":[\"entity.name.type.enum.cs\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Dart String\",\"scope\":[\"string.interpolated.single.dart\",\"string.interpolated.double.dart\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Dart Class\",\"scope\":[\"support.class.dart\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Tag names in Stylesheets\",\"scope\":[\"entity.name.tag.css\",\"entity.name.tag.less\",\"entity.name.tag.custom.css\",\"support.constant.property-value.css\"],\"settings\":{\"foreground\":\"#ff6d6d\",\"fontStyle\":\"\"}},{\"name\":\"Wildcard(*) selector in Stylesheets\",\"scope\":[\"entity.name.tag.wildcard.css\",\"entity.name.tag.wildcard.less\",\"entity.name.tag.wildcard.scss\",\"entity.name.tag.wildcard.sass\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"CSS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Attribute Name for CSS\",\"scope\":[\"meta.attribute-selector.css entity.other.attribute-name.attribute\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Elixir Classes\",\"scope\":[\"source.elixir support.type.elixir\",\"source.elixir meta.module.elixir entity.name.class.elixir\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Elixir Functions\",\"scope\":[\"source.elixir entity.name.function\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir Constants\",\"scope\":[\"source.elixir constant.other.symbol.elixir\",\"source.elixir constant.other.keywords.elixir\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Elixir String Punctuations\",\"scope\":[\"source.elixir punctuation.definition.string\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir\",\"scope\":[\"source.elixir variable.other.readwrite.module.elixir\",\"source.elixir variable.other.readwrite.module.elixir punctuation.definition.variable.elixir\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir Binary Punctuations\",\"scope\":[\"source.elixir .punctuation.binary.elixir\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Closure Constant Keyword\",\"scope\":[\"constant.keyword.clojure\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Go Function Calls\",\"scope\":[\"source.go meta.function-call.go\"],\"settings\":{\"foreground\":\"#dddddd\"}},{\"name\":\"Go Keywords\",\"scope\":[\"source.go keyword.package.go\",\"source.go keyword.import.go\",\"source.go keyword.function.go\",\"source.go keyword.type.go\",\"source.go keyword.struct.go\",\"source.go keyword.interface.go\",\"source.go keyword.const.go\",\"source.go keyword.var.go\",\"source.go keyword.map.go\",\"source.go keyword.channel.go\",\"source.go keyword.control.go\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Go Constants e.g. nil, string format (%s, %d, etc.)\",\"scope\":[\"source.go constant.language.go\",\"source.go constant.other.placeholder.go\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"C++ Functions\",\"scope\":[\"entity.name.function.preprocessor.cpp\",\"entity.scope.name.cpp\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"C++ Meta Namespace\",\"scope\":[\"meta.namespace-block.cpp\"],\"settings\":{\"foreground\":\"#e0dec6\"}},{\"name\":\"C++ Language Primitive Storage\",\"scope\":[\"storage.type.language.primitive.cpp\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"C++ Preprocessor Macro\",\"scope\":[\"meta.preprocessor.macro.cpp\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C++ Variable Parameter\",\"scope\":[\"variable.parameter\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Powershell Variables\",\"scope\":[\"variable.other.readwrite.powershell\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Powershell Function\",\"scope\":[\"support.function.powershell\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"ID Attribute Name in HTML\",\"scope\":[\"entity.other.attribute-name.id.html\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"HTML Punctuation Definition Tag\",\"scope\":[\"punctuation.definition.tag.html\"],\"settings\":{\"foreground\":\"#6ae9f0\"}},{\"name\":\"HTML Doctype\",\"scope\":[\"meta.tag.sgml.doctype.html\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Classes\",\"scope\":[\"meta.class entity.name.type.class.js\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"JavaScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.js\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"JavaScript Terminator\",\"scope\":[\"terminator.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Meta Punctuation Definition\",\"scope\":[\"meta.js punctuation.definition.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Entity Names in Code Documentations\",\"scope\":[\"entity.name.type.instance.jsdoc\",\"entity.name.type.instance.phpdoc\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"Other Variables in Code Documentations\",\"scope\":[\"variable.other.jsdoc\",\"variable.other.phpdoc\"],\"settings\":{\"foreground\":\"#78ccf0\"}},{\"name\":\"JavaScript module imports and exports\",\"scope\":[\"variable.other.meta.import.js\",\"meta.import.js variable.other\",\"variable.other.meta.export.js\",\"meta.export.js variable.other\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Variable Parameter Function\",\"scope\":[\"variable.parameter.function.js\"],\"settings\":{\"foreground\":\"#8b96ea\"}},{\"name\":\"JavaScript[React] Variable Other Object\",\"scope\":[\"variable.other.object.js\",\"variable.other.object.jsx\",\"variable.object.property.js\",\"variable.object.property.jsx\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Variables\",\"scope\":[\"variable.js\",\"variable.other.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Entity Name Type\",\"scope\":[\"entity.name.type.js\",\"entity.name.type.module.js\"],\"settings\":{\"foreground\":\"#ffcb8b\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Support Classes\",\"scope\":[\"support.class.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JSON Property Names\",\"scope\":[\"support.type.property-name.json\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"JSON Support Constants\",\"scope\":[\"support.constant.json\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"JSON Property values (string)\",\"scope\":[\"meta.structure.dictionary.value.json string.quoted.double\"],\"settings\":{\"foreground\":\"#c789d6\"}},{\"name\":\"Strings in JSON values\",\"scope\":[\"string.quoted.double.json punctuation.definition.string.json\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Specific JSON Property values like null\",\"scope\":[\"meta.structure.dictionary.json meta.structure.dictionary.value constant.language\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"JavaScript Other Variable\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Ruby Variables\",\"scope\":[\"variable.other.ruby\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Ruby Class\",\"scope\":[\"entity.name.type.class.ruby\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"Ruby Hashkeys\",\"scope\":[\"constant.language.symbol.hashkey.ruby\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"LESS Tag names\",\"scope\":[\"entity.name.tag.less\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"LESS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Attribute Name for LESS\",\"scope\":[\"meta.attribute-selector.less entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Markdown Headings\",\"scope\":[\"markup.heading.markdown\",\"markup.heading.setext.1.markdown\",\"markup.heading.setext.2.markdown\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown Italics\",\"scope\":[\"markup.italic.markdown\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"italic\"}},{\"name\":\"Markdown Bold\",\"scope\":[\"markup.bold.markdown\"],\"settings\":{\"foreground\":\"#c5e478\",\"fontStyle\":\"bold\"}},{\"name\":\"Markdown Quote + others\",\"scope\":[\"markup.quote.markdown\"],\"settings\":{\"foreground\":\"#969bb7\",\"fontStyle\":\"\"}},{\"name\":\"Markdown Raw Code + others\",\"scope\":[\"markup.inline.raw.markdown\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Markdown Links\",\"scope\":[\"markup.underline.link.markdown\",\"markup.underline.link.image.markdown\"],\"settings\":{\"foreground\":\"#ff869a\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Link Title and Description\",\"scope\":[\"string.other.link.title.markdown\",\"string.other.link.description.markdown\"],\"settings\":{\"foreground\":\"#d6deeb\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Punctuation\",\"scope\":[\"punctuation.definition.string.markdown\",\"punctuation.definition.string.begin.markdown\",\"punctuation.definition.string.end.markdown\",\"meta.link.inline.markdown punctuation.definition.string\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown MetaData Punctuation\",\"scope\":[\"punctuation.definition.metadata.markdown\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Markdown List Punctuation\",\"scope\":[\"beginning.punctuation.definition.list.markdown\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown Inline Raw String\",\"scope\":[\"markup.inline.raw.string.markdown\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"PHP Variables\",\"scope\":[\"variable.other.php\"],\"settings\":{\"foreground\":\"#bec5d4\"}},{\"name\":\"Support Classes in PHP\",\"scope\":[\"support.class.php\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Punctuations in PHP function calls\",\"scope\":[\"meta.function-call.php punctuation\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"PHP Global Variables\",\"scope\":[\"variable.other.global.php\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Declaration Punctuation in PHP Global Variables\",\"scope\":[\"variable.other.global.php punctuation.definition.variable\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Language Constants in Python\",\"scope\":[\"constant.language.python\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Python Function Parameter and Arguments\",\"scope\":[\"variable.parameter.function.python\",\"meta.function-call.arguments.python\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Python Function Call\",\"scope\":[\"meta.function-call.python\",\"meta.function-call.generic.python\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"Punctuations in Python\",\"scope\":[\"punctuation.python\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Decorator Functions in Python\",\"scope\":[\"entity.name.function.decorator.python\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Python Language Variable\",\"scope\":[\"source.python variable.language.special\"],\"settings\":{\"foreground\":\"#8eace3\"}},{\"name\":\"Python import control keyword\",\"scope\":[\"keyword.control\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"SCSS Variable\",\"scope\":[\"variable.scss\",\"variable.sass\",\"variable.parameter.url.scss\",\"variable.parameter.url.sass\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#bec5d4\"}},{\"name\":\"Attribute Name for SASS\",\"scope\":[\"meta.attribute-selector.scss entity.other.attribute-name.attribute\",\"meta.attribute-selector.sass entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Tag names in SASS\",\"scope\":[\"entity.name.tag.scss\",\"entity.name.tag.sass\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"SASS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.scss\",\"keyword.other.unit.sass\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"TypeScript[React] Variables and Object Properties\",\"scope\":[\"variable.other.readwrite.alias.ts\",\"variable.other.readwrite.alias.tsx\",\"variable.other.readwrite.ts\",\"variable.other.readwrite.tsx\",\"variable.other.object.ts\",\"variable.other.object.tsx\",\"variable.object.property.ts\",\"variable.object.property.tsx\",\"variable.other.ts\",\"variable.other.tsx\",\"variable.tsx\",\"variable.ts\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript[React] Entity Name Types\",\"scope\":[\"entity.name.type.ts\",\"entity.name.type.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript[React] Node Classes\",\"scope\":[\"support.class.node.ts\",\"support.class.node.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"TypeScript[React] Entity Name Types as Parameters\",\"scope\":[\"meta.type.parameters.ts entity.name.type\",\"meta.type.parameters.tsx entity.name.type\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"TypeScript[React] Import/Export Punctuations\",\"scope\":[\"meta.import.ts punctuation.definition.block\",\"meta.import.tsx punctuation.definition.block\",\"meta.export.ts punctuation.definition.block\",\"meta.export.tsx punctuation.definition.block\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.decorator punctuation.decorator.ts\",\"meta.decorator punctuation.decorator.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.tag.js meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"YAML Entity Name Tags\",\"scope\":[\"entity.name.tag.yaml\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"JavaScript Variable Other ReadWrite\",\"scope\":[\"variable.other.readwrite.js\",\"variable.parameter\"],\"settings\":{\"foreground\":\"#d7dbe0\"}},{\"name\":\"Support Class Component\",\"scope\":[\"support.class.component.js\",\"support.class.component.tsx\"],\"settings\":{\"foreground\":\"#f78c6c\",\"fontStyle\":\"\"}},{\"name\":\"Text nested in React tags\",\"scope\":[\"meta.jsx.children\",\"meta.jsx.children.js\",\"meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript Classes\",\"scope\":[\"meta.class entity.name.type.class.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript Entity Name Type\",\"scope\":[\"entity.name.type.tsx\",\"entity.name.type.module.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript Class Variable Keyword\",\"scope\":[\"meta.class.ts meta.var.expr.ts storage.type.ts\",\"meta.class.tsx meta.var.expr.tsx storage.type.tsx\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"TypeScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.ts\",\"meta.method.declaration storage.type.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"normalize font style of certain components\",\"scope\":[\"meta.property-list.css meta.property-value.css variable.other.less\",\"meta.property-list.scss variable.scss\",\"meta.property-list.sass variable.sass\",\"meta.brace\",\"keyword.operator.operator\",\"keyword.operator.or.regexp\",\"keyword.operator.expression.in\",\"keyword.operator.relational\",\"keyword.operator.assignment\",\"keyword.operator.comparison\",\"keyword.operator.type\",\"keyword.operator\",\"keyword\",\"punctuation.definintion.string\",\"punctuation\",\"variable.other.readwrite.js\",\"storage.type\",\"source.css\",\"string.quoted\"],\"settings\":{\"fontStyle\":\"\"}}],\"styleOverrides\":{\"frames\":{\"editorBackground\":\"var(--sl-color-gray-6)\",\"terminalBackground\":\"var(--sl-color-gray-6)\",\"editorActiveTabBackground\":\"var(--sl-color-gray-6)\",\"terminalTitlebarDotsForeground\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"terminalTitlebarDotsOpacity\":\"0.75\",\"inlineButtonForeground\":\"var(--sl-color-text)\",\"frameBoxShadowCssValue\":\"none\"},\"textMarkers\":{\"markBackground\":\"#ffffff17\",\"markBorderColor\":\"#ffffff40\"}}},{\"name\":\"Night Owl Light\",\"type\":\"light\",\"colors\":{\"focusBorder\":\"#93a1a1\",\"foreground\":\"#403f53\",\"disabledForeground\":\"#61616180\",\"descriptionForeground\":\"#403f53\",\"errorForeground\":\"#403f53\",\"icon.foreground\":\"#424242\",\"contrastActiveBorder\":null,\"contrastBorder\":null,\"textBlockQuote.background\":\"#7f7f7f1a\",\"textBlockQuote.border\":\"#007acc80\",\"textCodeBlock.background\":\"#dcdcdc66\",\"textLink.activeForeground\":\"#006ab1\",\"textLink.foreground\":\"#006ab1\",\"textPreformat.foreground\":\"#a31515\",\"textSeparator.foreground\":\"#0000002e\",\"editor.background\":\"#f6f7f9\",\"editor.foreground\":\"#403f53\",\"editorLineNumber.foreground\":\"#90a7b2\",\"editorLineNumber.activeForeground\":\"#403f53\",\"editorActiveLineNumber.foreground\":\"#0b216f\",\"editor.selectionBackground\":\"#e0e0e0\",\"editor.inactiveSelectionBackground\":\"#e0e0e080\",\"editor.selectionHighlightBackground\":\"#339cec33\",\"editorError.foreground\":\"#e64d49\",\"editorWarning.foreground\":\"#daaa01\",\"editorInfo.foreground\":\"#1a85ff\",\"editorHint.foreground\":\"#6c6c6c\",\"problemsErrorIcon.foreground\":\"#e64d49\",\"problemsWarningIcon.foreground\":\"#daaa01\",\"problemsInfoIcon.foreground\":\"#1a85ff\",\"editor.findMatchBackground\":\"#93a1a16c\",\"editor.findMatchHighlightBackground\":\"#93a1a16c\",\"editor.findRangeHighlightBackground\":\"#7497a633\",\"editorLink.activeForeground\":\"#0000ff\",\"editorLightBulb.foreground\":\"#ddb100\",\"editorLightBulbAutoFix.foreground\":\"#007acc\",\"diffEditor.insertedTextBackground\":\"#9ccc2c40\",\"diffEditor.insertedTextBorder\":null,\"diffEditor.removedTextBackground\":\"#ff000033\",\"diffEditor.removedTextBorder\":null,\"diffEditor.insertedLineBackground\":\"#9bb95533\",\"diffEditor.removedLineBackground\":\"#ff000033\",\"editorStickyScroll.background\":\"#fbfbfb\",\"editorStickyScrollHover.background\":\"#f0f0f0\",\"editorInlayHint.background\":\"#2aa29899\",\"editorInlayHint.foreground\":\"#f0f0f0\",\"editorInlayHint.typeBackground\":\"#2aa29899\",\"editorInlayHint.typeForeground\":\"#f0f0f0\",\"editorInlayHint.parameterBackground\":\"#2aa29899\",\"editorInlayHint.parameterForeground\":\"#f0f0f0\",\"editorPane.background\":\"#fbfbfb\",\"editorGroup.emptyBackground\":null,\"editorGroup.focusedEmptyBorder\":null,\"editorGroupHeader.tabsBackground\":\"var(--sl-color-gray-6)\",\"editorGroupHeader.tabsBorder\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"editorGroupHeader.noTabsBackground\":\"#f0f0f0\",\"editorGroupHeader.border\":null,\"editorGroup.border\":\"#f0f0f0\",\"editorGroup.dropBackground\":\"#2677cb2d\",\"editorGroup.dropIntoPromptForeground\":\"#403f53\",\"editorGroup.dropIntoPromptBackground\":\"#f0f0f0\",\"editorGroup.dropIntoPromptBorder\":null,\"sideBySideEditor.horizontalBorder\":\"#f0f0f0\",\"sideBySideEditor.verticalBorder\":\"#f0f0f0\",\"scrollbar.shadow\":\"#cccccc\",\"scrollbarSlider.background\":\"#0000001a\",\"scrollbarSlider.hoverBackground\":\"#00000055\",\"scrollbarSlider.activeBackground\":\"#00000099\",\"panel.background\":\"#f0f0f0\",\"panel.border\":\"#d9d9d9\",\"panelTitle.activeBorder\":\"#424242\",\"panelTitle.activeForeground\":\"#424242\",\"panelTitle.inactiveForeground\":\"#424242bf\",\"panelSectionHeader.background\":\"#80808051\",\"terminal.background\":\"#f6f6f6\",\"widget.shadow\":\"#d9d9d9\",\"editorWidget.background\":\"#f0f0f0\",\"editorWidget.foreground\":\"#403f53\",\"editorWidget.border\":\"#d9d9d9\",\"quickInput.background\":\"#f0f0f0\",\"quickInput.foreground\":\"#403f53\",\"quickInputTitle.background\":\"#0000000f\",\"pickerGroup.foreground\":\"#403f53\",\"pickerGroup.border\":\"#d9d9d9\",\"editor.hoverHighlightBackground\":\"#339cec33\",\"editorHoverWidget.background\":\"#f0f0f0\",\"editorHoverWidget.foreground\":\"#403f53\",\"editorHoverWidget.border\":\"#d9d9d9\",\"editorHoverWidget.statusBarBackground\":\"#e4e4e4\",\"titleBar.activeBackground\":\"var(--sl-color-gray-6)\",\"titleBar.activeForeground\":\"var(--sl-color-text)\",\"titleBar.inactiveBackground\":\"#f0f0f099\",\"titleBar.inactiveForeground\":\"#33333399\",\"titleBar.border\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"toolbar.hoverBackground\":\"#b8b8b850\",\"toolbar.activeBackground\":\"#a6a6a650\",\"tab.activeBackground\":\"#f6f6f6\",\"tab.unfocusedActiveBackground\":\"#f6f6f6\",\"tab.inactiveBackground\":\"#f0f0f0\",\"tab.unfocusedInactiveBackground\":\"#f0f0f0\",\"tab.activeForeground\":\"var(--sl-color-text)\",\"tab.inactiveForeground\":\"#403f53\",\"tab.unfocusedActiveForeground\":\"#403f53b3\",\"tab.unfocusedInactiveForeground\":\"#403f5380\",\"tab.hoverBackground\":null,\"tab.unfocusedHoverBackground\":null,\"tab.hoverForeground\":null,\"tab.unfocusedHoverForeground\":null,\"tab.border\":\"#f0f0f0\",\"tab.lastPinnedBorder\":\"#a9a9a9\",\"tab.activeBorder\":\"transparent\",\"tab.unfocusedActiveBorder\":null,\"tab.activeBorderTop\":\"var(--sl-color-accent)\",\"tab.unfocusedActiveBorderTop\":null,\"tab.hoverBorder\":null,\"tab.unfocusedHoverBorder\":null,\"tab.activeModifiedBorder\":\"#2aa298\",\"tab.inactiveModifiedBorder\":\"#93a1a1\",\"tab.unfocusedActiveModifiedBorder\":\"#93a1a1\",\"tab.unfocusedInactiveModifiedBorder\":\"#93a1a1\",\"badge.background\":\"#2aa298\",\"badge.foreground\":\"#f0f0f0\",\"button.background\":\"#2aa298\",\"button.foreground\":\"#f0f0f0\",\"button.border\":null,\"button.separator\":\"#f0f0f066\",\"button.hoverBackground\":\"#22827a\",\"button.secondaryBackground\":\"#5f6a79\",\"button.secondaryForeground\":\"#ffffff\",\"button.secondaryHoverBackground\":\"#4c5561\",\"dropdown.background\":\"#f0f0f0\",\"dropdown.foreground\":\"#403f53\",\"dropdown.border\":\"#d9d9d9\",\"list.activeSelectionBackground\":\"#d3e8f8\",\"list.activeSelectionForeground\":\"#403f53\",\"tree.indentGuidesStroke\":\"#a9a9a9\",\"input.background\":\"#f0f0f0\",\"input.foreground\":\"#403f53\",\"input.placeholderForeground\":\"#93a1a1\",\"inputOption.activeBorder\":\"#2aa298\",\"inputOption.hoverBackground\":\"#b8b8b850\",\"inputOption.activeBackground\":\"#93a1a133\",\"inputOption.activeForeground\":\"#000000\",\"inputValidation.infoBackground\":\"#f0f0f0\",\"inputValidation.infoBorder\":\"#d0d0d0\",\"inputValidation.warningBackground\":\"#daaa01\",\"inputValidation.warningBorder\":\"#e0af02\",\"inputValidation.errorBackground\":\"#f76e6e\",\"inputValidation.errorBorder\":\"#de3d3b\",\"keybindingLabel.background\":\"#dddddd66\",\"keybindingLabel.foreground\":\"#555555\",\"keybindingLabel.border\":\"#cccccc66\",\"keybindingLabel.bottomBorder\":\"#bbbbbb66\",\"menu.foreground\":\"#403f53\",\"menu.background\":\"#f0f0f0\",\"menu.selectionForeground\":\"#403f53\",\"menu.selectionBackground\":\"#d3e8f8\",\"menu.separatorBackground\":\"#d4d4d4\",\"editor.snippetTabstopHighlightBackground\":\"#0a326433\",\"editor.snippetFinalTabstopHighlightBorder\":\"#0a326480\",\"terminal.ansiBlack\":\"#403f53\",\"terminal.ansiRed\":\"#de3d3b\",\"terminal.ansiGreen\":\"#08916a\",\"terminal.ansiYellow\":\"#e0af02\",\"terminal.ansiBlue\":\"#288ed7\",\"terminal.ansiMagenta\":\"#d6438a\",\"terminal.ansiCyan\":\"#2aa298\",\"terminal.ansiWhite\":\"#f0f0f0\",\"terminal.ansiBrightBlack\":\"#403f53\",\"terminal.ansiBrightRed\":\"#de3d3b\",\"terminal.ansiBrightGreen\":\"#08916a\",\"terminal.ansiBrightYellow\":\"#daaa01\",\"terminal.ansiBrightBlue\":\"#288ed7\",\"terminal.ansiBrightMagenta\":\"#d6438a\",\"terminal.ansiBrightCyan\":\"#2aa298\",\"terminal.ansiBrightWhite\":\"#f0f0f0\",\"selection.background\":\"#7a8181ad\",\"notifications.background\":\"#f0f0f0\",\"notifications.foreground\":\"#403f53\",\"notificationLink.foreground\":\"#994cc3\",\"notifications.border\":\"#cccccc\",\"notificationCenter.border\":\"#cccccc\",\"notificationToast.border\":\"#cccccc\",\"notificationCenterHeader.foreground\":\"#403f53\",\"notificationCenterHeader.background\":\"#f0f0f0\",\"input.border\":\"#d9d9d9\",\"progressBar.background\":\"#2aa298\",\"list.inactiveSelectionBackground\":\"#e0e7ea\",\"list.inactiveSelectionForeground\":\"#403f53\",\"list.focusBackground\":\"#d3e8f8\",\"list.hoverBackground\":\"#d3e8f8\",\"list.focusForeground\":\"#403f53\",\"list.hoverForeground\":\"#403f53\",\"list.highlightForeground\":\"#403f53\",\"list.errorForeground\":\"#e64d49\",\"list.warningForeground\":\"#daaa01\",\"activityBar.background\":\"#f0f0f0\",\"activityBar.foreground\":\"#403f53\",\"activityBar.dropBackground\":\"#d0d0d0\",\"activityBarBadge.background\":\"#403f53\",\"activityBarBadge.foreground\":\"#f0f0f0\",\"activityBar.border\":\"#f0f0f0\",\"sideBar.background\":\"#f0f0f0\",\"sideBar.foreground\":\"#403f53\",\"sideBarTitle.foreground\":\"#403f53\",\"sideBar.border\":\"#f0f0f0\",\"editorGroup.background\":\"#f6f6f6\",\"editorCursor.foreground\":\"#90a7b2\",\"editor.wordHighlightBackground\":\"#339cec33\",\"editor.wordHighlightStrongBackground\":\"#007dd659\",\"editor.lineHighlightBackground\":\"#f0f0f0\",\"editor.rangeHighlightBackground\":\"#7497a633\",\"editorWhitespace.foreground\":\"#d9d9d9\",\"editorIndentGuide.background\":\"#d9d9d9\",\"editorCodeLens.foreground\":\"#403f53\",\"editorBracketMatch.background\":\"#d3e8f8\",\"editorBracketMatch.border\":\"#2aa298\",\"editorError.border\":\"#fbfbfb\",\"editorWarning.border\":\"#daaa01\",\"editorGutter.addedBackground\":\"#49d0c5\",\"editorGutter.modifiedBackground\":\"#6fbef6\",\"editorGutter.deletedBackground\":\"#f76e6e\",\"editorRuler.foreground\":\"#d9d9d9\",\"editorOverviewRuler.errorForeground\":\"#e64d49\",\"editorOverviewRuler.warningForeground\":\"#daaa01\",\"editorSuggestWidget.background\":\"#f0f0f0\",\"editorSuggestWidget.foreground\":\"#403f53\",\"editorSuggestWidget.highlightForeground\":\"#403f53\",\"editorSuggestWidget.selectedBackground\":\"#d3e8f8\",\"editorSuggestWidget.border\":\"#d9d9d9\",\"debugExceptionWidget.background\":\"#f0f0f0\",\"debugExceptionWidget.border\":\"#d9d9d9\",\"editorMarkerNavigation.background\":\"#d0d0d0\",\"editorMarkerNavigationError.background\":\"#f76e6e\",\"editorMarkerNavigationWarning.background\":\"#daaa01\",\"debugToolBar.background\":\"#f0f0f0\",\"extensionButton.prominentBackground\":\"#2aa298\",\"extensionButton.prominentForeground\":\"#f0f0f0\",\"statusBar.background\":\"#f0f0f0\",\"statusBar.border\":\"#f0f0f0\",\"statusBar.debuggingBackground\":\"#f0f0f0\",\"statusBar.debuggingForeground\":\"#403f53\",\"statusBar.foreground\":\"#403f53\",\"statusBar.noFolderBackground\":\"#f0f0f0\",\"statusBar.noFolderForeground\":\"#403f53\",\"peekView.border\":\"#d9d9d9\",\"peekViewEditor.background\":\"#f6f6f6\",\"peekViewEditorGutter.background\":\"#f6f6f6\",\"peekViewEditor.matchHighlightBackground\":\"#49d0c5\",\"peekViewResult.background\":\"#f0f0f0\",\"peekViewResult.fileForeground\":\"#403f53\",\"peekViewResult.lineForeground\":\"#403f53\",\"peekViewResult.matchHighlightBackground\":\"#49d0c5\",\"peekViewResult.selectionBackground\":\"#e0e7ea\",\"peekViewResult.selectionForeground\":\"#403f53\",\"peekViewTitle.background\":\"#f0f0f0\",\"peekViewTitleLabel.foreground\":\"#403f53\",\"peekViewTitleDescription.foreground\":\"#403f53\",\"terminal.foreground\":\"#403f53\"},\"fg\":\"#403f53\",\"bg\":\"#f6f7f9\",\"semanticHighlighting\":false,\"settings\":[{\"name\":\"Changed\",\"scope\":[\"markup.changed\",\"meta.diff.header.git\",\"meta.diff.header.from-file\",\"meta.diff.header.to-file\"],\"settings\":{\"foreground\":\"#556484\"}},{\"name\":\"Deleted\",\"scope\":[\"markup.deleted.diff\"],\"settings\":{\"foreground\":\"#ae3c3afd\"}},{\"name\":\"Inserted\",\"scope\":[\"markup.inserted.diff\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Global settings\",\"settings\":{\"background\":\"#011627\",\"foreground\":\"#403f53\"}},{\"name\":\"Comment\",\"scope\":[\"comment\"],\"settings\":{\"foreground\":\"#5f636f\"}},{\"name\":\"String\",\"scope\":[\"string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"String Quoted\",\"scope\":[\"string.quoted\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Support Constant Math\",\"scope\":[\"support.constant.math\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Number\",\"scope\":[\"constant.numeric\",\"constant.character.numeric\"],\"settings\":{\"foreground\":\"#aa0982\",\"fontStyle\":\"\"}},{\"name\":\"Built-in constant\",\"scope\":[\"constant.language\",\"punctuation.definition.constant\",\"variable.other.constant\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"User-defined constant\",\"scope\":[\"constant.character\",\"constant.other\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Constant Character Escape\",\"scope\":[\"constant.character.escape\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"RegExp String\",\"scope\":[\"string.regexp\",\"string.regexp keyword.other\"],\"settings\":{\"foreground\":\"#3a688f\"}},{\"name\":\"Comma in functions\",\"scope\":[\"meta.function punctuation.separator.comma\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"Variable\",\"scope\":[\"variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Keyword\",\"scope\":[\"punctuation.accessor\",\"keyword\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage\",\"scope\":[\"storage\",\"meta.var.expr\",\"meta.class meta.method.declaration meta.var.expr storage.type.js\",\"storage.type.property.js\",\"storage.type.property.ts\",\"storage.type.property.tsx\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type.function.arrow.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Class name\",\"scope\":[\"entity.name.class\",\"meta.class entity.name.type.class\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Inherited class\",\"scope\":[\"entity.other.inherited-class\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Function name\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Meta Tag\",\"scope\":[\"punctuation.definition.tag\",\"meta.tag\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"HTML Tag names\",\"scope\":[\"entity.name.tag\",\"meta.tag.other.html\",\"meta.tag.other.js\",\"meta.tag.other.tsx\",\"entity.name.tag.tsx\",\"entity.name.tag.js\",\"entity.name.tag\",\"meta.tag.js\",\"meta.tag.tsx\",\"meta.tag.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Tag attribute\",\"scope\":[\"entity.other.attribute-name\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Entity Name Tag Custom\",\"scope\":[\"entity.name.tag.custom\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Library (function & constant)\",\"scope\":[\"support.function\",\"support.constant\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Support Constant Property Value meta\",\"scope\":[\"support.constant.meta.property-value\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Library class/type\",\"scope\":[\"support.type\",\"support.class\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Support Variable DOM\",\"scope\":[\"support.variable.dom\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Invalid\",\"scope\":[\"invalid\"],\"settings\":{\"foreground\":\"#bb2060\"}},{\"name\":\"Invalid deprecated\",\"scope\":[\"invalid.deprecated\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Keyword Operator\",\"scope\":[\"keyword.operator\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Relational\",\"scope\":[\"keyword.operator.relational\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Assignment\",\"scope\":[\"keyword.operator.assignment\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Arithmetic\",\"scope\":[\"keyword.operator.arithmetic\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Bitwise\",\"scope\":[\"keyword.operator.bitwise\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Increment\",\"scope\":[\"keyword.operator.increment\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Ternary\",\"scope\":[\"keyword.operator.ternary\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Double-Slashed Comment\",\"scope\":[\"comment.line.double-slash\"],\"settings\":{\"foreground\":\"#5d6376\"}},{\"name\":\"Object\",\"scope\":[\"object\"],\"settings\":{\"foreground\":\"#58656a\"}},{\"name\":\"Null\",\"scope\":[\"constant.language.null\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Meta Brace\",\"scope\":[\"meta.brace\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Meta Delimiter Period\",\"scope\":[\"meta.delimiter.period\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Punctuation Definition String\",\"scope\":[\"punctuation.definition.string\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Punctuation Definition String Markdown\",\"scope\":[\"punctuation.definition.string.begin.markdown\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Boolean\",\"scope\":[\"constant.language.boolean\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Object Comma\",\"scope\":[\"object.comma\"],\"settings\":{\"foreground\":\"#646464\"}},{\"name\":\"Variable Parameter Function\",\"scope\":[\"variable.parameter.function\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Support Type Property Name & entity name tags\",\"scope\":[\"support.type.vendor.property-name\",\"support.constant.vendor.property-value\",\"support.type.property-name\",\"meta.property-list entity.name.tag\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Entity Name tag reference in stylesheets\",\"scope\":[\"meta.property-list entity.name.tag.reference\"],\"settings\":{\"foreground\":\"#286d70\"}},{\"name\":\"Constant Other Color RGB Value Punctuation Definition Constant\",\"scope\":[\"constant.other.color.rgb-value punctuation.definition.constant\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Constant Other Color\",\"scope\":[\"constant.other.color\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Keyword Other Unit\",\"scope\":[\"keyword.other.unit\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Meta Selector\",\"scope\":[\"meta.selector\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Entity Other Attribute Name Id\",\"scope\":[\"entity.other.attribute-name.id\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Meta Property Name\",\"scope\":[\"meta.property-name\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Doctypes\",\"scope\":[\"entity.name.tag.doctype\",\"meta.tag.sgml.doctype\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Punctuation Definition Parameters\",\"scope\":[\"punctuation.definition.parameters\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Keyword Control Operator\",\"scope\":[\"keyword.control.operator\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Keyword Operator Logical\",\"scope\":[\"keyword.operator.logical\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"\"}},{\"name\":\"Variable Instances\",\"scope\":[\"variable.instance\",\"variable.other.instance\",\"variable.readwrite.instance\",\"variable.other.readwrite.instance\",\"variable.other.property\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Variable Property Other object property\",\"scope\":[\"variable.other.object.property\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Variable Property Other object\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Entity Name Function\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Keyword Operator Comparison, imports, returns and Keyword Operator Ruby\",\"scope\":[\"keyword.operator.comparison\",\"keyword.control.flow.js\",\"keyword.control.flow.ts\",\"keyword.control.flow.tsx\",\"keyword.control.ruby\",\"keyword.control.module.ruby\",\"keyword.control.class.ruby\",\"keyword.control.def.ruby\",\"keyword.control.loop.js\",\"keyword.control.loop.ts\",\"keyword.control.import.js\",\"keyword.control.import.ts\",\"keyword.control.import.tsx\",\"keyword.control.from.js\",\"keyword.control.from.ts\",\"keyword.control.from.tsx\",\"keyword.operator.instanceof.js\",\"keyword.operator.expression.instanceof.ts\",\"keyword.operator.expression.instanceof.tsx\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Control Conditional\",\"scope\":[\"keyword.control.conditional.js\",\"keyword.control.conditional.ts\",\"keyword.control.switch.js\",\"keyword.control.switch.ts\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"\"}},{\"name\":\"Support Constant, `new` keyword, Special Method Keyword, `debugger`, other keywords\",\"scope\":[\"support.constant\",\"keyword.other.special-method\",\"keyword.other.new\",\"keyword.other.debugger\",\"keyword.control\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Support Function\",\"scope\":[\"support.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Invalid Broken\",\"scope\":[\"invalid.broken\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Invalid Unimplemented\",\"scope\":[\"invalid.unimplemented\"],\"settings\":{\"foreground\":\"#486e26\"}},{\"name\":\"Invalid Illegal\",\"scope\":[\"invalid.illegal\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Language Variable\",\"scope\":[\"variable.language\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Support Variable Property\",\"scope\":[\"support.variable.property\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Variable Function\",\"scope\":[\"variable.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variable Interpolation\",\"scope\":[\"variable.interpolation\"],\"settings\":{\"foreground\":\"#a64348\"}},{\"name\":\"Meta Function Call\",\"scope\":[\"meta.function-call\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Punctuation Section Embedded\",\"scope\":[\"punctuation.section.embedded\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Punctuation Tweaks\",\"scope\":[\"punctuation.terminator.expression\",\"punctuation.definition.arguments\",\"punctuation.definition.array\",\"punctuation.section.array\",\"meta.array\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"More Punctuation Tweaks\",\"scope\":[\"punctuation.definition.list.begin\",\"punctuation.definition.list.end\",\"punctuation.separator.arguments\",\"punctuation.definition.list\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Template Strings\",\"scope\":[\"string.template meta.template.expression\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Backtics(``) in Template Strings\",\"scope\":[\"string.template punctuation.definition.string\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Italics\",\"scope\":[\"italic\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"italic\"}},{\"name\":\"Bold\",\"scope\":[\"bold\"],\"settings\":{\"foreground\":\"#3b61b0\",\"fontStyle\":\"bold\"}},{\"name\":\"Quote\",\"scope\":[\"quote\"],\"settings\":{\"foreground\":\"#5c6285\"}},{\"name\":\"Raw Code\",\"scope\":[\"raw\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"CoffeScript Variable Assignment\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#186e73\"}},{\"name\":\"CoffeScript Parameter Function\",\"scope\":[\"variable.parameter.function.coffee\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"CoffeeScript Assignments\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"C# Readwrite Variables\",\"scope\":[\"variable.other.readwrite.cs\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"C# Classes & Storage types\",\"scope\":[\"entity.name.type.class.cs\",\"storage.type.cs\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"C# Namespaces\",\"scope\":[\"entity.name.type.namespace.cs\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Tag names in Stylesheets\",\"scope\":[\"entity.name.tag.css\",\"entity.name.tag.less\",\"entity.name.tag.custom.css\",\"support.constant.property-value.css\"],\"settings\":{\"foreground\":\"#984e4d\",\"fontStyle\":\"\"}},{\"name\":\"Wildcard(*) selector in Stylesheets\",\"scope\":[\"entity.name.tag.wildcard.css\",\"entity.name.tag.wildcard.less\",\"entity.name.tag.wildcard.scss\",\"entity.name.tag.wildcard.sass\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"CSS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Attribute Name for CSS\",\"scope\":[\"meta.attribute-selector.css entity.other.attribute-name.attribute\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Elixir Classes\",\"scope\":[\"source.elixir support.type.elixir\",\"source.elixir meta.module.elixir entity.name.class.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Functions\",\"scope\":[\"source.elixir entity.name.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Constants\",\"scope\":[\"source.elixir constant.other.symbol.elixir\",\"source.elixir constant.other.keywords.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir String Punctuations\",\"scope\":[\"source.elixir punctuation.definition.string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir\",\"scope\":[\"source.elixir variable.other.readwrite.module.elixir\",\"source.elixir variable.other.readwrite.module.elixir punctuation.definition.variable.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Binary Punctuations\",\"scope\":[\"source.elixir .punctuation.binary.elixir\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Closure Constant Keyword\",\"scope\":[\"constant.keyword.clojure\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Go Function Calls\",\"scope\":[\"source.go meta.function-call.go\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Go Keywords\",\"scope\":[\"source.go keyword.package.go\",\"source.go keyword.import.go\",\"source.go keyword.function.go\",\"source.go keyword.type.go\",\"source.go keyword.struct.go\",\"source.go keyword.interface.go\",\"source.go keyword.const.go\",\"source.go keyword.var.go\",\"source.go keyword.map.go\",\"source.go keyword.channel.go\",\"source.go keyword.control.go\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Go Constants e.g. nil, string format (%s, %d, etc.)\",\"scope\":[\"source.go constant.language.go\",\"source.go constant.other.placeholder.go\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"C++ Functions\",\"scope\":[\"entity.name.function.preprocessor.cpp\",\"entity.scope.name.cpp\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"C++ Meta Namespace\",\"scope\":[\"meta.namespace-block.cpp\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"C++ Language Primitive Storage\",\"scope\":[\"storage.type.language.primitive.cpp\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"C++ Preprocessor Macro\",\"scope\":[\"meta.preprocessor.macro.cpp\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"C++ Variable Parameter\",\"scope\":[\"variable.parameter\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Powershell Variables\",\"scope\":[\"variable.other.readwrite.powershell\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Powershell Function\",\"scope\":[\"support.function.powershell\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"ID Attribute Name in HTML\",\"scope\":[\"entity.other.attribute-name.id.html\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"HTML Punctuation Definition Tag\",\"scope\":[\"punctuation.definition.tag.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"HTML Doctype\",\"scope\":[\"meta.tag.sgml.doctype.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"JavaScript Classes\",\"scope\":[\"meta.class entity.name.type.class.js\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"JavaScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.js\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"JavaScript Terminator\",\"scope\":[\"terminator.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Meta Punctuation Definition\",\"scope\":[\"meta.js punctuation.definition.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Entity Names in Code Documentations\",\"scope\":[\"entity.name.type.instance.jsdoc\",\"entity.name.type.instance.phpdoc\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"Other Variables in Code Documentations\",\"scope\":[\"variable.other.jsdoc\",\"variable.other.phpdoc\"],\"settings\":{\"foreground\":\"#3e697c\"}},{\"name\":\"JavaScript module imports and exports\",\"scope\":[\"variable.other.meta.import.js\",\"meta.import.js variable.other\",\"variable.other.meta.export.js\",\"meta.export.js variable.other\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Variable Parameter Function\",\"scope\":[\"variable.parameter.function.js\"],\"settings\":{\"foreground\":\"#555ea2\"}},{\"name\":\"JavaScript[React] Variable Other Object\",\"scope\":[\"variable.other.object.js\",\"variable.other.object.jsx\",\"variable.object.property.js\",\"variable.object.property.jsx\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Variables\",\"scope\":[\"variable.js\",\"variable.other.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Entity Name Type\",\"scope\":[\"entity.name.type.js\",\"entity.name.type.module.js\"],\"settings\":{\"foreground\":\"#111111\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Support Classes\",\"scope\":[\"support.class.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JSON Property Names\",\"scope\":[\"support.type.property-name.json\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"JSON Support Constants\",\"scope\":[\"support.constant.json\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"JSON Property values (string)\",\"scope\":[\"meta.structure.dictionary.value.json string.quoted.double\"],\"settings\":{\"foreground\":\"#7c5686\"}},{\"name\":\"Strings in JSON values\",\"scope\":[\"string.quoted.double.json punctuation.definition.string.json\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Specific JSON Property values like null\",\"scope\":[\"meta.structure.dictionary.json meta.structure.dictionary.value constant.language\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"JavaScript Other Variable\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Ruby Variables\",\"scope\":[\"variable.other.ruby\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Ruby Class\",\"scope\":[\"entity.name.type.class.ruby\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Ruby Hashkeys\",\"scope\":[\"constant.language.symbol.hashkey.ruby\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Ruby Symbols\",\"scope\":[\"constant.language.symbol.ruby\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"LESS Tag names\",\"scope\":[\"entity.name.tag.less\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"LESS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Attribute Name for LESS\",\"scope\":[\"meta.attribute-selector.less entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Markdown Headings\",\"scope\":[\"markup.heading.markdown\",\"markup.heading.setext.1.markdown\",\"markup.heading.setext.2.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown Italics\",\"scope\":[\"markup.italic.markdown\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"italic\"}},{\"name\":\"Markdown Bold\",\"scope\":[\"markup.bold.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\",\"fontStyle\":\"bold\"}},{\"name\":\"Markdown Quote + others\",\"scope\":[\"markup.quote.markdown\"],\"settings\":{\"foreground\":\"#5c6285\"}},{\"name\":\"Markdown Raw Code + others\",\"scope\":[\"markup.inline.raw.markdown\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Markdown Links\",\"scope\":[\"markup.underline.link.markdown\",\"markup.underline.link.image.markdown\"],\"settings\":{\"foreground\":\"#954f5a\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Link Title and Description\",\"scope\":[\"string.other.link.title.markdown\",\"string.other.link.description.markdown\"],\"settings\":{\"foreground\":\"#403f53\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Punctuation\",\"scope\":[\"punctuation.definition.string.markdown\",\"punctuation.definition.string.begin.markdown\",\"punctuation.definition.string.end.markdown\",\"meta.link.inline.markdown punctuation.definition.string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown MetaData Punctuation\",\"scope\":[\"punctuation.definition.metadata.markdown\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Markdown List Punctuation\",\"scope\":[\"beginning.punctuation.definition.list.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown Inline Raw String\",\"scope\":[\"markup.inline.raw.string.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"PHP Variables\",\"scope\":[\"variable.other.php\",\"variable.other.property.php\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Support Classes in PHP\",\"scope\":[\"support.class.php\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Punctuations in PHP function calls\",\"scope\":[\"meta.function-call.php punctuation\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"PHP Global Variables\",\"scope\":[\"variable.other.global.php\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Declaration Punctuation in PHP Global Variables\",\"scope\":[\"variable.other.global.php punctuation.definition.variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Language Constants in Python\",\"scope\":[\"constant.language.python\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Python Function Parameter and Arguments\",\"scope\":[\"variable.parameter.function.python\",\"meta.function-call.arguments.python\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Python Function Call\",\"scope\":[\"meta.function-call.python\",\"meta.function-call.generic.python\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Punctuations in Python\",\"scope\":[\"punctuation.python\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Decorator Functions in Python\",\"scope\":[\"entity.name.function.decorator.python\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Python Language Variable\",\"scope\":[\"source.python variable.language.special\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Python import control keyword\",\"scope\":[\"keyword.control\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"SCSS Variable\",\"scope\":[\"variable.scss\",\"variable.sass\",\"variable.parameter.url.scss\",\"variable.parameter.url.sass\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Attribute Name for SASS\",\"scope\":[\"meta.attribute-selector.scss entity.other.attribute-name.attribute\",\"meta.attribute-selector.sass entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Tag names in SASS\",\"scope\":[\"entity.name.tag.scss\",\"entity.name.tag.sass\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"SASS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.scss\",\"keyword.other.unit.sass\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"TypeScript[React] Variables and Object Properties\",\"scope\":[\"variable.other.readwrite.alias.ts\",\"variable.other.readwrite.alias.tsx\",\"variable.other.readwrite.ts\",\"variable.other.readwrite.tsx\",\"variable.other.object.ts\",\"variable.other.object.tsx\",\"variable.object.property.ts\",\"variable.object.property.tsx\",\"variable.other.ts\",\"variable.other.tsx\",\"variable.tsx\",\"variable.ts\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript[React] Entity Name Types\",\"scope\":[\"entity.name.type.ts\",\"entity.name.type.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript[React] Node Classes\",\"scope\":[\"support.class.node.ts\",\"support.class.node.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"TypeScript[React] Entity Name Types as Parameters\",\"scope\":[\"meta.type.parameters.ts entity.name.type\",\"meta.type.parameters.tsx entity.name.type\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"TypeScript[React] Import/Export Punctuations\",\"scope\":[\"meta.import.ts punctuation.definition.block\",\"meta.import.tsx punctuation.definition.block\",\"meta.export.ts punctuation.definition.block\",\"meta.export.tsx punctuation.definition.block\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.decorator punctuation.decorator.ts\",\"meta.decorator punctuation.decorator.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.tag.js meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"YAML Entity Name Tags\",\"scope\":[\"entity.name.tag.yaml\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"JavaScript Variable Other ReadWrite\",\"scope\":[\"variable.other.readwrite.js\",\"variable.parameter\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Support Class Component\",\"scope\":[\"support.class.component.js\",\"support.class.component.tsx\"],\"settings\":{\"foreground\":\"#aa0982\",\"fontStyle\":\"\"}},{\"name\":\"Text nested in React tags\",\"scope\":[\"meta.jsx.children\",\"meta.jsx.children.js\",\"meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript Classes\",\"scope\":[\"meta.class entity.name.type.class.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript Entity Name Type\",\"scope\":[\"entity.name.type.tsx\",\"entity.name.type.module.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript Class Variable Keyword\",\"scope\":[\"meta.class.ts meta.var.expr.ts storage.type.ts\",\"meta.class.tsx meta.var.expr.tsx storage.type.tsx\"],\"settings\":{\"foreground\":\"#76578b\"}},{\"name\":\"TypeScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.ts\",\"meta.method.declaration storage.type.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"normalize font style of certain components\",\"scope\":[\"meta.property-list.css meta.property-value.css variable.other.less\",\"meta.property-list.scss variable.scss\",\"meta.property-list.sass variable.sass\",\"meta.brace\",\"keyword.operator.operator\",\"keyword.operator.or.regexp\",\"keyword.operator.expression.in\",\"keyword.operator.relational\",\"keyword.operator.assignment\",\"keyword.operator.comparison\",\"keyword.operator.type\",\"keyword.operator\",\"keyword\",\"punctuation.definintion.string\",\"punctuation\",\"variable.other.readwrite.js\",\"storage.type\",\"source.css\",\"string.quoted\"],\"settings\":{\"fontStyle\":\"\"}}],\"styleOverrides\":{\"frames\":{\"editorBackground\":\"var(--sl-color-gray-7)\",\"terminalBackground\":\"var(--sl-color-gray-7)\",\"editorActiveTabBackground\":\"var(--sl-color-gray-7)\",\"terminalTitlebarDotsForeground\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"terminalTitlebarDotsOpacity\":\"0.75\",\"inlineButtonForeground\":\"var(--sl-color-text)\",\"frameBoxShadowCssValue\":\"none\"},\"textMarkers\":{\"markBackground\":\"#0000001a\",\"markBorderColor\":\"#00000055\"}}}],\"defaultLocale\":\"en\",\"styleOverrides\":{\"borderRadius\":\"0px\",\"borderWidth\":\"1px\",\"codePaddingBlock\":\"0.75rem\",\"codePaddingInline\":\"1rem\",\"codeFontFamily\":\"var(--__sl-font-mono)\",\"codeFontSize\":\"var(--sl-text-code)\",\"codeLineHeight\":\"var(--sl-line-height)\",\"uiFontFamily\":\"var(--__sl-font)\",\"textMarkers\":{\"lineDiffIndicatorMarginLeft\":\"0.25rem\",\"defaultChroma\":\"45\",\"backgroundOpacity\":\"60%\"}},\"plugins\":[{\"name\":\"Starlight Plugin\",\"hooks\":{}},{\"name\":\"astro-expressive-code\",\"hooks\":{}}]}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false},\"prefetch\":{\"prefetchAll\":true},\"i18n\":{\"defaultLocale\":\"en\",\"locales\":[\"en\"],\"routing\":{\"prefixDefaultLocale\":false,\"redirectToDefaultLocale\":false,\"fallbackType\":\"redirect\"}}}","docs",["Map",11,12,25,26,36,37,47,48,58,59,69,70,80,81,91,92,102,103,113,114,124,125,135,136,146,147,157,158,168,169,179,180,190,191,201,202,212,213,223,224,234,235,245,246,256,257,267,268,278,279,289,290,300,301,311,312,322,323,333,334,344,345,355,356,366,367,377,378,388,389,399,400,410,411],"api/cache-management",{"id":11,"data":13,"body":22,"filePath":23,"digest":24,"deferredRender":16},{"title":14,"description":15,"editUrl":16,"head":17,"template":18,"sidebar":19,"pagefind":16,"draft":20},"Cache Management","Functions for managing local model cache in OPFS storage",true,[],"doc",{"hidden":20,"attrs":21},false,{},"WebLLM.io provides two utilities for managing the local model cache stored in OPFS (Origin Private File System). These functions delegate to `@mlc-ai/web-llm`'s cache management system and are only available when the MLC peer dependency is installed.\n\n## Functions\n\n### `hasModelInCache()`\n\nCheck if a specific model is cached in OPFS.\n\n#### Signature\n\n```ts\nasync function hasModelInCache(modelId: string): Promise\u003Cboolean>;\n```\n\n#### Parameters\n\n- `modelId` - MLC model identifier (e.g., `'Llama-3.1-8B-Instruct-q4f16_1-MLC'`)\n\n#### Return Value\n\n- Returns `true` if the model is fully cached in OPFS\n- Returns `false` if:\n  - The model is not cached\n  - The model is partially cached (incomplete download)\n  - `@mlc-ai/web-llm` is not installed\n\n#### Examples\n\n```ts\nimport { hasModelInCache } from '@webllm-io/sdk';\n\n// Check if model is cached before loading\nconst modelId = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\nconst isCached = await hasModelInCache(modelId);\n\nif (isCached) {\n  console.log('Model is cached, loading will be fast');\n} else {\n  console.log('Model will be downloaded (~4-8GB)');\n}\n```\n\n```ts\n// Show cache status in UI\nasync function updateCacheStatus() {\n  const models = [\n    'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n    'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n  ];\n\n  for (const model of models) {\n    const cached = await hasModelInCache(model);\n    console.log(`${model}: ${cached ? ' Cached' : ' Not cached'}`);\n  }\n}\n```\n\n### `deleteModelFromCache()`\n\nRemove a specific model from OPFS cache to free up storage.\n\n#### Signature\n\n```ts\nasync function deleteModelFromCache(modelId: string): Promise\u003Cvoid>;\n```\n\n#### Parameters\n\n- `modelId` - MLC model identifier to delete\n\n#### Return Value\n\nReturns a Promise that resolves when deletion completes.\n\n#### Errors\n\n- Throws if `@mlc-ai/web-llm` is not installed\n- Throws if OPFS access fails (e.g., browser doesn't support OPFS)\n- Silently succeeds if model is not cached\n\n#### Examples\n\n```ts\nimport { deleteModelFromCache } from '@webllm-io/sdk';\n\n// Delete a specific model\nawait deleteModelFromCache('Llama-3.1-8B-Instruct-q4f16_1-MLC');\nconsole.log('Model deleted from cache');\n```\n\n```ts\n// Clear all cached models\nasync function clearAllCachedModels() {\n  const models = [\n    'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n    'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n  ];\n\n  for (const model of models) {\n    try {\n      await deleteModelFromCache(model);\n      console.log(`Deleted: ${model}`);\n    } catch (error) {\n      console.error(`Failed to delete ${model}:`, error);\n    }\n  }\n}\n```\n\n## Complete Examples\n\n### Preload check with user confirmation\n\n```ts\nimport { hasModelInCache, createClient } from '@webllm-io/sdk';\n\nasync function initializeWithCacheCheck() {\n  const modelId = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\n  const isCached = await hasModelInCache(modelId);\n\n  if (!isCached) {\n    const confirmed = confirm(\n      `Model not cached. Download ~6GB?\\n\\nThis may take several minutes.`\n    );\n\n    if (!confirmed) {\n      console.log('User cancelled download');\n      return null;\n    }\n  }\n\n  const client = createClient({\n    local: { model: modelId },\n    onProgress: (progress) => {\n      console.log(`${progress.stage}: ${Math.round(progress.progress * 100)}%`);\n    }\n  });\n\n  await client.init();\n  return client;\n}\n```\n\n### Cache size estimation\n\n```ts\nimport { hasModelInCache } from '@webllm-io/sdk';\n\nasync function estimateCacheSize() {\n  const modelSizes = {\n    'Llama-3.1-8B-Instruct-q4f16_1-MLC': 6000, // ~6GB\n    'Qwen2.5-3B-Instruct-q4f16_1-MLC': 3000,   // ~3GB\n    'Qwen2.5-1.5B-Instruct-q4f16_1-MLC': 1500  // ~1.5GB\n  };\n\n  let totalSize = 0;\n\n  for (const [model, size] of Object.entries(modelSizes)) {\n    const cached = await hasModelInCache(model);\n    if (cached) {\n      totalSize += size;\n      console.log(`${model}: ${size}MB`);\n    }\n  }\n\n  console.log(`Total cache size: ~${totalSize}MB`);\n  return totalSize;\n}\n```\n\n### Selective cache cleanup\n\n```ts\nimport { hasModelInCache, deleteModelFromCache } from '@webllm-io/sdk';\n\nasync function cleanupOldModels(keepModel: string) {\n  const allModels = [\n    'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n    'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n  ];\n\n  for (const model of allModels) {\n    if (model === keepModel) continue;\n\n    const cached = await hasModelInCache(model);\n    if (cached) {\n      await deleteModelFromCache(model);\n      console.log(`Deleted: ${model}`);\n    }\n  }\n}\n\n// Keep only the 1.5B model, delete others\nawait cleanupOldModels('Qwen2.5-1.5B-Instruct-q4f16_1-MLC');\n```\n\n### Cache status dashboard\n\n```tsx\nimport { hasModelInCache, deleteModelFromCache } from '@webllm-io/sdk';\nimport { useState, useEffect } from 'react';\n\ninterface CacheEntry {\n  model: string;\n  cached: boolean;\n  size: number;\n}\n\nfunction CacheDashboard() {\n  const [entries, setEntries] = useState\u003CCacheEntry[]>([]);\n\n  useEffect(() => {\n    loadCacheStatus();\n  }, []);\n\n  async function loadCacheStatus() {\n    const models = [\n      { model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC', size: 6000 },\n      { model: 'Qwen2.5-3B-Instruct-q4f16_1-MLC', size: 3000 },\n      { model: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC', size: 1500 }\n    ];\n\n    const results = await Promise.all(\n      models.map(async ({ model, size }) => ({\n        model,\n        size,\n        cached: await hasModelInCache(model)\n      }))\n    );\n\n    setEntries(results);\n  }\n\n  async function handleDelete(model: string) {\n    await deleteModelFromCache(model);\n    await loadCacheStatus(); // Refresh\n  }\n\n  return (\n    \u003Cdiv>\n      \u003Ch2>Model Cache\u003C/h2>\n      \u003Cul>\n        {entries.map((entry) => (\n          \u003Cli key={entry.model}>\n            \u003Cspan>{entry.model}\u003C/span>\n            \u003Cspan>{entry.cached ? ' Cached' : ' Not cached'}\u003C/span>\n            \u003Cspan>{entry.size}MB\u003C/span>\n            {entry.cached && (\n              \u003Cbutton onClick={() => handleDelete(entry.model)}>\n                Delete\n              \u003C/button>\n            )}\n          \u003C/li>\n        ))}\n      \u003C/ul>\n      \u003Cp>\n        Total: {entries.filter((e) => e.cached).reduce((sum, e) => sum + e.size, 0)}MB\n      \u003C/p>\n    \u003C/div>\n  );\n}\n```\n\n### Switch models with cleanup\n\n```ts\nimport { hasModelInCache, deleteModelFromCache, createClient } from '@webllm-io/sdk';\n\nasync function switchModel(\n  currentClient: WebLLMClient | null,\n  newModelId: string\n) {\n  // Dispose current client\n  if (currentClient) {\n    await currentClient.dispose();\n  }\n\n  // Check if new model is cached\n  const isCached = await hasModelInCache(newModelId);\n\n  if (!isCached) {\n    console.log(`Downloading ${newModelId}...`);\n  }\n\n  // Create new client with new model\n  const newClient = createClient({\n    local: { model: newModelId },\n    onProgress: (progress) => {\n      console.log(`Loading: ${Math.round(progress.progress * 100)}%`);\n    }\n  });\n\n  await newClient.init();\n  return newClient;\n}\n\n// Usage\nlet client = await switchModel(null, 'Llama-3.1-8B-Instruct-q4f16_1-MLC');\n\n// Later: switch to smaller model and delete old one\nconst oldModel = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\nclient = await switchModel(client, 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC');\nawait deleteModelFromCache(oldModel);\n```\n\n### Conditional cache preloading\n\n```ts\nimport { hasModelInCache, checkCapability, createClient } from '@webllm-io/sdk';\n\nasync function smartInitialize() {\n  const cap = await checkCapability();\n\n  // Determine best model for device\n  let modelId: string;\n  switch (cap.grade) {\n    case 'S':\n      modelId = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\n      break;\n    case 'A':\n      modelId = 'Qwen2.5-3B-Instruct-q4f16_1-MLC';\n      break;\n    default:\n      modelId = 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC';\n  }\n\n  // Check cache status\n  const isCached = await hasModelInCache(modelId);\n\n  // Decide whether to use local or cloud\n  const useLocal = isCached || cap.connection.effectiveType === '4g';\n\n  const client = createClient({\n    local: useLocal ? { model: modelId } : false,\n    cloud: !useLocal ? process.env.OPENAI_API_KEY : undefined\n  });\n\n  return client;\n}\n```\n\n## Browser Compatibility\n\nBoth functions require:\n\n- **OPFS support** - Chrome 102+, Edge 102+, Safari 15.2+\n- **`@mlc-ai/web-llm`** installed as a peer dependency\n\nIf `@mlc-ai/web-llm` is not installed:\n- `hasModelInCache()` returns `false`\n- `deleteModelFromCache()` throws an error\n\n## Storage Considerations\n\n### OPFS Storage Limits\n\n- **Chrome/Edge:** ~60% of available disk space\n- **Firefox:** ~50% of available disk space\n- **Safari:** ~1GB default, can request more\n\nCheck available storage:\n\n```ts\nif ('storage' in navigator && 'estimate' in navigator.storage) {\n  const estimate = await navigator.storage.estimate();\n  console.log(`Used: ${estimate.usage}MB`);\n  console.log(`Available: ${estimate.quota}MB`);\n}\n```\n\n### Model Sizes\n\nTypical MLC model sizes:\n\n- Llama-3.1-8B (q4f16_1): ~6GB\n- Qwen2.5-3B (q4f16_1): ~3GB\n- Qwen2.5-1.5B (q4f16_1): ~1.5GB\n- Phi-3.5-mini (q4f16_1): ~2.5GB\n\nPlan storage usage accordingly.\n\n## Implementation Notes\n\n### Delegation to @mlc-ai/web-llm\n\nBoth functions are thin wrappers around `@mlc-ai/web-llm` APIs:\n\n```ts\nimport * as webllm from '@mlc-ai/web-llm';\n\nasync function hasModelInCache(modelId: string): Promise\u003Cboolean> {\n  try {\n    return await webllm.hasModelInCache(modelId);\n  } catch {\n    return false; // If web-llm not installed\n  }\n}\n\nasync function deleteModelFromCache(modelId: string): Promise\u003Cvoid> {\n  return await webllm.deleteModelAllInfoInCache(modelId);\n}\n```\n\n### When to Use\n\n**Use `hasModelInCache()` when:**\n- Showing cache status in UI\n- Deciding whether to download a model\n- Estimating load time\n- Preloading models in the background\n\n**Use `deleteModelFromCache()` when:**\n- User explicitly requests cache cleanup\n- Freeing storage for new models\n- Implementing cache eviction policies\n- Resetting application state\n\n**Do NOT use for:**\n- Automatic cache invalidation (models don't expire)\n- Performance optimization (cache management is already optimized)\n- Detecting model updates (MLC models are immutable)\n\n## See Also\n\n- [createClient()](/api/create-client) - Client initialization with cache options\n- [Config Types](/api/config-types) - `useCache` configuration\n- [WebLLMClient](/api/webllm-client) - Client interface\n- [Local Inference Guide](/guides/local-inference) - OPFS cache usage","src/content/docs/api/cache-management.mdx","f49fa6aa278decfa","index",{"id":25,"data":27,"body":33,"filePath":34,"digest":35,"deferredRender":16},{"title":28,"description":29,"editUrl":16,"head":30,"template":18,"sidebar":31,"pagefind":16,"draft":20},"Welcome to WebLLM.io","The AI Runtime for Every Browser - Smart routing between local WebGPU and cloud inference",[],{"hidden":20,"attrs":32},{},"# Welcome to WebLLM.io\n\n**The AI Runtime for Every Browser**  One unified API that intelligently routes between local WebGPU inference and cloud providers.\n\n## Why WebLLM.io?\n\nWebLLM.io brings AI inference directly to the browser with smart fallback capabilities. It automatically chooses the best execution strategy based on your user's device capabilities.\n\n### Key Features\n\n- **Smart Routing**  Automatically selects local or cloud inference based on device capability\n- **Zero Configuration**  Works out of the box with sensible defaults\n- **WebWorker Isolation**  Runs inference in Web Workers to keep your UI responsive\n- **OPFS Caching**  Efficient model storage using the Origin Private File System\n- **OpenAI Compatible**  Familiar API interface for seamless integration\n- **Progressive Enhancement**  Gracefully falls back to cloud when local inference isn't available\n\n## How It Works\n\nWebLLM.io scores your user's device (S/A/B/C grades based on VRAM) and automatically:\n\n- **High-end devices**  Run powerful models locally via WebGPU\n- **Mid-range devices**  Use lightweight models or cloud fallback\n- **Low-end devices**  Seamlessly route to cloud providers\n\nAll with the same simple API call.\n\n## Quick Links\n\n- [Installation](/getting-started/installation)  Get started in 2 minutes\n- [Quick Start](/getting-started/quick-start)  Your first completion in 5 lines of code\n- [Playground](/getting-started/playground)  Try it live in your browser\n\n## Example\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient({\n  local: 'auto',\n  cloud: { baseURL: 'https://api.openai.com/v1', apiKey: 'sk-...' },\n});\n\nconst result = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello, world!' }],\n});\n\nconsole.log(result.choices[0].message.content);\n```\n\nThat's it. WebLLM.io handles device detection, model selection, WebWorker orchestration, and cloud fallback automatically.\n\n## Browser Support\n\n- **Local Inference**  Chrome 113+, Edge 113+ (WebGPU required)\n- **Cloud Mode**  All modern browsers\n\nReady to get started? Head over to the [Installation Guide](/getting-started/installation).","src/content/docs/index.mdx","a52b70480a515eb6","api/chat-completions",{"id":36,"data":38,"body":44,"filePath":45,"digest":46,"deferredRender":16},{"title":39,"description":40,"editUrl":16,"head":41,"template":18,"sidebar":42,"pagefind":16,"draft":20},"Chat Completions","OpenAI-compatible chat completions API with streaming support and automatic local/cloud routing",[],{"hidden":20,"attrs":43},{},"The chat completions API provides OpenAI-compatible text generation with both streaming and non-streaming modes, automatic provider routing, and abort support.\n\n## Interface\n\n```ts\ninterface Completions {\n  create(req: ChatCompletionRequest & { stream: true }): AsyncIterable\u003CChatCompletionChunk>;\n  create(req: ChatCompletionRequest & { stream?: false }): Promise\u003CChatCompletion>;\n}\n```\n\n## Request\n\n### `ChatCompletionRequest`\n\n```ts\ninterface ChatCompletionRequest {\n  messages: Message[];\n  model?: string;\n  stream?: boolean;\n  temperature?: number;\n  max_tokens?: number;\n  response_format?: { type: 'text' | 'json_object' };\n  signal?: AbortSignal;\n  provider?: 'local' | 'cloud';\n}\n```\n\n#### `messages` (required)\n\nArray of conversation messages in chronological order.\n\n- Type: `Message[]`\n\n```ts\ninterface Message {\n  role: 'system' | 'user' | 'assistant';\n  content: string;\n}\n```\n\n**Example:**\n\n```ts\nmessages: [\n  { role: 'system', content: 'You are a helpful assistant.' },\n  { role: 'user', content: 'What is the capital of France?' },\n  { role: 'assistant', content: 'The capital of France is Paris.' },\n  { role: 'user', content: 'What about Spain?' }\n]\n```\n\n#### `model` (optional)\n\nOverride the model for this request. If not specified, uses the model configured in the provider.\n\n- Type: `string`\n- Default: Provider's default model\n\n**Example:**\n\n```ts\nmodel: 'gpt-4o-mini'  // Cloud\nmodel: 'Llama-3.1-8B-Instruct-q4f16_1-MLC'  // Local\n```\n\n#### `stream` (optional)\n\nEnable streaming mode for real-time token-by-token responses.\n\n- Type: `boolean`\n- Default: `false`\n\n#### `temperature` (optional)\n\nSampling temperature between 0 and 2. Higher values increase randomness.\n\n- Type: `number`\n- Range: `0.0` - `2.0`\n- Default: `1.0`\n\n#### `max_tokens` (optional)\n\nMaximum number of tokens to generate.\n\n- Type: `number`\n- Default: Provider-specific (typically unlimited or model's context limit)\n\n#### `response_format` (optional)\n\nSpecify output format. Use `'json_object'` to enable JSON mode.\n\n- Type: `{ type: 'text' | 'json_object' }`\n- Default: `{ type: 'text' }`\n\n**Example:**\n\n```ts\nresponse_format: { type: 'json_object' }\n```\n\nSee also [`withJsonOutput()`](/api/structured-output) helper.\n\n#### `signal` (optional)\n\nAbortSignal to cancel the request.\n\n- Type: `AbortSignal`\n- Default: `undefined`\n\n**Example:**\n\n```ts\nconst controller = new AbortController();\nconst response = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Long task...' }],\n  signal: controller.signal\n});\n\n// Cancel after 5 seconds\nsetTimeout(() => controller.abort(), 5000);\n```\n\n#### `provider` (optional)\n\nForce a specific provider instead of automatic routing.\n\n- Type: `'local' | 'cloud'`\n- Default: Automatic routing based on availability and device capability\n\n## Response\n\n### Non-streaming (`stream: false`)\n\nReturns a `ChatCompletion` object with the complete generated response.\n\n```ts\ninterface ChatCompletion {\n  id: string;\n  object: 'chat.completion';\n  created: number;\n  model: string;\n  choices: ChatCompletionChoice[];\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\ninterface ChatCompletionChoice {\n  index: number;\n  message: Message;\n  finish_reason: 'stop' | 'length' | 'content_filter' | null;\n}\n```\n\n**Example:**\n\n```ts\nconst response = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n// \"Hello! How can I assist you today?\"\n\nconsole.log(response.model);\n// \"gpt-4o-mini\" or \"Llama-3.1-8B-Instruct-q4f16_1-MLC\"\n\nconsole.log(response.usage);\n// { prompt_tokens: 10, completion_tokens: 8, total_tokens: 18 }\n```\n\n### Streaming (`stream: true`)\n\nReturns an `AsyncIterable\u003CChatCompletionChunk>` for real-time token streaming.\n\n```ts\ninterface ChatCompletionChunk {\n  id: string;\n  object: 'chat.completion.chunk';\n  created: number;\n  model: string;\n  choices: ChatCompletionChunkChoice[];\n}\n\ninterface ChatCompletionChunkChoice {\n  index: number;\n  delta: {\n    role?: 'assistant';\n    content?: string;\n  };\n  finish_reason: 'stop' | 'length' | 'content_filter' | null;\n}\n```\n\n**Example:**\n\n```ts\nconst stream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write a poem about the ocean.' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n## Usage Examples\n\n### Basic completion\n\n```ts\nconst response = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'What is 2 + 2?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// \"2 + 2 equals 4.\"\n```\n\n### Streaming with React\n\n```tsx\nimport { useState } from 'react';\n\nfunction ChatComponent() {\n  const [output, setOutput] = useState('');\n\n  const handleSubmit = async (userMessage: string) => {\n    const stream = await client.chat.completions.create({\n      messages: [{ role: 'user', content: userMessage }],\n      stream: true\n    });\n\n    for await (const chunk of stream) {\n      const content = chunk.choices[0]?.delta?.content || '';\n      setOutput(prev => prev + content);\n    }\n  };\n\n  return \u003Cdiv>{output}\u003C/div>;\n}\n```\n\n### Multi-turn conversation\n\n```ts\nconst messages: Message[] = [\n  { role: 'system', content: 'You are a math tutor.' }\n];\n\n// First turn\nmessages.push({ role: 'user', content: 'What is a prime number?' });\nconst response1 = await client.chat.completions.create({ messages });\nmessages.push(response1.choices[0].message);\n\n// Second turn\nmessages.push({ role: 'user', content: 'Give me examples.' });\nconst response2 = await client.chat.completions.create({ messages });\nmessages.push(response2.choices[0].message);\n```\n\n### JSON output\n\n```ts\nimport { withJsonOutput } from '@webllm-io/sdk';\n\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'user',\n        content: 'List 3 colors in JSON format: {\"colors\": [\"...\", \"...\", \"...\"]}'\n      }\n    ]\n  })\n);\n\nconst data = JSON.parse(response.choices[0].message.content);\nconsole.log(data.colors); // [\"red\", \"blue\", \"green\"]\n```\n\n### Force local or cloud\n\n```ts\n// Force local inference\nconst localResponse = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello' }],\n  provider: 'local'\n});\n\n// Force cloud inference\nconst cloudResponse = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello' }],\n  provider: 'cloud'\n});\n```\n\n### Abort streaming request\n\n```ts\nconst controller = new AbortController();\n\nconst stream = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write a very long story...' }],\n  stream: true,\n  signal: controller.signal\n});\n\nsetTimeout(() => controller.abort(), 3000);\n\ntry {\n  for await (const chunk of stream) {\n    console.log(chunk.choices[0]?.delta?.content);\n  }\n} catch (err) {\n  if (err.code === 'ABORTED') {\n    console.log('Request aborted');\n  }\n}\n```\n\n### Temperature and max_tokens\n\n```ts\n// More creative output\nconst creative = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write a tagline for a coffee shop' }],\n  temperature: 1.5,\n  max_tokens: 20\n});\n\n// More deterministic output\nconst deterministic = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'What is 15 * 23?' }],\n  temperature: 0.0\n});\n```\n\n## Error Handling\n\n```ts\nimport { WebLLMError } from '@webllm-io/sdk';\n\ntry {\n  const response = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Hello' }]\n  });\n} catch (err) {\n  if (err instanceof WebLLMError) {\n    switch (err.code) {\n      case 'INFERENCE_FAILED':\n        console.error('Inference failed:', err.message);\n        break;\n      case 'CLOUD_REQUEST_FAILED':\n        console.error('Cloud API error:', err.message);\n        break;\n      case 'ABORTED':\n        console.log('Request aborted');\n        break;\n      default:\n        console.error('Unknown error:', err);\n    }\n  }\n}\n```\n\n## See Also\n\n- [WebLLMClient](/api/webllm-client) - Client interface\n- [Structured Output](/api/structured-output) - JSON mode helper\n- [Errors](/api/errors) - Error types and codes\n- [Config Types](/api/config-types) - Provider configuration","src/content/docs/api/chat-completions.mdx","6b887e3c1ee6f193","api/check-capability",{"id":47,"data":49,"body":55,"filePath":56,"digest":57,"deferredRender":16},{"title":50,"description":51,"editUrl":16,"head":52,"template":18,"sidebar":53,"pagefind":16,"draft":20},"checkCapability()","Detect device capabilities including WebGPU support, GPU info, VRAM, device grade, and system resources",[],{"hidden":20,"attrs":54},{},"Standalone function to detect device capabilities without creating a client. Returns comprehensive information about WebGPU availability, GPU hardware, estimated VRAM, device performance grade, network connection, battery status, and system memory.\n\n## Signature\n\n```ts\nfunction checkCapability(): Promise\u003CCapabilityReport>;\n```\n\n## Return Value\n\n### `CapabilityReport`\n\n```ts\ninterface CapabilityReport {\n  webgpu: boolean;\n  gpu: GpuInfo | null;\n  grade: DeviceGrade;\n  connection: ConnectionInfo;\n  battery: BatteryInfo | null;\n  memory: number;\n}\n```\n\n#### `webgpu`\n\nWhether WebGPU is available in the current browser.\n\n- Type: `boolean`\n- `true` - WebGPU is supported and accessible\n- `false` - WebGPU is not available (unsupported browser or disabled)\n\n#### `gpu`\n\nGPU hardware information, or `null` if WebGPU is not available.\n\n- Type: `GpuInfo | null`\n\n```ts\ninterface GpuInfo {\n  vendor: string;\n  name: string;\n  vram: number;\n}\n```\n\n**Fields:**\n- `vendor` - GPU vendor (e.g., `'Apple'`, `'NVIDIA'`, `'AMD'`, `'Intel'`)\n- `name` - GPU model name (e.g., `'Apple M3 Pro'`, `'NVIDIA GeForce RTX 4090'`)\n- `vram` - Estimated VRAM in megabytes, derived from `maxStorageBufferBindingSize`\n\n#### `grade`\n\nDevice performance grade based on VRAM estimation.\n\n- Type: `DeviceGrade`\n\n```ts\ntype DeviceGrade = 'S' | 'A' | 'B' | 'C';\n```\n\n**Grades:**\n- `'S'` - High-end (8GB VRAM) - Can run large models like Llama-3.1-8B\n- `'A'` - Mid-high (4GB VRAM) - Can run medium models like Qwen2.5-3B\n- `'B'` - Mid-low (2GB VRAM) - Can run small models like Qwen2.5-1.5B\n- `'C'` - Low-end (&lt;2GB VRAM) - Limited to lightweight models like Qwen2.5-1.5B\n\n**Note:** All grades support local inference. Grade C uses optimized lightweight models.\n\n#### `connection`\n\nNetwork connection information from the Network Information API.\n\n- Type: `ConnectionInfo`\n\n```ts\ninterface ConnectionInfo {\n  effectiveType: '4g' | '3g' | '2g' | 'slow-2g' | 'unknown';\n  downlink?: number;\n  rtt?: number;\n  saveData?: boolean;\n}\n```\n\n**Fields:**\n- `effectiveType` - Connection quality estimate\n- `downlink` - Downlink speed in Mbps (optional)\n- `rtt` - Round-trip time in ms (optional)\n- `saveData` - Whether data saver mode is enabled (optional)\n\n#### `battery`\n\nBattery status from the Battery Status API, or `null` if unavailable.\n\n- Type: `BatteryInfo | null`\n\n```ts\ninterface BatteryInfo {\n  charging: boolean;\n  level: number;\n}\n```\n\n**Fields:**\n- `charging` - Whether the device is charging\n- `level` - Battery level from 0.0 to 1.0\n\n#### `memory`\n\nSystem memory (RAM) in gigabytes, estimated from `navigator.deviceMemory`.\n\n- Type: `number`\n- Falls back to `4` if unavailable\n\n## Examples\n\n### Basic usage\n\n```ts\nimport { checkCapability } from '@webllm-io/sdk';\n\nconst report = await checkCapability();\n\nconsole.log('WebGPU:', report.webgpu);\nconsole.log('Device grade:', report.grade);\n\nif (report.gpu) {\n  console.log(`GPU: ${report.gpu.vendor} ${report.gpu.name}`);\n  console.log(`VRAM: ${report.gpu.vram}MB`);\n}\n```\n\n### Conditional feature enablement\n\n```ts\nconst cap = await checkCapability();\n\nif (!cap.webgpu) {\n  alert('WebGPU not supported. Local inference unavailable.');\n  // Use cloud-only mode\n}\n\nif (cap.grade === 'S' || cap.grade === 'A') {\n  enableLocalInference();\n} else {\n  showCloudFallbackWarning();\n}\n```\n\n### Display device info in UI\n\n```tsx\nimport { checkCapability } from '@webllm-io/sdk';\nimport { useEffect, useState } from 'react';\n\nfunction DeviceInfo() {\n  const [cap, setCap] = useState(null);\n\n  useEffect(() => {\n    checkCapability().then(setCap);\n  }, []);\n\n  if (!cap) return \u003Cdiv>Detecting capabilities...\u003C/div>;\n\n  return (\n    \u003Cdiv>\n      \u003Ch3>Device Information\u003C/h3>\n      \u003Cp>WebGPU: {cap.webgpu ? ' Available' : ' Not available'}\u003C/p>\n      \u003Cp>Grade: {cap.grade}\u003C/p>\n      {cap.gpu && (\n        \u003C>\n          \u003Cp>GPU: {cap.gpu.vendor} {cap.gpu.name}\u003C/p>\n          \u003Cp>VRAM: {cap.gpu.vram}MB\u003C/p>\n        \u003C/>\n      )}\n      \u003Cp>RAM: {cap.memory}GB\u003C/p>\n      \u003Cp>Network: {cap.connection.effectiveType}\u003C/p>\n      {cap.battery && (\n        \u003Cp>Battery: {Math.round(cap.battery.level * 100)}%\n           {cap.battery.charging && ' (charging)'}\n        \u003C/p>\n      )}\n    \u003C/div>\n  );\n}\n```\n\n### Select model based on device grade\n\n```ts\nconst cap = await checkCapability();\n\nlet model: string;\nswitch (cap.grade) {\n  case 'S':\n    model = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\n    break;\n  case 'A':\n    model = 'Qwen2.5-3B-Instruct-q4f16_1-MLC';\n    break;\n  case 'B':\n  case 'C':\n    model = 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC';\n    break;\n}\n\nconst client = createClient({\n  local: { model }\n});\n```\n\n### Adaptive loading strategy\n\n```ts\nconst cap = await checkCapability();\n\n// Use cloud fallback on slow network\nconst useCloud = cap.connection.effectiveType === 'slow-2g' ||\n                 cap.connection.effectiveType === '2g';\n\n// Disable cache on low storage\nconst useCache = cap.grade !== 'C';\n\n// Use WebWorker unless battery is low\nconst useWebWorker = !cap.battery || cap.battery.level > 0.2;\n\nconst client = createClient({\n  local: useCloud ? false : {\n    model: 'auto',\n    useCache,\n    useWebWorker\n  },\n  cloud: useCloud ? process.env.OPENAI_API_KEY : undefined\n});\n```\n\n### Check before showing UI\n\n```ts\nasync function initializeApp() {\n  const cap = await checkCapability();\n\n  if (!cap.webgpu && !cloudAPIKey) {\n    showErrorScreen('WebGPU not supported and no cloud API key configured');\n    return;\n  }\n\n  if (cap.grade === 'C' && cap.connection.effectiveType === 'slow-2g') {\n    showWarning('Slow network detected. Performance may be degraded.');\n  }\n\n  // Proceed with initialization\n  const client = createClient({\n    local: cap.webgpu ? 'auto' : false,\n    cloud: cloudAPIKey\n  });\n}\n```\n\n### Debug logging\n\n```ts\nconst cap = await checkCapability();\n\nconsole.table({\n  'WebGPU': cap.webgpu,\n  'Grade': cap.grade,\n  'Vendor': cap.gpu?.vendor || 'N/A',\n  'GPU': cap.gpu?.name || 'N/A',\n  'VRAM': cap.gpu ? `${cap.gpu.vram}MB` : 'N/A',\n  'RAM': `${cap.memory}GB`,\n  'Network': cap.connection.effectiveType,\n  'Battery': cap.battery ? `${Math.round(cap.battery.level * 100)}%` : 'N/A',\n  'Charging': cap.battery?.charging || false\n});\n```\n\n## Browser Compatibility\n\n- **WebGPU detection:** All browsers (returns `false` if unsupported)\n- **GPU info:** Chrome 113+, Edge 113+, Safari 18+ (requires WebGPU)\n- **Network info:** Chrome, Edge, Opera (partial support)\n- **Battery info:** Chrome, Edge, Opera (deprecated in some browsers)\n- **Memory info:** Chrome 63+, Edge 79+, Opera 50+\n\nUnsupported features gracefully degrade with fallback values.\n\n## See Also\n\n- [createClient()](/api/create-client) - Client creation\n- [WebLLMClient](/api/webllm-client) - Client interface (has `capability()` method)\n- [Config Types](/api/config-types) - Configuration options","src/content/docs/api/check-capability.mdx","e963ff6ab4452abb","faq",{"id":58,"data":60,"body":66,"filePath":67,"digest":68,"deferredRender":16},{"title":61,"description":62,"editUrl":16,"head":63,"template":18,"sidebar":64,"pagefind":16,"draft":20},"FAQ","Frequently asked questions about WebLLM.io",[],{"hidden":20,"attrs":65},{},"## Browser Support\n\n### Which browsers support WebGPU?\n\nWebGPU is required for local inference. Supported browsers:\n\n- **Chrome/Edge 113+** (Windows, macOS, Linux, ChromeOS)\n- **Safari 18+** (macOS, iOS)\n\nYou can check browser support at [caniuse.com/webgpu](https://caniuse.com/webgpu).\n\n:::tip\nFor cloud-only mode, any modern browser works (no WebGPU required).\n:::\n\n### How do I enable WebGPU in my browser?\n\nWebGPU is enabled by default in supported versions. To verify:\n\n```typescript\nimport { checkCapability } from '@webllm-io/sdk';\n\nconst cap = await checkCapability();\nconsole.log('WebGPU available:', cap.webgpu);\n```\n\nIf `false`, ensure you're using a supported browser version.\n\n---\n\n## Dependencies\n\n### Do I need @mlc-ai/web-llm for cloud-only mode?\n\n**No!** When using cloud-only mode, you don't need to install `@mlc-ai/web-llm`:\n\n```json\n{\n  \"dependencies\": {\n    \"@webllm-io/sdk\": \"^1.0.0\"\n    // No @mlc-ai/web-llm needed!\n  }\n}\n```\n\nOnly install `@mlc-ai/web-llm` if you want local inference:\n\n```json\n{\n  \"dependencies\": {\n    \"@webllm-io/sdk\": \"^1.0.0\"\n  },\n  \"peerDependencies\": {\n    \"@mlc-ai/web-llm\": \"^0.2.x\"\n  }\n}\n```\n\n---\n\n## Hardware Requirements\n\n### How much VRAM does my device need?\n\nWebLLM.io supports devices across all grades:\n\n| Grade | VRAM | Model Size | Minimum GPU |\n|-------|------|------------|-------------|\n| **S** | 32GB | Up to 8GB | RTX 4090, M3 Max |\n| **A** | 8GB | ~4.5GB | RTX 3060, M1 Pro |\n| **B** | 4GB | ~2.5GB | RTX 2060, M1 |\n| **C** | 2GB | ~1.5GB | Integrated GPUs |\n\nEven **Grade C** devices (entry-level GPUs) can run local inference using lightweight models like Qwen2.5-1.5B-Instruct.\n\n### How do I check my device's VRAM?\n\n```typescript\nimport { checkCapability } from '@webllm-io/sdk';\n\nconst cap = await checkCapability();\nconsole.log('Estimated VRAM:', cap.vramGB, 'GB');\nconsole.log('Device Grade:', cap.grade);\n```\n\n---\n\n## COOP/COEP Headers\n\n### Why do I need COOP/COEP headers?\n\nLocal inference uses `SharedArrayBuffer` for performance, which requires Cross-Origin-Opener-Policy (COOP) and Cross-Origin-Embedder-Policy (COEP) headers.\n\n### How do I set COOP/COEP headers?\n\n**Vite:**\n\n```typescript\n// vite.config.ts\nimport { defineConfig } from 'vite';\n\nexport default defineConfig({\n  plugins: [\n    {\n      name: 'coop-coep',\n      configureServer(server) {\n        server.middlewares.use((req, res, next) => {\n          res.setHeader('Cross-Origin-Opener-Policy', 'same-origin');\n          res.setHeader('Cross-Origin-Embedder-Policy', 'require-corp');\n          next();\n        });\n      }\n    }\n  ]\n});\n```\n\n**Next.js:**\n\n```javascript\n// next.config.js\nmodule.exports = {\n  async headers() {\n    return [\n      {\n        source: '/(.*)',\n        headers: [\n          { key: 'Cross-Origin-Opener-Policy', value: 'same-origin' },\n          { key: 'Cross-Origin-Embedder-Policy', value: 'require-corp' }\n        ]\n      }\n    ];\n  }\n};\n```\n\n**Express:**\n\n```javascript\napp.use((req, res, next) => {\n  res.setHeader('Cross-Origin-Opener-Policy', 'same-origin');\n  res.setHeader('Cross-Origin-Embedder-Policy', 'require-corp');\n  next();\n});\n```\n\n### What if I can't set COOP/COEP headers?\n\nUse cloud-only mode, which doesn't require `SharedArrayBuffer`:\n\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n```\n\n---\n\n## Cloud Providers\n\n### Can I use a non-OpenAI cloud provider?\n\n**Yes!** Any OpenAI-compatible API works:\n\n**Anthropic Claude:**\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://api.anthropic.com/v1',\n    apiKey: 'sk-ant-...',\n    model: 'claude-3-5-sonnet-20241022'\n  }\n});\n```\n\n**Azure OpenAI:**\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT',\n    apiKey: 'YOUR_AZURE_KEY',\n    model: 'gpt-4'\n  }\n});\n```\n\n**Self-hosted (Ollama, LocalAI):**\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'http://localhost:11434/v1',\n    apiKey: 'not-required',\n    model: 'llama3.1'\n  }\n});\n```\n\n---\n\n## Model Downloads\n\n### How large are the model downloads?\n\nModel sizes vary by device grade:\n\n- **Grade S/A**: ~4.5GB (Llama-3.1-8B)\n- **Grade B**: ~2.5GB (Qwen2.5-3B)\n- **Grade C**: ~1.5GB (Qwen2.5-1.5B)\n\nModels are downloaded once and cached in OPFS (Origin Private File System) for future use.\n\n### How do I check if a model is cached?\n\n```typescript\nimport { hasModelInCache } from '@webllm-io/sdk';\n\nconst modelId = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\nconst isCached = await hasModelInCache(modelId);\n\nif (isCached) {\n  console.log('Model is cached, initialization will be fast!');\n} else {\n  console.log('Model will be downloaded (~4.5GB)');\n}\n```\n\n### Can I delete cached models?\n\n```typescript\nimport { deleteModelFromCache } from '@webllm-io/sdk';\n\nconst modelId = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\nawait deleteModelFromCache(modelId);\nconsole.log('Model removed from cache');\n```\n\n---\n\n## Privacy & Security\n\n### Is my data sent to the cloud in local mode?\n\n**No!** When using local-only mode, all inference happens on your device. No data is transmitted to external servers.\n\n```typescript\n// 100% local, zero network requests for inference\nconst client = await createClient({\n  local: 'auto'\n});\n```\n\n### Where are models stored?\n\nModels are cached in **OPFS (Origin Private File System)**, a secure browser storage API:\n\n- Isolated per origin (domain)\n- Not accessible to other websites\n- Persists across sessions\n- Can be cleared via browser settings\n\n---\n\n## Performance\n\n### Why is first-time initialization slow?\n\nThe first initialization downloads the model (1.5GB - 4.5GB). Subsequent initializations are fast because the model is cached.\n\nShow progress to users:\n\n```typescript\nconst client = await createClient({\n  local: {\n    model: 'auto',\n    onProgress: (report) => {\n      console.log(`${report.text} - ${Math.round(report.progress * 100)}%`);\n    }\n  }\n});\n```\n\n### Does local inference run on the GPU?\n\n**Yes!** Local inference uses WebGPU to run on your GPU, providing hardware-accelerated performance.\n\n### Why does inference block my UI?\n\nBy default, inference runs in a Web Worker to avoid blocking the main thread. If you disabled Web Workers:\n\n```typescript\n// DON'T do this unless debugging\nconst client = await createClient({\n  local: mlc({\n    model: 'auto',\n    useWebWorker: false  //  Blocks main thread\n  })\n});\n```\n\nAlways keep Web Workers enabled for production:\n\n```typescript\n//  Default: runs in Web Worker\nconst client = await createClient({\n  local: 'auto'\n});\n```\n\n---\n\n## Troubleshooting\n\n### \"Failed to create GPUAdapter\" error\n\nThis means WebGPU is not available. Possible causes:\n\n1. **Unsupported browser**  Use Chrome 113+, Edge 113+, or Safari 18+\n2. **GPU drivers outdated**  Update your graphics drivers\n3. **WebGPU disabled**  Check browser flags (should be enabled by default)\n\n**Solution:** Fall back to cloud mode:\n\n```typescript\nimport { checkCapability, createClient } from '@webllm-io/sdk';\n\nconst cap = await checkCapability();\n\nconst client = await createClient({\n  local: cap.webgpu ? 'auto' : undefined,\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n```\n\n### \"SharedArrayBuffer is not defined\" error\n\nCOOP/COEP headers are not set. See [COOP/COEP Headers](#coopcoep-headers) section above.\n\n### Model download fails or times out\n\n1. **Check network connection**  Large downloads require stable internet\n2. **Check storage space**  Ensure sufficient disk space (5GB+ recommended)\n3. **Try a smaller model**  Use Grade C models if limited storage\n4. **Clear cache and retry**  Delete partial downloads\n\n```typescript\nimport { deleteModelFromCache } from '@webllm-io/sdk';\n\nawait deleteModelFromCache('Llama-3.1-8B-Instruct-q4f16_1-MLC');\n// Retry initialization\n```\n\n---\n\n## Getting Help\n\n- **GitHub Issues**: [github.com/WebLLM-io/webllm.io](https://github.com/WebLLM-io/webllm.io/issues)\n- **X (Twitter)**: [@webllm_io](https://x.com/webllm_io)\n- **Documentation**: [webllm.io/docs](https://webllm.io/docs)\n\nFor bug reports, include:\n- Browser version\n- Device grade (from `checkCapability()`)\n- Error messages and stack traces\n- Minimal reproduction code","src/content/docs/faq.mdx","ee3a1e14ed05b298","api/errors",{"id":69,"data":71,"body":77,"filePath":78,"digest":79,"deferredRender":16},{"title":72,"description":73,"editUrl":16,"head":74,"template":18,"sidebar":75,"pagefind":16,"draft":20},"Errors","WebLLMError class and error code reference.",[],{"hidden":20,"attrs":76},{},"All errors thrown by the SDK are instances of `WebLLMError`, which extends the native `Error` class with a typed error code.\n\n## WebLLMError\n\n```ts\nclass WebLLMError extends Error {\n  code: WebLLMErrorCode;\n  cause?: unknown;\n\n  constructor(code: WebLLMErrorCode, message: string, cause?: unknown);\n}\n```\n\n### Properties\n\n| Property | Type | Description |\n|---|---|---|\n| `code` | `WebLLMErrorCode` | Machine-readable error category |\n| `message` | `string` | Human-readable error description |\n| `cause` | `unknown` | Original error that caused this error (if any) |\n| `name` | `string` | Always `'WebLLMError'` |\n\n## Error Codes\n\n```ts\ntype WebLLMErrorCode =\n  | 'WEBGPU_NOT_AVAILABLE'\n  | 'MODEL_LOAD_FAILED'\n  | 'INFERENCE_FAILED'\n  | 'CLOUD_REQUEST_FAILED'\n  | 'NO_PROVIDER_AVAILABLE'\n  | 'ABORTED'\n  | 'TIMEOUT'\n  | 'QUEUE_FULL';\n```\n\n### Reference\n\n| Code | When | Recoverable? |\n|---|---|---|\n| `WEBGPU_NOT_AVAILABLE` | Browser does not support WebGPU or no adapter found | Use cloud fallback |\n| `MODEL_LOAD_FAILED` | Model download, compilation, or initialization failed | Retry or use cloud |\n| `INFERENCE_FAILED` | Local inference produced an error during generation | SDK auto-falls back to cloud if configured |\n| `CLOUD_REQUEST_FAILED` | Cloud API returned an error or network failure | Check API key, URL, connectivity |\n| `NO_PROVIDER_AVAILABLE` | Neither local nor cloud backend is configured or usable | Configure at least one provider |\n| `ABORTED` | Request was cancelled via `AbortSignal` | Intentional  no fallback attempted |\n| `TIMEOUT` | Cloud request exceeded the configured timeout | Increase timeout or retry |\n| `QUEUE_FULL` | Local inference queue is at capacity | Wait and retry |\n\n## Error Handling\n\n### Basic\n\n```ts\nimport { WebLLMError } from '@webllm-io/sdk';\n\ntry {\n  const res = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Hello' }],\n  });\n} catch (err) {\n  if (err instanceof WebLLMError) {\n    switch (err.code) {\n      case 'ABORTED':\n        // User cancelled  ignore\n        break;\n      case 'NO_PROVIDER_AVAILABLE':\n        showSetupInstructions();\n        break;\n      default:\n        showErrorToast(err.message);\n    }\n  }\n}\n```\n\n### Checking the Cause\n\nThe `cause` property preserves the original error for debugging:\n\n```ts\ntry {\n  await client.chat.completions.create({ ... });\n} catch (err) {\n  if (err instanceof WebLLMError && err.code === 'CLOUD_REQUEST_FAILED') {\n    console.error('Cloud error:', err.message);\n    console.error('Original error:', err.cause);\n  }\n}\n```\n\n## Automatic Fallback\n\nThe SDK automatically handles some error scenarios:\n\n- **Local inference fails**  falls back to cloud (if configured)\n- **Cloud fails**  falls back to local (if loaded and ready)\n- **Request aborted**  no fallback (intentional cancellation)\n\nThis means many errors are handled transparently. You only see errors when all fallback options are exhausted.","src/content/docs/api/errors.mdx","dbd515955368a064","api/config-types",{"id":80,"data":82,"body":88,"filePath":89,"digest":90,"deferredRender":16},{"title":83,"description":84,"editUrl":16,"head":85,"template":18,"sidebar":86,"pagefind":16,"draft":20},"Config Types","Type reference for all configuration interfaces.",[],{"hidden":20,"attrs":87},{},"## CreateClientOptions\n\nThe options object passed to `createClient()`.\n\n```ts\ninterface CreateClientOptions {\n  local?: LocalConfig;\n  cloud?: CloudConfig;\n  onProgress?: ProgressCallback;\n}\n```\n\n---\n\n## LocalConfig\n\nConfigures the local inference backend.\n\n```ts\ntype LocalConfig =\n  | 'auto'                              // Auto-detect device, pick model\n  | false | null                        // Disable local inference\n  | string                              // Fixed model ID\n  | LocalObjectConfig                   // Full configuration object\n  | ((stats: DeviceStats) => string | null)  // Dynamic model selector\n  | ResolvedLocalBackend;               // Explicit provider (mlc())\n```\n\n### LocalObjectConfig\n\n```ts\ninterface LocalObjectConfig {\n  tiers?: TiersConfig;\n  model?: string;\n  useCache?: boolean;      // Default: true\n  useWebWorker?: boolean;  // Default: true\n}\n```\n\n---\n\n## CloudConfig\n\nConfigures the cloud inference backend.\n\n```ts\ntype CloudConfig =\n  | string                  // URL shorthand  { baseURL: string }\n  | CloudObjectConfig       // Full configuration object\n  | CloudFn                 // Custom function\n  | ResolvedCloudBackend;   // Explicit provider (fetchSSE())\n```\n\n### CloudObjectConfig\n\n```ts\ninterface CloudObjectConfig {\n  baseURL: string;\n  apiKey?: string;\n  model?: string;\n  headers?: Record\u003Cstring, string>;\n  timeout?: number;   // ms\n  retries?: number;\n}\n```\n\n### CloudFn\n\nA function that handles inference directly.\n\n```ts\ntype CloudFn = (\n  messages: Message[],\n  context: RouteContext,\n) => Promise\u003CChatCompletion> | AsyncIterable\u003CChatCompletionChunk>;\n```\n\n---\n\n## TiersConfig\n\nMaps device grade categories to model IDs.\n\n```ts\ninterface TiersConfig {\n  high?: string | 'auto' | null;    // Grade S/A devices\n  medium?: string | 'auto' | null;  // Grade B devices\n  low?: string | 'auto' | null;     // Grade C devices\n}\n```\n\n- `'auto'`  SDK picks a default model for that tier\n- `null`  Disable local inference for that tier\n- `string`  Specific model ID\n\n---\n\n## LoadProgress\n\nProgress information emitted during model loading.\n\n```ts\ninterface LoadProgress {\n  stage: 'download' | 'compile' | 'warmup';\n  progress: number;        // 0 to 1\n  model: string;           // Model ID being loaded\n  bytesLoaded?: number;\n  bytesTotal?: number;\n}\n```\n\n## ProgressCallback\n\n```ts\ntype ProgressCallback = (progress: LoadProgress) => void;\n```\n\n---\n\n## DeviceStats\n\nHardware information collected during capability detection.\n\n```ts\ninterface DeviceStats {\n  gpu: GpuInfo | null;\n  grade: DeviceGrade;          // 'S' | 'A' | 'B' | 'C'\n  connection: ConnectionInfo;\n  battery: BatteryInfo | null;\n  memory: number;              // navigator.deviceMemory (GB)\n}\n```\n\n---\n\n## CapabilityReport\n\nReturned by `checkCapability()`.\n\n```ts\ninterface CapabilityReport {\n  webgpu: boolean;\n  gpu: GpuInfo | null;\n  grade: DeviceGrade;\n  connection: ConnectionInfo;\n  battery: BatteryInfo | null;\n  memory: number;\n}\n```\n\n---\n\n## Supporting Types\n\n```ts\ntype DeviceGrade = 'S' | 'A' | 'B' | 'C';\n\ninterface GpuInfo {\n  vendor: string;\n  name: string;\n  vram: number;    // Estimated VRAM in MB\n}\n\ninterface ConnectionInfo {\n  type: string;\n  downlink: number;\n  saveData: boolean;\n}\n\ninterface BatteryInfo {\n  level: number;\n  charging: boolean;\n}\n```\n\n---\n\n## ModelLoadState\n\nInternal state tracked by the load manager.\n\n```ts\ninterface ModelLoadState {\n  modelId: string;\n  status: 'idle' | 'downloading' | 'compiling' | 'ready' | 'error';\n  progress: number;\n  error?: Error;\n}\n```\n\n---\n\n## CacheInfo\n\n```ts\ninterface CacheInfo {\n  modelId: string;\n  cached: boolean;\n}\n```","src/content/docs/api/config-types.mdx","7f96ea1ebac092e7","api/create-client",{"id":91,"data":93,"body":99,"filePath":100,"digest":101,"deferredRender":16},{"title":94,"description":95,"editUrl":16,"head":96,"template":18,"sidebar":97,"pagefind":16,"draft":20},"createClient()","Factory function to create a WebLLM client instance with local and cloud inference capabilities",[],{"hidden":20,"attrs":98},{},"Creates a new WebLLM client instance with optional local and cloud configuration.\n\n## Signature\n\n```ts\nfunction createClient(options?: CreateClientOptions): WebLLMClient;\n```\n\n## Parameters\n\n### `options` (optional)\n\nConfiguration object for the client.\n\n```ts\ninterface CreateClientOptions {\n  local?: LocalConfig;\n  cloud?: CloudConfig;\n  onProgress?: ProgressCallback;\n}\n```\n\n#### `local`\n\nConfiguration for local inference using WebGPU and MLC models. See [Config Types](/api/config-types) for all supported formats.\n\n- Type: `LocalConfig`\n- Default: `'auto'`\n- Can be: `'auto'`, `false`, `null`, model string, config object, function, or provider instance\n\n#### `cloud`\n\nConfiguration for cloud inference using OpenAI-compatible APIs. See [Config Types](/api/config-types) for all supported formats.\n\n- Type: `CloudConfig`\n- Default: `undefined`\n- Can be: API key string, config object, custom function, or provider instance\n\n#### `onProgress`\n\nCallback invoked during local model loading with progress updates.\n\n- Type: `ProgressCallback`\n- Default: `undefined`\n\n```ts\ntype ProgressCallback = (progress: LoadProgress) => void;\n\ninterface LoadProgress {\n  stage: 'download' | 'compile' | 'warmup';\n  progress: number;\n  model: string;\n  bytesLoaded?: number;\n  bytesTotal?: number;\n}\n```\n\n## Return Value\n\nReturns a [`WebLLMClient`](/api/webllm-client) instance with chat completions API and lifecycle methods.\n\n## Requirements\n\n**At least one of `local` or `cloud` must be configured.** If both are disabled, the client will throw an error during initialization.\n\n## Examples\n\n### Zero-config (local only)\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient();\n// Uses local inference with auto device-based model selection\n```\n\n### Cloud only\n\n```ts\nconst client = createClient({\n  local: false,\n  cloud: process.env.OPENAI_API_KEY\n});\n```\n\n### Dual engine with progress tracking\n\n```ts\nconst client = createClient({\n  local: {\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      medium: 'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n      low: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n    },\n    useCache: true,\n    useWebWorker: true\n  },\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  },\n  onProgress: (progress) => {\n    console.log(`${progress.stage}: ${Math.round(progress.progress * 100)}%`);\n  }\n});\n```\n\n### Custom provider functions\n\n```ts\nimport { mlc, fetchSSE } from '@webllm-io/sdk/providers';\n\nconst client = createClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useWebWorker: true\n  }),\n  cloud: fetchSSE({\n    baseURL: 'https://api.anthropic.com/v1',\n    apiKey: process.env.ANTHROPIC_API_KEY,\n    model: 'claude-3-5-sonnet-20241022',\n    headers: {\n      'anthropic-version': '2023-06-01'\n    }\n  })\n});\n```\n\n### Dynamic model selection\n\n```ts\nconst client = createClient({\n  local: (stats) => {\n    if (stats.vram > 8000) return 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\n    if (stats.vram > 4000) return 'Qwen2.5-3B-Instruct-q4f16_1-MLC';\n    return 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC';\n  },\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n```\n\n## See Also\n\n- [WebLLMClient](/api/webllm-client) - Client instance methods\n- [Chat Completions](/api/chat-completions) - Inference API\n- [Config Types](/api/config-types) - All configuration options\n- [Providers (MLC)](/api/providers-mlc) - Local inference provider\n- [Providers (Fetch)](/api/providers-fetch) - Cloud inference provider","src/content/docs/api/create-client.mdx","f36741d105f53f33","api/providers-mlc",{"id":102,"data":104,"body":110,"filePath":111,"digest":112,"deferredRender":16},{"title":105,"description":106,"editUrl":16,"head":107,"template":18,"sidebar":108,"pagefind":16,"draft":20},"mlc()","Provider function for local WebGPU inference using MLC Engine with device-adaptive model selection",[],{"hidden":20,"attrs":109},{},"Creates a local inference provider using MLC Engine with WebGPU. Supports device-adaptive model selection via tier configuration, OPFS caching, and WebWorker isolation.\n\n## Import\n\n```ts\nimport { mlc } from '@webllm-io/sdk/providers/mlc';\n```\n\n## Signature\n\n```ts\nfunction mlc(options?: MLCProviderOptions): ResolvedLocalBackend;\n```\n\n## Parameters\n\n### `options` (optional)\n\nConfiguration object for the MLC provider.\n\n```ts\ninterface MLCProviderOptions {\n  model?: string;\n  tiers?: {\n    high?: string | 'auto' | null;\n    medium?: string | 'auto' | null;\n    low?: string | 'auto' | null;\n  };\n  useCache?: boolean;\n  useWebWorker?: boolean;\n}\n```\n\n#### `model` (optional)\n\nFixed model ID to use regardless of device capability. Overrides tier-based selection.\n\n- Type: `string`\n- Default: `undefined` (uses tier-based auto-selection)\n- Example: `'Llama-3.1-8B-Instruct-q4f16_1-MLC'`\n\n#### `tiers` (optional)\n\nDevice-adaptive model mapping by performance grade.\n\n- Type: `TiersConfig`\n\n```ts\ninterface TiersConfig {\n  high?: string | 'auto' | null;\n  medium?: string | 'auto' | null;\n  low?: string | 'auto' | null;\n}\n```\n\n**Tier mapping:**\n- `high` - Used for grade S (8GB VRAM)\n- `medium` - Used for grade A (4GB VRAM)\n- `low` - Used for grades B and C (&lt;4GB VRAM)\n\n**Values:**\n- Model ID string (e.g., `'Llama-3.1-8B-Instruct-q4f16_1-MLC'`)\n- `'auto'` - Use SDK's default model for this tier\n- `null` - Disable local inference for this tier\n\n**Default tiers:**\n```ts\n{\n  high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n  medium: 'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n  low: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n}\n```\n\n#### `useCache` (optional)\n\nEnable OPFS (Origin Private File System) caching for downloaded models.\n\n- Type: `boolean`\n- Default: `true`\n- Recommended: Keep enabled to avoid re-downloading models\n\n#### `useWebWorker` (optional)\n\nRun MLC Engine in a WebWorker to prevent UI blocking during inference.\n\n- Type: `boolean`\n- Default: `true`\n- Recommended: Keep enabled for better UX\n\n## Return Value\n\nReturns a `ResolvedLocalBackend` instance ready for use with `createClient()`.\n\n## Examples\n\n### Basic usage (auto tier selection)\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\nimport { mlc } from '@webllm-io/sdk/providers/mlc';\n\nconst client = createClient({\n  local: mlc()\n});\n// Automatically selects model based on device grade\n```\n\n### Fixed model (no adaptive selection)\n\n```ts\nconst client = createClient({\n  local: mlc({\n    model: 'Qwen2.5-3B-Instruct-q4f16_1-MLC'\n  })\n});\n// Always uses Qwen2.5-3B regardless of device capability\n```\n\n### Custom tier configuration\n\n```ts\nconst client = createClient({\n  local: mlc({\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      medium: 'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n      low: null  // Disable local inference on low-end devices\n    }\n  })\n});\n```\n\n### Disable caching (testing/development)\n\n```ts\nconst client = createClient({\n  local: mlc({\n    useCache: false  // Models will be re-downloaded each time\n  })\n});\n```\n\n### Main thread inference (not recommended)\n\n```ts\nconst client = createClient({\n  local: mlc({\n    useWebWorker: false  // Runs in main thread, may freeze UI\n  })\n});\n```\n\n### Combine with cloud fallback\n\n```ts\nimport { mlc } from '@webllm-io/sdk/providers/mlc';\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch';\n\nconst client = createClient({\n  local: mlc({\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      medium: 'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n      low: null  // Low-end devices fall back to cloud\n    }\n  }),\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  })\n});\n```\n\n### Preload with progress tracking\n\n```ts\nconst client = createClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useCache: true,\n    useWebWorker: true\n  }),\n  onProgress: (progress) => {\n    console.log(`${progress.stage}: ${Math.round(progress.progress * 100)}%`);\n    if (progress.bytesLoaded && progress.bytesTotal) {\n      const mb = (progress.bytesLoaded / 1024 / 1024).toFixed(1);\n      const totalMb = (progress.bytesTotal / 1024 / 1024).toFixed(1);\n      console.log(`Downloaded: ${mb}MB / ${totalMb}MB`);\n    }\n  }\n});\n\nawait client.local.load('Llama-3.1-8B-Instruct-q4f16_1-MLC');\n```\n\n### Device-specific configuration\n\n```ts\nimport { checkCapability } from '@webllm-io/sdk';\n\nconst cap = await checkCapability();\n\nconst client = createClient({\n  local: mlc({\n    model: cap.grade === 'S' ? 'Llama-3.1-8B-Instruct-q4f16_1-MLC' :\n           cap.grade === 'A' ? 'Qwen2.5-3B-Instruct-q4f16_1-MLC' :\n           'Qwen2.5-1.5B-Instruct-q4f16_1-MLC',\n    useCache: cap.grade !== 'C',  // Disable cache on low-end\n    useWebWorker: true\n  })\n});\n```\n\n### Conditional local provider\n\n```ts\nimport { checkCapability } from '@webllm-io/sdk';\n\nconst cap = await checkCapability();\n\nconst client = createClient({\n  local: cap.webgpu ? mlc() : false,\n  cloud: process.env.OPENAI_API_KEY\n});\n```\n\n## Model Compatibility\n\nThe `mlc()` provider works with MLC-compiled models from the [@mlc-ai/web-llm](https://github.com/mlc-ai/web-llm) library.\n\n**Common models:**\n- `Llama-3.1-8B-Instruct-q4f16_1-MLC` (requires ~4.5GB VRAM)\n- `Qwen2.5-3B-Instruct-q4f16_1-MLC` (requires ~2GB VRAM)\n- `Qwen2.5-1.5B-Instruct-q4f16_1-MLC` (requires ~1GB VRAM)\n\nFor a full list of available models, see the [MLC Web LLM model library](https://github.com/mlc-ai/web-llm/blob/main/src/config.ts).\n\n## Performance Notes\n\n- **First load:** Models are downloaded and cached in OPFS (several GB, can take minutes)\n- **Subsequent loads:** Models load from cache (seconds)\n- **WebWorker overhead:** Minimal (~10-20ms per request for message passing)\n- **Main thread mode:** Faster startup but blocks UI during inference\n\n## Requirements\n\n- **Browser:** Chrome 113+, Edge 113+, or Safari 18+ (WebGPU support required)\n- **Headers:** `Cross-Origin-Opener-Policy: same-origin` and `Cross-Origin-Embedder-Policy: require-corp` (for SharedArrayBuffer)\n- **Dependency:** `@mlc-ai/web-llm` must be installed (peer dependency)\n\n## Troubleshooting\n\n### Model loading fails\n\n```ts\n// Check capability first\nconst cap = await checkCapability();\nif (!cap.webgpu) {\n  console.error('WebGPU not available');\n}\n```\n\n### Out of memory errors\n\n```ts\n// Use smaller model or disable cache\nconst client = createClient({\n  local: mlc({\n    model: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC',\n    useCache: false\n  })\n});\n```\n\n### UI freezing during inference\n\n```ts\n// Ensure WebWorker is enabled\nconst client = createClient({\n  local: mlc({\n    useWebWorker: true  // Should be true (default)\n  })\n});\n```\n\n## See Also\n\n- [createClient()](/api/create-client) - Client creation\n- [Config Types](/api/config-types) - All configuration options\n- [Providers (Fetch)](/api/providers-fetch) - Cloud inference provider\n- [checkCapability()](/api/check-capability) - Device detection\n- [Cache Management](/api/cache-management) - OPFS cache utilities","src/content/docs/api/providers-mlc.mdx","3b8ef35c14a7da65","api/providers-fetch",{"id":113,"data":115,"body":121,"filePath":122,"digest":123,"deferredRender":16},{"title":116,"description":117,"editUrl":16,"head":118,"template":18,"sidebar":119,"pagefind":16,"draft":20},"fetchSSE()","Provider function for cloud inference using OpenAI-compatible APIs with SSE streaming support",[],{"hidden":20,"attrs":120},{},"Creates a cloud inference provider using OpenAI-compatible Chat Completions API with Server-Sent Events (SSE) streaming. Supports custom endpoints, headers, timeouts, and automatic retries.\n\n## Import\n\n```ts\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch';\n```\n\n## Signature\n\n```ts\nfunction fetchSSE(options: FetchSSEOptions | string): ResolvedCloudBackend;\n```\n\n## Parameters\n\n### `options`\n\nConfiguration object or API key string.\n\n#### String shorthand\n\nWhen passed a string, it's treated as the API key with default OpenAI endpoint.\n\n```ts\nfetchSSE('sk-...')\n// Equivalent to:\nfetchSSE({\n  baseURL: 'https://api.openai.com/v1',\n  apiKey: 'sk-...',\n  model: 'gpt-4o-mini'\n})\n```\n\n#### Object configuration\n\n```ts\ninterface FetchSSEOptions {\n  baseURL: string;\n  apiKey?: string;\n  model?: string;\n  headers?: Record\u003Cstring, string>;\n  timeout?: number;\n  retries?: number;\n}\n```\n\n##### `baseURL` (required)\n\nBase URL for the Chat Completions API endpoint.\n\n- Type: `string`\n- Must include protocol and path (e.g., `https://api.openai.com/v1`)\n- The SDK appends `/chat/completions` to this URL\n\n**Examples:**\n- OpenAI: `https://api.openai.com/v1`\n- Azure OpenAI: `https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT`\n- Anthropic: `https://api.anthropic.com/v1`\n- Custom: `https://your-api.example.com/v1`\n\n##### `apiKey` (optional)\n\nAPI authentication key. Sent as `Authorization: Bearer \u003CapiKey>` header.\n\n- Type: `string`\n- Default: `undefined`\n- Omit if using custom authentication via `headers`\n\n##### `model` (optional)\n\nDefault model identifier for requests.\n\n- Type: `string`\n- Default: `'gpt-4o-mini'` (for OpenAI)\n- Can be overridden per request via `ChatCompletionRequest.model`\n\n##### `headers` (optional)\n\nCustom HTTP headers for all requests.\n\n- Type: `Record\u003Cstring, string>`\n- Default: `{}`\n- Use for custom authentication, API versioning, or provider-specific headers\n\n**Example:**\n```ts\nheaders: {\n  'anthropic-version': '2023-06-01',\n  'x-custom-header': 'value'\n}\n```\n\n##### `timeout` (optional)\n\nRequest timeout in milliseconds.\n\n- Type: `number`\n- Default: `30000` (30 seconds)\n- Applies to both streaming and non-streaming requests\n\n##### `retries` (optional)\n\nNumber of retry attempts on network or 5xx errors.\n\n- Type: `number`\n- Default: `3`\n- Uses exponential backoff (1s, 2s, 4s, ...)\n\n## Return Value\n\nReturns a `ResolvedCloudBackend` instance ready for use with `createClient()`.\n\n## Examples\n\n### OpenAI (shorthand)\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch';\n\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE(process.env.OPENAI_API_KEY)\n});\n```\n\n### OpenAI (explicit config)\n\n```ts\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o',\n    timeout: 60000,\n    retries: 5\n  })\n});\n```\n\n### Anthropic Claude\n\n```ts\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: 'https://api.anthropic.com/v1',\n    apiKey: process.env.ANTHROPIC_API_KEY,\n    model: 'claude-3-5-sonnet-20241022',\n    headers: {\n      'anthropic-version': '2023-06-01'\n    }\n  })\n});\n```\n\n### Azure OpenAI\n\n```ts\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: `https://${process.env.AZURE_RESOURCE}.openai.azure.com/openai/deployments/${process.env.AZURE_DEPLOYMENT}`,\n    headers: {\n      'api-key': process.env.AZURE_API_KEY\n    },\n    model: 'gpt-4o'\n  })\n});\n```\n\n### Custom OpenAI-compatible API\n\n```ts\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: 'https://api.together.xyz/v1',\n    apiKey: process.env.TOGETHER_API_KEY,\n    model: 'meta-llama/Llama-3.1-8B-Instruct-Turbo',\n    timeout: 120000\n  })\n});\n```\n\n### Local OpenAI-compatible server\n\n```ts\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: 'http://localhost:8000/v1',\n    model: 'llama-3.1-8b'\n  })\n  // No apiKey needed for local server\n});\n```\n\n### Dual provider (local + cloud)\n\n```ts\nimport { mlc } from '@webllm-io/sdk/providers/mlc';\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch';\n\nconst client = createClient({\n  local: mlc(),\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  })\n});\n\n// Uses local by default, cloud as fallback\nconst response = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\n// Force cloud\nconst cloudResponse = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Complex task' }],\n  provider: 'cloud'\n});\n```\n\n### Custom retry and timeout\n\n```ts\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o',\n    timeout: 120000,  // 2 minutes\n    retries: 10       // Retry up to 10 times\n  })\n});\n```\n\n### Environment-based configuration\n\n```ts\nconst isDev = process.env.NODE_ENV === 'development';\n\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: isDev\n      ? 'http://localhost:8000/v1'\n      : 'https://api.openai.com/v1',\n    apiKey: isDev ? undefined : process.env.OPENAI_API_KEY,\n    model: isDev ? 'local-model' : 'gpt-4o-mini',\n    timeout: isDev ? 300000 : 60000  // Longer timeout in dev\n  })\n});\n```\n\n### Per-request model override\n\n```ts\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'  // Default model\n  })\n});\n\n// Use default model (gpt-4o-mini)\nconst response1 = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Simple task' }]\n});\n\n// Override to use gpt-4o\nconst response2 = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Complex task' }],\n  model: 'gpt-4o'\n});\n```\n\n## Streaming Support\n\nThe `fetchSSE()` provider supports both streaming and non-streaming modes using Server-Sent Events (SSE).\n\n```ts\n// Streaming\nconst stream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write a story' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  console.log(chunk.choices[0]?.delta?.content || '');\n}\n\n// Non-streaming\nconst response = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello' }],\n  stream: false\n});\n```\n\n## Error Handling\n\n```ts\nimport { WebLLMError } from '@webllm-io/sdk';\n\ntry {\n  const response = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Hello' }]\n  });\n} catch (err) {\n  if (err instanceof WebLLMError) {\n    switch (err.code) {\n      case 'CLOUD_REQUEST_FAILED':\n        console.error('API request failed:', err.message);\n        console.error('Cause:', err.cause);\n        break;\n      case 'TIMEOUT':\n        console.error('Request timed out');\n        break;\n      case 'ABORTED':\n        console.log('Request aborted');\n        break;\n    }\n  }\n}\n```\n\n## API Compatibility\n\nThe `fetchSSE()` provider implements OpenAI's Chat Completions API format. It should work with any provider that follows this standard, including:\n\n- **OpenAI** - Native support\n- **Anthropic** - Compatible with custom headers\n- **Azure OpenAI** - Compatible with custom base URL\n- **Together AI** - Compatible\n- **Anyscale** - Compatible\n- **Groq** - Compatible\n- **Ollama** - Compatible (with `/v1` endpoint)\n- **LM Studio** - Compatible\n- **LocalAI** - Compatible\n- **vLLM** - Compatible\n\n## Performance Notes\n\n- **Zero dependencies:** SSE parsing is self-implemented (~30 lines), no `openai` SDK dependency\n- **Automatic retries:** Exponential backoff on network/5xx errors\n- **Abort support:** Full AbortSignal support for canceling requests\n- **Streaming:** Real-time token-by-token streaming via SSE\n\n## Requirements\n\n- **Network:** HTTPS connection (or HTTP for localhost)\n- **CORS:** API must allow cross-origin requests (if used in browser)\n- **Format:** API must implement OpenAI Chat Completions API format\n\n## Troubleshooting\n\n### CORS errors\n\nEnsure the API endpoint has CORS headers configured:\n```\nAccess-Control-Allow-Origin: *\nAccess-Control-Allow-Methods: POST, OPTIONS\nAccess-Control-Allow-Headers: Content-Type, Authorization\n```\n\n### Authentication errors\n\n```ts\n// Ensure API key is correct and has proper format\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: 'sk-...',  // Must start with 'sk-'\n  })\n});\n```\n\n### Timeout errors\n\n```ts\n// Increase timeout for slow responses\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    timeout: 300000  // 5 minutes\n  })\n});\n```\n\n## See Also\n\n- [createClient()](/api/create-client) - Client creation\n- [Config Types](/api/config-types) - All configuration options\n- [Providers (MLC)](/api/providers-mlc) - Local inference provider\n- [Chat Completions](/api/chat-completions) - Inference API\n- [Errors](/api/errors) - Error handling","src/content/docs/api/providers-fetch.mdx","09be617806eae18a","examples/basic-chat",{"id":124,"data":126,"body":132,"filePath":133,"digest":134,"deferredRender":16},{"title":127,"description":128,"editUrl":16,"head":129,"template":18,"sidebar":130,"pagefind":16,"draft":20},"Basic Chat","A simple chat example using WebLLM.io with automatic local inference",[],{"hidden":20,"attrs":131},{},"This example demonstrates the simplest way to get started with WebLLM.io. The client will automatically detect your device capabilities and select an appropriate local model.\n\n## Complete Example\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\n// Create a client with automatic local model selection\nconst client = await createClient({\n  local: 'auto'\n});\n\n// Send a chat message\nconst response = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'What is WebGPU?' }\n  ]\n});\n\n// Log the response\nconsole.log(response.choices[0].message.content);\n```\n\n## How It Works\n\n1. **Client Creation**: `createClient({ local: 'auto' })` automatically:\n   - Detects your device's WebGPU capabilities\n   - Assigns a device grade (S/A/B/C)\n   - Selects an appropriate model based on available VRAM\n   - Downloads and caches the model in OPFS (if not already cached)\n\n2. **Inference**: The `chat.completions.create()` call runs entirely in your browser using WebGPU. No data is sent to external servers.\n\n3. **Response**: You receive a complete response object following the OpenAI Chat Completions API format.\n\n## Device Grades and Model Selection\n\nWebLLM.io automatically selects models based on your device grade:\n\n- **Grade S** (32GB VRAM): High-performance models\n- **Grade A** (8GB VRAM): Standard models like Llama-3.1-8B\n- **Grade B** (4GB VRAM): Optimized models\n- **Grade C** (&lt;4GB VRAM): Lightweight models like Qwen2.5-1.5B\n\n## Next Steps\n\n- [Streaming Chat](/examples/streaming-chat)  Stream responses token by token\n- [Loading Progress](/examples/local-only)  Display model download progress\n- [Device Detection](/examples/device-detection)  Check device capabilities before initialization","src/content/docs/examples/basic-chat.mdx","1a92c8bf43b5e159","api/structured-output",{"id":135,"data":137,"body":143,"filePath":144,"digest":145,"deferredRender":16},{"title":138,"description":139,"editUrl":16,"head":140,"template":18,"sidebar":141,"pagefind":16,"draft":20},"withJsonOutput()","Helper function to add JSON output formatting to chat completion requests",[],{"hidden":20,"attrs":142},{},"Simple utility function that adds `response_format: { type: 'json_object' }` to chat completion requests, instructing models to output valid JSON.\n\n## Signature\n\n```ts\nfunction withJsonOutput\u003CT extends ChatCompletionRequest>(req: T): T;\n```\n\n## Parameters\n\n### `req`\n\nChat completion request object to modify.\n\n- Type: `T extends ChatCompletionRequest`\n- Required fields from `ChatCompletionRequest`:\n  - `messages` - Array of chat messages\n  - `model?` - Optional model identifier\n  - `temperature?` - Optional temperature setting\n  - `max_tokens?` - Optional max token limit\n  - Additional OpenAI-compatible parameters\n\n## Return Value\n\nReturns the same request object with `response_format` added:\n\n```ts\n{\n  ...req,\n  response_format: { type: 'json_object' }\n}\n```\n\nThe function performs a shallow merge, preserving all original request properties.\n\n## Type Safety\n\nThe function is generic and preserves the input type:\n\n```ts\nconst req = { messages: [...], temperature: 0.7 };\nconst jsonReq = withJsonOutput(req);\n// jsonReq has type: typeof req & { response_format: { type: 'json_object' } }\n```\n\n## Examples\n\n### Basic usage\n\n```ts\nimport { createClient, withJsonOutput } from '@webllm-io/sdk';\n\nconst client = createClient();\n\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: 'You are a helpful assistant that outputs valid JSON.'\n      },\n      {\n        role: 'user',\n        content: 'Generate a user profile with name, age, and occupation.'\n      }\n    ]\n  })\n);\n\nconst data = JSON.parse(response.choices[0].message.content);\nconsole.log(data);\n// { name: \"John Doe\", age: 30, occupation: \"Engineer\" }\n```\n\n### With streaming\n\n```ts\nconst stream = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'user',\n        content: 'List 3 programming languages as JSON array'\n      }\n    ],\n    stream: true\n  })\n);\n\nlet jsonString = '';\nfor await (const chunk of stream) {\n  jsonString += chunk.choices[0]?.delta?.content || '';\n}\n\nconst languages = JSON.parse(jsonString);\nconsole.log(languages);\n// [\"Python\", \"JavaScript\", \"TypeScript\"]\n```\n\n### Structured data extraction\n\n```ts\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: `Extract structured data from user input.\nOutput format: { \"intent\": string, \"entities\": string[], \"sentiment\": \"positive\" | \"neutral\" | \"negative\" }`\n      },\n      {\n        role: 'user',\n        content: 'I love the new iPhone 15 Pro and its amazing camera!'\n      }\n    ],\n    temperature: 0.3\n  })\n);\n\nconst extracted = JSON.parse(response.choices[0].message.content);\nconsole.log(extracted);\n// {\n//   intent: \"product_review\",\n//   entities: [\"iPhone 15 Pro\", \"camera\"],\n//   sentiment: \"positive\"\n// }\n```\n\n### With custom model settings\n\n```ts\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'user',\n        content: 'Generate 5 random todo items with id, title, completed fields'\n      }\n    ],\n    model: 'gpt-4o-mini',\n    temperature: 0.8,\n    max_tokens: 500\n  })\n);\n\nconst todos = JSON.parse(response.choices[0].message.content);\nconsole.log(todos);\n// [\n//   { id: 1, title: \"Buy groceries\", completed: false },\n//   { id: 2, title: \"Finish report\", completed: true },\n//   ...\n// ]\n```\n\n### TypeScript type narrowing\n\n```ts\ninterface UserProfile {\n  name: string;\n  age: number;\n  email: string;\n}\n\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: `Generate a user profile with this schema:\n{ name: string, age: number, email: string }`\n      },\n      {\n        role: 'user',\n        content: 'Create a profile for a software engineer'\n      }\n    ]\n  })\n);\n\nconst profile: UserProfile = JSON.parse(\n  response.choices[0].message.content\n);\n\nconsole.log(profile.name); // Type-safe access\n```\n\n### Error handling\n\n```ts\ntry {\n  const response = await client.chat.completions.create(\n    withJsonOutput({\n      messages: [\n        {\n          role: 'user',\n          content: 'Generate product catalog as JSON'\n        }\n      ]\n    })\n  );\n\n  const data = JSON.parse(response.choices[0].message.content);\n  console.log(data);\n} catch (error) {\n  if (error instanceof SyntaxError) {\n    console.error('Model returned invalid JSON:', error.message);\n  } else {\n    console.error('Request failed:', error);\n  }\n}\n```\n\n### Combining with other request options\n\n```ts\nimport { withJsonOutput } from '@webllm-io/sdk';\n\nconst baseRequest = {\n  messages: [\n    { role: 'user', content: 'Summarize this article as JSON' }\n  ],\n  temperature: 0.5,\n  max_tokens: 1000,\n  top_p: 0.9,\n  frequency_penalty: 0.2\n};\n\nconst jsonRequest = withJsonOutput(baseRequest);\n\nconsole.log(jsonRequest);\n// {\n//   messages: [...],\n//   temperature: 0.5,\n//   max_tokens: 1000,\n//   top_p: 0.9,\n//   frequency_penalty: 0.2,\n//   response_format: { type: 'json_object' }\n// }\n```\n\n## Important Notes\n\n### JSON Validity\n\nThe `response_format: { type: 'json_object' }` parameter instructs the model to output valid JSON, but it does **not guarantee**:\n\n1. **Schema adherence** - The JSON structure may not match your expected schema\n2. **Field presence** - Required fields may be missing\n3. **Type correctness** - Field types may differ from expectations\n\n**Always validate the parsed JSON** against your schema:\n\n```ts\nimport { z } from 'zod';\n\nconst UserSchema = z.object({\n  name: z.string(),\n  age: z.number(),\n  email: z.string().email()\n});\n\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      { role: 'user', content: 'Generate user profile' }\n    ]\n  })\n);\n\nconst parsed = JSON.parse(response.choices[0].message.content);\nconst validated = UserSchema.parse(parsed); // Throws if invalid\n```\n\n### Prompt Engineering\n\nFor best results, **explicitly describe the JSON schema** in your system prompt:\n\n```ts\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: `You are a JSON-only assistant. Always respond with valid JSON.\nSchema: { \"answer\": string, \"confidence\": number (0-1), \"sources\": string[] }`\n      },\n      {\n        role: 'user',\n        content: 'What is the capital of France?'\n      }\n    ]\n  })\n);\n```\n\n### Model Compatibility\n\n**Not all models support structured output.**\n\n- **OpenAI models:** `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`, `gpt-3.5-turbo` support `response_format`\n- **Local MLC models:** May not respect `response_format` parameter; rely on prompt engineering\n- **Other providers:** Check provider documentation for JSON mode support\n\nIf using an unsupported model, the parameter will be ignored. Ensure your prompt includes instructions for JSON output.\n\n### Alternative: Function Calling\n\nFor stricter schema enforcement, consider using **function calling** (if supported):\n\n```ts\nconst response = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Extract user info from: John Doe, 30, engineer' }\n  ],\n  tools: [\n    {\n      type: 'function',\n      function: {\n        name: 'save_user',\n        description: 'Save user profile',\n        parameters: {\n          type: 'object',\n          properties: {\n            name: { type: 'string' },\n            age: { type: 'number' },\n            occupation: { type: 'string' }\n          },\n          required: ['name', 'age', 'occupation']\n        }\n      }\n    }\n  ],\n  tool_choice: { type: 'function', function: { name: 'save_user' } }\n});\n\nconst args = JSON.parse(\n  response.choices[0].message.tool_calls[0].function.arguments\n);\n```\n\n## Implementation Details\n\nThe function is a simple helper that performs a shallow object merge:\n\n```ts\nfunction withJsonOutput\u003CT extends ChatCompletionRequest>(req: T): T {\n  return {\n    ...req,\n    response_format: { type: 'json_object' }\n  };\n}\n```\n\nThis is equivalent to manually adding the field:\n\n```ts\n// Using withJsonOutput\nconst req = withJsonOutput({ messages: [...] });\n\n// Manual equivalent\nconst req = {\n  messages: [...],\n  response_format: { type: 'json_object' }\n};\n```\n\n## See Also\n\n- [Chat Completions](/api/chat-completions) - Core inference API\n- [createClient()](/api/create-client) - Client initialization\n- [WebLLMClient](/api/webllm-client) - Client interface\n- [JSON Output Example](/examples/json-output) - Complete example usage","src/content/docs/api/structured-output.mdx","10cdfedbdae44dda","api/webllm-client",{"id":146,"data":148,"body":154,"filePath":155,"digest":156,"deferredRender":16},{"title":149,"description":150,"editUrl":16,"head":151,"template":18,"sidebar":152,"pagefind":16,"draft":20},"WebLLMClient","Client interface for managing local and cloud inference, model lifecycle, and device capabilities",[],{"hidden":20,"attrs":153},{},"The main client interface returned by [`createClient()`](/api/create-client). Provides chat completions API, local model management, capability detection, and resource cleanup.\n\n## Interface\n\n```ts\ninterface WebLLMClient {\n  chat: { completions: Completions; };\n  local: {\n    load(modelId: string): Promise\u003Cvoid>;\n    unload(): Promise\u003Cvoid>;\n    isLoaded(): boolean;\n  };\n  capability(): Promise\u003CCapabilityReport>;\n  dispose(): Promise\u003Cvoid>;\n}\n```\n\n## Properties\n\n### `chat.completions`\n\nChat completions API for inference with automatic local/cloud routing.\n\n- Type: [`Completions`](/api/chat-completions)\n- Methods:\n  - `create()` - Generate text completions (streaming or non-streaming)\n\nSee [Chat Completions API](/api/chat-completions) for detailed documentation.\n\n## Methods\n\n### `local.load(modelId)`\n\nExplicitly loads a local MLC model into memory. Useful for preloading models before inference.\n\n**Parameters:**\n- `modelId` (string) - MLC model identifier (e.g., `'Llama-3.1-8B-Instruct-q4f16_1-MLC'`)\n\n**Returns:** `Promise\u003Cvoid>`\n\n**Throws:** `WebLLMError` with code `MODEL_LOAD_FAILED` if loading fails\n\n**Example:**\n\n```ts\nawait client.local.load('Llama-3.1-8B-Instruct-q4f16_1-MLC');\nconsole.log('Model preloaded');\n```\n\n### `local.unload()`\n\nUnloads the currently loaded local model and frees GPU memory.\n\n**Returns:** `Promise\u003Cvoid>`\n\n**Example:**\n\n```ts\nawait client.local.unload();\nconsole.log('Model unloaded, GPU memory freed');\n```\n\n### `local.isLoaded()`\n\nChecks if a local model is currently loaded in memory.\n\n**Returns:** `boolean` - `true` if a model is loaded, `false` otherwise\n\n**Example:**\n\n```ts\nif (client.local.isLoaded()) {\n  console.log('Model ready for inference');\n} else {\n  console.log('No model loaded');\n}\n```\n\n### `capability()`\n\nDetects device capabilities including WebGPU support, GPU info, VRAM, device grade, and system resources.\n\n**Returns:** `Promise\u003CCapabilityReport>`\n\nSee [`checkCapability()`](/api/check-capability) for detailed report format.\n\n**Example:**\n\n```ts\nconst report = await client.capability();\nconsole.log(`Device grade: ${report.grade}`);\nconsole.log(`VRAM: ${report.gpu?.vram}MB`);\nconsole.log(`WebGPU: ${report.webgpu ? 'Available' : 'Not available'}`);\n```\n\n### `dispose()`\n\nCleans up all resources including loaded models, workers, and GPU memory. Call this when the client is no longer needed.\n\n**Returns:** `Promise\u003Cvoid>`\n\n**Example:**\n\n```ts\nawait client.dispose();\nconsole.log('Client disposed, all resources freed');\n```\n\n## Usage Examples\n\n### Basic chat with automatic routing\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient({\n  local: 'auto',\n  cloud: process.env.OPENAI_API_KEY\n});\n\nconst response = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n### Preload model before inference\n\n```ts\nconst client = createClient();\n\n// Preload model\nawait client.local.load('Qwen2.5-3B-Instruct-q4f16_1-MLC');\n\n// Model is already loaded, inference starts immediately\nconst response = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'What is 2+2?' }],\n  provider: 'local'\n});\n```\n\n### Check capabilities and adapt\n\n```ts\nconst client = createClient({\n  local: 'auto',\n  cloud: process.env.OPENAI_API_KEY\n});\n\nconst cap = await client.capability();\n\nif (cap.grade === 'S' || cap.grade === 'A') {\n  console.log('High-end device, using local inference');\n  await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Complex task...' }],\n    provider: 'local'\n  });\n} else {\n  console.log('Low-end device, using cloud fallback');\n  await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Complex task...' }],\n    provider: 'cloud'\n  });\n}\n```\n\n### Resource cleanup on unmount\n\n```ts\n// React example\nuseEffect(() => {\n  const client = createClient();\n\n  return () => {\n    client.dispose();\n  };\n}, []);\n```\n\n### Manual model switching\n\n```ts\nconst client = createClient();\n\n// Load first model\nawait client.local.load('Qwen2.5-1.5B-Instruct-q4f16_1-MLC');\nconst response1 = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Quick question' }],\n  provider: 'local'\n});\n\n// Switch to larger model\nawait client.local.unload();\nawait client.local.load('Llama-3.1-8B-Instruct-q4f16_1-MLC');\nconst response2 = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Complex reasoning task' }],\n  provider: 'local'\n});\n\n// Cleanup\nawait client.dispose();\n```\n\n## See Also\n\n- [createClient()](/api/create-client) - Create client instances\n- [Chat Completions](/api/chat-completions) - Inference API\n- [checkCapability()](/api/check-capability) - Device detection\n- [Errors](/api/errors) - Error handling","src/content/docs/api/webllm-client.mdx","765ccc674ac432d1","examples/cloud-only",{"id":157,"data":159,"body":165,"filePath":166,"digest":167,"deferredRender":16},{"title":160,"description":161,"editUrl":16,"head":162,"template":18,"sidebar":163,"pagefind":16,"draft":20},"Cloud-Only Mode","Use WebLLM.io with cloud providers without any local dependencies",[],{"hidden":20,"attrs":164},{},"Cloud-only mode lets you use WebLLM.io's unified API with cloud providers like OpenAI, without installing the local inference engine (`@mlc-ai/web-llm`).\n\n## Basic Cloud Setup\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: 'sk-...',\n    model: 'gpt-4o-mini'\n  }\n});\n\nconst response = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Explain quantum computing in simple terms.' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n## No Local Dependencies Required\n\nWhen using cloud-only mode, you **do not need** to install `@mlc-ai/web-llm`:\n\n```json\n{\n  \"dependencies\": {\n    \"@webllm-io/sdk\": \"^1.0.0\"\n    // @mlc-ai/web-llm NOT needed for cloud-only\n  }\n}\n```\n\nThe SDK has zero external dependencies for cloud mode  even Server-Sent Events (SSE) parsing is implemented internally.\n\n## Streaming Responses\n\nCloud streaming works identically to local streaming:\n\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\nconst stream = await client.chat.completions.create({\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'Write a haiku about coding.' }\n  ],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const delta = chunk.choices[0]?.delta?.content;\n  if (delta) {\n    process.stdout.write(delta);\n  }\n}\n```\n\n## Cloud Configuration Options\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: 'sk-...',\n    model: 'gpt-4o-mini',\n\n    // Optional: Request timeout (default: 60000ms)\n    timeout: 30000,\n\n    // Optional: Retry attempts (default: 2)\n    retries: 3,\n\n    // Optional: Custom headers\n    headers: {\n      'X-Custom-Header': 'value'\n    }\n  }\n});\n```\n\n## Using Alternative Cloud Providers\n\nAny OpenAI-compatible API works:\n\n### Anthropic Claude (via OpenAI SDK compatibility)\n\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://api.anthropic.com/v1',\n    apiKey: 'sk-ant-...',\n    model: 'claude-3-5-sonnet-20241022',\n    headers: {\n      'anthropic-version': '2023-06-01'\n    }\n  }\n});\n```\n\n### Azure OpenAI\n\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT',\n    apiKey: 'YOUR_AZURE_KEY',\n    model: 'gpt-4',\n    headers: {\n      'api-key': 'YOUR_AZURE_KEY'\n    }\n  }\n});\n```\n\n### Self-Hosted (Ollama, LocalAI, etc.)\n\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'http://localhost:11434/v1',  // Ollama\n    apiKey: 'not-required',  // Some local servers don't need keys\n    model: 'llama3.1'\n  }\n});\n```\n\n## Environment Variables (Recommended)\n\nStore sensitive keys in environment variables:\n\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: import.meta.env.VITE_OPENAI_BASE_URL,\n    apiKey: import.meta.env.VITE_OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n```\n\n```bash\n# .env.local\nVITE_OPENAI_BASE_URL=https://api.openai.com/v1\nVITE_OPENAI_API_KEY=sk-...\n```\n\n:::danger\nNever commit API keys to version control. Use environment variables or secure key management.\n:::\n\n## Error Handling\n\n```typescript\ntry {\n  const response = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Hello!' }]\n  });\n} catch (error) {\n  if (error.message.includes('401')) {\n    console.error('Invalid API key');\n  } else if (error.message.includes('timeout')) {\n    console.error('Request timed out');\n  } else {\n    console.error('Request failed:', error);\n  }\n}\n```\n\n## Cloud-Only Use Cases\n\nCloud-only mode is ideal when:\n\n-  You need access to the latest frontier models (GPT-4, Claude, etc.)\n-  User devices don't support WebGPU\n-  You want to minimize bundle size (no local inference engine)\n-  Your application already has cloud API infrastructure\n-  You need guaranteed consistency across all devices\n\n## Next Steps\n\n- [Hybrid Mode](/examples/hybrid-mode)  Combine local and cloud for automatic fallback\n- [Custom Providers](/guides/custom-providers)  Implement custom cloud logic\n- [API Reference](/api/create-client)  Full configuration options","src/content/docs/examples/cloud-only.mdx","e7c801eb5967e653","examples/device-detection",{"id":168,"data":170,"body":176,"filePath":177,"digest":178,"deferredRender":16},{"title":171,"description":172,"editUrl":16,"head":173,"template":18,"sidebar":174,"pagefind":16,"draft":20},"Device Detection","Detect device capabilities and optimize experience based on hardware",[],{"hidden":20,"attrs":175},{},"Use device detection to provide the best experience for each user based on their hardware capabilities.\n\n## Basic Capability Check\n\n```typescript\nimport { checkCapability } from '@webllm-io/sdk';\n\nconst capability = await checkCapability();\n\nconsole.log('WebGPU available:', capability.webgpu);\nconsole.log('Device grade:', capability.grade);\nconsole.log('Estimated VRAM:', capability.vramGB, 'GB');\nconsole.log('GPU info:', capability.gpu);\n```\n\n## Capability Object Structure\n\n```typescript\n{\n  webgpu: true,           // WebGPU API available\n  grade: 'A',             // Device grade: S, A, B, or C\n  vramGB: 8.2,           // Estimated VRAM in GB\n  gpu: {\n    vendor: 'Apple',\n    architecture: 'M1',\n    maxStorageBufferBindingSize: 8589934592\n  }\n}\n```\n\n## Device Grades\n\nWebLLM.io assigns grades based on `maxStorageBufferBindingSize` (a proxy for VRAM):\n\n| Grade | VRAM Estimate | Recommended Model Size | Example Devices |\n|-------|---------------|------------------------|-----------------|\n| **S** | 32GB | High-end models | M3 Max, RTX 4090 |\n| **A** | 8GB | Standard models (8B params) | M1 Pro, RTX 3060 |\n| **B** | 4GB | Optimized models (4B params) | M1, RTX 2060 |\n| **C** | &lt;4GB | Lightweight models (1.5B params) | Integrated GPUs |\n\n:::note\nAll grades support local inference. Grade C devices use lightweight models like Qwen2.5-1.5B-Instruct.\n:::\n\n## Conditional Feature Enabling\n\n```typescript\nimport { checkCapability, createClient } from '@webllm-io/sdk';\n\nconst capability = await checkCapability();\n\nif (!capability.webgpu) {\n  // WebGPU not available, cloud-only mode\n  console.warn('Local inference not available, using cloud');\n\n  const client = await createClient({\n    cloud: {\n      baseURL: 'https://api.openai.com/v1',\n      apiKey: process.env.OPENAI_API_KEY,\n      model: 'gpt-4o-mini'\n    }\n  });\n} else {\n  // WebGPU available, enable local\n  console.log(`Device grade ${capability.grade} detected`);\n\n  const client = await createClient({\n    local: 'auto'\n  });\n}\n```\n\n## Display Capability Info to Users\n\n```typescript\nimport { checkCapability } from '@webllm-io/sdk';\n\nasync function displayDeviceInfo() {\n  const cap = await checkCapability();\n\n  const infoElement = document.getElementById('device-info');\n\n  if (!cap.webgpu) {\n    infoElement.innerHTML = `\n      \u003Cdiv class=\"alert alert-warning\">\n        \u003Cstrong>WebGPU Not Available\u003C/strong>\n        \u003Cp>Your browser doesn't support WebGPU. Update to Chrome 113+ or Edge 113+.\u003C/p>\n        \u003Cp>Falling back to cloud mode.\u003C/p>\n      \u003C/div>\n    `;\n    return;\n  }\n\n  infoElement.innerHTML = `\n    \u003Cdiv class=\"device-info\">\n      \u003Ch3>Device Capabilities\u003C/h3>\n      \u003Cul>\n        \u003Cli>\u003Cstrong>GPU:\u003C/strong> ${cap.gpu.vendor} ${cap.gpu.architecture || ''}\u003C/li>\n        \u003Cli>\u003Cstrong>VRAM:\u003C/strong> ~${cap.vramGB.toFixed(1)} GB\u003C/li>\n        \u003Cli>\u003Cstrong>Grade:\u003C/strong> ${cap.grade} (${getGradeDescription(cap.grade)})\u003C/li>\n        \u003Cli>\u003Cstrong>Local Inference:\u003C/strong>  Enabled\u003C/li>\n      \u003C/ul>\n    \u003C/div>\n  `;\n}\n\nfunction getGradeDescription(grade: string): string {\n  const descriptions = {\n    'S': 'High-end GPU',\n    'A': 'Standard GPU',\n    'B': 'Mid-range GPU',\n    'C': 'Entry-level GPU'\n  };\n  return descriptions[grade] || 'Unknown';\n}\n\ndisplayDeviceInfo();\n```\n\n## Warn About Large Downloads\n\n```typescript\nimport { checkCapability, hasModelInCache } from '@webllm-io/sdk';\n\nasync function initializeWithWarning() {\n  const capability = await checkCapability();\n\n  if (!capability.webgpu) {\n    return initCloudMode();\n  }\n\n  // Estimate model size based on grade\n  const modelSizes = {\n    'S': '8GB',\n    'A': '4.5GB',\n    'B': '2.5GB',\n    'C': '1.5GB'\n  };\n\n  const estimatedSize = modelSizes[capability.grade];\n  const modelId = getModelForGrade(capability.grade);\n\n  const isCached = await hasModelInCache(modelId);\n\n  if (!isCached) {\n    const userConfirmed = confirm(\n      `First-time setup will download ~${estimatedSize} of model data. ` +\n      `The model will be cached for future use. Continue?`\n    );\n\n    if (!userConfirmed) {\n      return initCloudMode();\n    }\n  }\n\n  return initLocalMode();\n}\n\nfunction getModelForGrade(grade: string): string {\n  const models = {\n    'S': 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    'A': 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    'B': 'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n    'C': 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n  };\n  return models[grade] || models['C'];\n}\n```\n\n## Adaptive UI Based on Grade\n\n```typescript\nimport { checkCapability, createClient } from '@webllm-io/sdk';\n\nasync function initializeApp() {\n  const capability = await checkCapability();\n\n  // Adapt max tokens based on device grade\n  const maxTokensLimits = {\n    'S': 4096,\n    'A': 2048,\n    'B': 1024,\n    'C': 512\n  };\n\n  const maxTokens = maxTokensLimits[capability.grade] || 512;\n\n  // Update UI\n  const tokenSlider = document.getElementById('max-tokens-slider') as HTMLInputElement;\n  tokenSlider.max = maxTokens.toString();\n  tokenSlider.value = Math.min(parseInt(tokenSlider.value), maxTokens).toString();\n\n  document.getElementById('max-tokens-label').textContent =\n    `Max Tokens (up to ${maxTokens} on your device)`;\n\n  // Initialize client\n  const client = await createClient({\n    local: capability.webgpu ? 'auto' : undefined,\n    cloud: {\n      baseURL: 'https://api.openai.com/v1',\n      apiKey: process.env.OPENAI_API_KEY,\n      model: 'gpt-4o-mini'\n    }\n  });\n\n  return { client, maxTokens };\n}\n```\n\n## Browser Compatibility Check\n\n```typescript\nfunction checkBrowserCompatibility() {\n  const isChrome = /Chrome/.test(navigator.userAgent);\n  const isEdge = /Edg/.test(navigator.userAgent);\n  const isSafari = /Safari/.test(navigator.userAgent) && !/Chrome/.test(navigator.userAgent);\n\n  // Chrome 113+ or Edge 113+ required for WebGPU\n  if (!isChrome && !isEdge && !isSafari) {\n    return {\n      compatible: false,\n      message: 'Please use Chrome 113+, Edge 113+, or Safari 18+ for local inference.'\n    };\n  }\n\n  if (!navigator.gpu) {\n    return {\n      compatible: false,\n      message: 'WebGPU is not available in this browser version.'\n    };\n  }\n\n  return { compatible: true };\n}\n\nconst browserCheck = checkBrowserCompatibility();\nif (!browserCheck.compatible) {\n  console.warn(browserCheck.message);\n  // Show upgrade prompt to user\n}\n```\n\n## Complete Detection Example\n\n```typescript\nimport { checkCapability, createClient, hasModelInCache } from '@webllm-io/sdk';\n\nasync function smartInitialize() {\n  // Step 1: Check WebGPU availability\n  const capability = await checkCapability();\n\n  if (!capability.webgpu) {\n    console.log('WebGPU not available, using cloud-only mode');\n    return await createClient({\n      cloud: {\n        baseURL: 'https://api.openai.com/v1',\n        apiKey: process.env.OPENAI_API_KEY,\n        model: 'gpt-4o-mini'\n      }\n    });\n  }\n\n  // Step 2: Display device info\n  console.log(`Device Grade: ${capability.grade}`);\n  console.log(`Estimated VRAM: ${capability.vramGB.toFixed(1)}GB`);\n  console.log(`GPU: ${capability.gpu.vendor}`);\n\n  // Step 3: Check cache status\n  const modelId = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\n  const cached = await hasModelInCache(modelId);\n\n  if (!cached) {\n    console.log('Model not cached, will download on first use (~4.5GB)');\n  }\n\n  // Step 4: Initialize with progress tracking\n  const client = await createClient({\n    local: {\n      model: 'auto',\n      onProgress: (report) => {\n        console.log(`[${Math.round(report.progress * 100)}%] ${report.text}`);\n      }\n    },\n    cloud: {\n      baseURL: 'https://api.openai.com/v1',\n      apiKey: process.env.OPENAI_API_KEY,\n      model: 'gpt-4o-mini'\n    }\n  });\n\n  return client;\n}\n\nconst client = await smartInitialize();\n```\n\n## Next Steps\n\n- [Local-Only Mode](/examples/local-only)  Configure local inference\n- [Hybrid Mode](/examples/hybrid-mode)  Combine local and cloud with auto-routing\n- [Cache Management](/guides/cache-management)  Manage downloaded models","src/content/docs/examples/device-detection.mdx","e1b803897ecc7181","examples/hybrid-mode",{"id":179,"data":181,"body":187,"filePath":188,"digest":189,"deferredRender":16},{"title":182,"description":183,"editUrl":16,"head":184,"template":18,"sidebar":185,"pagefind":16,"draft":20},"Hybrid Mode","Combine local and cloud inference for automatic fallback and routing",[],{"hidden":20,"attrs":186},{},"Hybrid mode gives you the best of both worlds: privacy and performance with local inference, with automatic fallback to cloud when needed.\n\n## Basic Hybrid Setup\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = await createClient({\n  // Local configuration\n  local: {\n    model: 'auto',  // Auto-select based on device\n    onProgress: (report) => {\n      console.log(`Loading: ${report.text}`);\n    }\n  },\n\n  // Cloud fallback\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\n// Automatically uses local if available, falls back to cloud if needed\nconst response = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'What is machine learning?' }\n  ]\n});\n\nconsole.log('Model used:', response.model);\nconsole.log('Response:', response.choices[0].message.content);\n```\n\n## How Routing Works\n\nWebLLM.io automatically decides between local and cloud based on:\n\n1. **Device capability**  If WebGPU is not available, route to cloud\n2. **Model availability**  If requested model is not available locally, route to cloud\n3. **Request parameters**  Some parameters may only be supported by cloud\n4. **User preferences**  Explicit provider selection overrides auto-routing\n\n## Explicit Provider Selection\n\nYou can force a specific provider for any request:\n\n```typescript\n// Force local inference\nconst localResponse = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }],\n  provider: 'local'  // Use local MLC engine\n});\n\n// Force cloud inference\nconst cloudResponse = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }],\n  provider: 'cloud'  // Use cloud API\n});\n```\n\n## Graceful Degradation\n\nHandle scenarios where local is not available:\n\n```typescript\nimport { createClient, checkCapability } from '@webllm-io/sdk';\n\nconst capability = await checkCapability();\n\nconst client = await createClient({\n  local: capability.webgpu ? 'auto' : undefined,\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\nif (!capability.webgpu) {\n  console.warn('WebGPU not available, using cloud-only mode');\n}\n\nconst response = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Explain neural networks.' }]\n});\n```\n\n## Routing Based on Request Complexity\n\nRoute simple queries locally, complex ones to cloud:\n\n```typescript\nasync function chat(userMessage: string) {\n  const isComplexQuery = userMessage.length > 500 ||\n                         userMessage.includes('code') ||\n                         userMessage.includes('analyze');\n\n  return await client.chat.completions.create({\n    messages: [\n      { role: 'user', content: userMessage }\n    ],\n    provider: isComplexQuery ? 'cloud' : 'local'\n  });\n}\n```\n\n## Streaming with Hybrid Mode\n\nStreaming works seamlessly with both providers:\n\n```typescript\nconst stream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write a short story.' }],\n  stream: true\n  // No provider specified = auto-route\n});\n\nconsole.log('Streaming from provider...');\n\nfor await (const chunk of stream) {\n  // First chunk tells you which provider is being used\n  if (chunk.choices[0]?.delta?.role) {\n    console.log('Using model:', chunk.model);\n  }\n\n  const delta = chunk.choices[0]?.delta?.content;\n  if (delta) {\n    process.stdout.write(delta);\n  }\n}\n```\n\n## Cost Optimization Strategy\n\nUse local for frequent queries, cloud for high-quality needs:\n\n```typescript\nconst client = await createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\n// Free local inference for drafts\nconst draft = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Draft an email about project status.' }\n  ],\n  provider: 'local'\n});\n\n// High-quality cloud model for final version\nconst final = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Polish this email:\\n\\n' + draft.choices[0].message.content }\n  ],\n  provider: 'cloud',\n  temperature: 0.3\n});\n```\n\n## Monitoring Provider Usage\n\nTrack which provider is being used:\n\n```typescript\nlet localRequests = 0;\nlet cloudRequests = 0;\n\nasync function monitoredChat(messages: Message[]) {\n  const response = await client.chat.completions.create({ messages });\n\n  // Check which model was used\n  if (response.model.includes('MLC')) {\n    localRequests++;\n    console.log('Local request #', localRequests);\n  } else {\n    cloudRequests++;\n    console.log('Cloud request #', cloudRequests);\n  }\n\n  return response;\n}\n\n// After some usage\nconsole.log(`Local: ${localRequests}, Cloud: ${cloudRequests}`);\nconsole.log(`Cost savings: ${localRequests * 0.001}$ (approx)`);\n```\n\n## Advanced: Custom Routing Logic\n\nImplement custom routing with provider composition:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\nimport { mlc } from '@webllm-io/sdk/providers/mlc';\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch';\n\n// Custom wrapper with intelligent routing\nfunction smartRouter(localProvider, cloudProvider) {\n  return async (request) => {\n    // Route long contexts to cloud (better quality)\n    const totalTokens = request.messages.reduce(\n      (sum, msg) => sum + msg.content.length / 4,\n      0\n    );\n\n    if (totalTokens > 2000) {\n      console.log('Long context detected, using cloud');\n      return cloudProvider(request);\n    }\n\n    // Route local by default\n    try {\n      return await localProvider(request);\n    } catch (error) {\n      console.warn('Local failed, falling back to cloud:', error);\n      return cloudProvider(request);\n    }\n  };\n}\n\nconst client = await createClient({\n  local: mlc({ model: 'auto' }),\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  })\n});\n```\n\n## Hybrid Mode Benefits\n\n-  **Privacy when possible**  Use local for sensitive data\n-  **Reliability**  Cloud fallback ensures availability\n-  **Cost efficiency**  Reduce cloud API costs with local inference\n-  **Performance**  Local is faster (no network latency)\n-  **Flexibility**  Choose the best provider per request\n\n## Next Steps\n\n- [Device Detection](/examples/device-detection)  Detect capabilities to optimize routing\n- [Custom Providers](/guides/custom-providers)  Build advanced routing logic\n- [Cache Management](/guides/cache-management)  Optimize local model storage","src/content/docs/examples/hybrid-mode.mdx","979930bde1d95821","examples/json-output",{"id":190,"data":192,"body":198,"filePath":199,"digest":200,"deferredRender":16},{"title":193,"description":194,"editUrl":16,"head":195,"template":18,"sidebar":196,"pagefind":16,"draft":20},"JSON Output","Generate structured JSON responses with type-safe parsing",[],{"hidden":20,"attrs":197},{},"WebLLM.io provides helpers for generating structured JSON output, useful for extracting data, function calling, and building structured workflows.\n\n## Basic JSON Output\n\n```typescript\nimport { createClient, withJsonOutput } from '@webllm-io/sdk';\n\nconst client = await createClient({\n  local: 'auto'\n});\n\n// Request JSON-formatted response\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: 'You are a helpful assistant that extracts information and returns valid JSON.'\n      },\n      {\n        role: 'user',\n        content: 'Extract the name, age, and city from: \"John is 25 years old and lives in Paris.\"'\n      }\n    ]\n  })\n);\n\n// Parse JSON response\nconst data = JSON.parse(response.choices[0].message.content);\nconsole.log(data);\n// { name: \"John\", age: 25, city: \"Paris\" }\n```\n\n## What Does `withJsonOutput()` Do?\n\nThe `withJsonOutput()` helper:\n\n1. Adds instructions to the system message to return valid JSON\n2. Sets `response_format: { type: 'json_object' }` for compatible providers\n3. Ensures the model outputs parseable JSON\n\n## Structured Extraction Example\n\n```typescript\nimport { createClient, withJsonOutput } from '@webllm-io/sdk';\n\ninterface Product {\n  name: string;\n  price: number;\n  category: string;\n  inStock: boolean;\n}\n\nconst client = await createClient({ local: 'auto' });\n\nconst productText = `\n  The new iPhone 15 Pro costs $999 and is available now.\n  Category: Electronics\n`;\n\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: `Extract product information and return as JSON with fields:\n                  name (string), price (number), category (string), inStock (boolean)`\n      },\n      {\n        role: 'user',\n        content: productText\n      }\n    ]\n  })\n);\n\nconst product: Product = JSON.parse(response.choices[0].message.content);\nconsole.log(product);\n// {\n//   name: \"iPhone 15 Pro\",\n//   price: 999,\n//   category: \"Electronics\",\n//   inStock: true\n// }\n```\n\n## Array Responses\n\n```typescript\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: 'Extract all mentioned cities as a JSON array of objects with \"name\" and \"country\" fields.'\n      },\n      {\n        role: 'user',\n        content: 'I visited Paris in France, Tokyo in Japan, and New York in the USA.'\n      }\n    ]\n  })\n);\n\nconst cities = JSON.parse(response.choices[0].message.content);\nconsole.log(cities);\n// [\n//   { name: \"Paris\", country: \"France\" },\n//   { name: \"Tokyo\", country: \"Japan\" },\n//   { name: \"New York\", country: \"USA\" }\n// ]\n```\n\n## Error Handling\n\nAlways validate JSON parsing:\n\n```typescript\ntry {\n  const response = await client.chat.completions.create(\n    withJsonOutput({\n      messages: [\n        { role: 'system', content: 'Return JSON with \"summary\" and \"keywords\" fields.' },\n        { role: 'user', content: 'Summarize: AI is transforming software development.' }\n      ]\n    })\n  );\n\n  const data = JSON.parse(response.choices[0].message.content);\n\n  // Validate structure\n  if (!data.summary || !Array.isArray(data.keywords)) {\n    throw new Error('Invalid JSON structure');\n  }\n\n  console.log('Summary:', data.summary);\n  console.log('Keywords:', data.keywords);\n\n} catch (error) {\n  console.error('JSON parsing failed:', error);\n  // Handle error (retry, use fallback, etc.)\n}\n```\n\n## Type-Safe JSON with Zod\n\nUse Zod for runtime validation:\n\n```typescript\nimport { z } from 'zod';\nimport { createClient, withJsonOutput } from '@webllm-io/sdk';\n\nconst PersonSchema = z.object({\n  name: z.string(),\n  age: z.number().int().positive(),\n  email: z.string().email(),\n  interests: z.array(z.string())\n});\n\ntype Person = z.infer\u003Ctypeof PersonSchema>;\n\nconst client = await createClient({ local: 'auto' });\n\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: `Extract person information as JSON with fields:\n                  name, age, email, interests (array of strings)`\n      },\n      {\n        role: 'user',\n        content: 'Sarah is 28, email sarah@example.com, loves hiking and photography'\n      }\n    ]\n  })\n);\n\nconst rawData = JSON.parse(response.choices[0].message.content);\nconst person: Person = PersonSchema.parse(rawData);  // Validates at runtime\n\nconsole.log(person);\n// Type-safe and validated!\n```\n\n## Function Calling Pattern\n\nSimulate function calling with JSON output:\n\n```typescript\ninterface FunctionCall {\n  function: string;\n  arguments: Record\u003Cstring, any>;\n}\n\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: `You have access to the following functions:\n                  - getWeather(city: string)\n                  - sendEmail(to: string, subject: string, body: string)\n\n                  Return a JSON object with \"function\" and \"arguments\" fields.`\n      },\n      {\n        role: 'user',\n        content: 'What\\'s the weather like in Tokyo?'\n      }\n    ]\n  })\n);\n\nconst functionCall: FunctionCall = JSON.parse(response.choices[0].message.content);\nconsole.log(functionCall);\n// {\n//   function: \"getWeather\",\n//   arguments: { city: \"Tokyo\" }\n// }\n\n// Execute the function\nif (functionCall.function === 'getWeather') {\n  const weather = await getWeather(functionCall.arguments.city);\n  console.log(weather);\n}\n```\n\n## Streaming JSON (Advanced)\n\nFor streaming JSON responses, collect all chunks first:\n\n```typescript\nconst stream = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      { role: 'system', content: 'Return a JSON array of 3 recipe suggestions.' },\n      { role: 'user', content: 'I have chicken, rice, and broccoli.' }\n    ],\n    stream: true\n  })\n);\n\nlet fullContent = '';\n\nfor await (const chunk of stream) {\n  const delta = chunk.choices[0]?.delta?.content;\n  if (delta) {\n    fullContent += delta;\n  }\n}\n\n// Parse complete JSON\nconst recipes = JSON.parse(fullContent);\nconsole.log(recipes);\n```\n\n## Complex Nested Structures\n\n```typescript\ninterface Article {\n  title: string;\n  author: string;\n  publishedDate: string;\n  sections: {\n    heading: string;\n    paragraphs: string[];\n  }[];\n  tags: string[];\n}\n\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'system',\n        content: `Generate an article outline as JSON with:\n                  title, author, publishedDate,\n                  sections (array of {heading, paragraphs}),\n                  tags (array of strings)`\n      },\n      {\n        role: 'user',\n        content: 'Create an outline for an article about WebGPU in browsers'\n      }\n    ]\n  })\n);\n\nconst article: Article = JSON.parse(response.choices[0].message.content);\nconsole.log(article.sections.map(s => s.heading));\n```\n\n## Cloud Provider Compatibility\n\nOpenAI and compatible providers support `response_format`:\n\n```typescript\nconst client = await createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\n// Works seamlessly with cloud providers\nconst response = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      { role: 'system', content: 'Return JSON with \"result\" field.' },\n      { role: 'user', content: 'Calculate 15% tip on $87.50' }\n    ]\n  })\n);\n\nconst data = JSON.parse(response.choices[0].message.content);\nconsole.log('Tip amount:', data.result);\n```\n\n## Best Practices\n\n1. **Clear schema in system message**  Specify exact field names and types\n2. **Validate JSON**  Always use try-catch with JSON.parse()\n3. **Use type guards**  Runtime validation with Zod, io-ts, or custom validators\n4. **Handle malformed output**  Some models may occasionally return invalid JSON\n5. **Provide examples**  Include JSON examples in the prompt for better results\n\n## Next Steps\n\n- [API Reference](/api/with-json-output)  Full `withJsonOutput` documentation\n- [Streaming Chat](/examples/streaming-chat)  Stream structured output\n- [Custom Providers](/guides/custom-providers)  Implement custom JSON parsing logic","src/content/docs/examples/json-output.mdx","3666c4cd8e34db7a","examples/local-only",{"id":201,"data":203,"body":209,"filePath":210,"digest":211,"deferredRender":16},{"title":204,"description":205,"editUrl":16,"head":206,"template":18,"sidebar":207,"pagefind":16,"draft":20},"Local-Only Mode","Run AI inference entirely in the browser with no cloud dependencies",[],{"hidden":20,"attrs":208},{},"Local-only mode ensures complete privacy by running all inference on the user's device. No data is ever sent to external servers.\n\n## Basic Local-Only Setup\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = await createClient({\n  local: 'auto'  // No cloud configuration\n});\n\nconst response = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n## Showing Model Download Progress\n\nWhen a model is loaded for the first time, it needs to be downloaded. Display progress to users:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\n// Create progress UI elements\nconst progressBar = document.getElementById('progress-bar');\nconst statusText = document.getElementById('status-text');\n\nconst client = await createClient({\n  local: {\n    model: 'auto',  // Auto-select based on device\n    onProgress: (report) => {\n      // Update progress UI\n      const percent = Math.round(report.progress * 100);\n      progressBar.style.width = `${percent}%`;\n      statusText.textContent = report.text;\n\n      console.log(`${report.text} - ${percent}%`);\n    }\n  }\n});\n\n// Hide progress UI once loaded\nprogressBar.parentElement.style.display = 'none';\n\n// Client is now ready to use\nconst response = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n## Progress Report Structure\n\nThe `onProgress` callback receives reports with the following structure:\n\n```typescript\n{\n  progress: 0.75,      // 0.0 to 1.0\n  text: \"Loading model weights...\",\n  // Additional fields may vary by stage\n}\n```\n\n## Explicit Model Selection\n\nInstead of `'auto'`, you can specify an exact model:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\nimport { mlc } from '@webllm-io/sdk/providers/mlc';\n\nconst client = await createClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    onProgress: (report) => {\n      console.log(report.text, `${Math.round(report.progress * 100)}%`);\n    }\n  })\n});\n```\n\n## Disable Web Worker (Advanced)\n\nBy default, inference runs in a Web Worker. You can disable this for debugging:\n\n```typescript\nconst client = await createClient({\n  local: mlc({\n    model: 'auto',\n    useWebWorker: false  // Run in main thread (may block UI)\n  })\n});\n```\n\n:::caution\nRunning inference in the main thread can freeze the UI during generation. Only disable Web Workers for debugging purposes.\n:::\n\n## Check Model Cache Before Initialization\n\nAvoid unnecessary downloads by checking if a model is already cached:\n\n```typescript\nimport { hasModelInCache } from '@webllm-io/sdk';\n\nconst modelId = 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\n\nif (await hasModelInCache(modelId)) {\n  console.log('Model is cached, initialization will be fast!');\n} else {\n  console.log('Model needs to be downloaded (~4.5GB)');\n  // Show warning to user\n}\n\nconst client = await createClient({\n  local: { model: modelId }\n});\n```\n\n## Disable OPFS Caching (Testing Only)\n\nFor testing, you can disable persistent caching:\n\n```typescript\nconst client = await createClient({\n  local: mlc({\n    model: 'auto',\n    useCache: false  // Don't cache in OPFS\n  })\n});\n```\n\n:::warning\nDisabling cache means the model will be downloaded on every page refresh. Only use this for testing.\n:::\n\n## Privacy Benefits\n\nLocal-only mode provides:\n\n-  **Zero data transmission**  All processing happens on-device\n-  **No API keys required**  No authentication needed\n-  **Offline capable**  Works without internet (after initial download)\n-  **Full control**  You own the inference pipeline\n-  **No usage limits**  No rate limiting or quotas\n\n## Requirements\n\n- **WebGPU support**  Chrome 113+, Edge 113+, or compatible browser\n- **Sufficient VRAM**  At least 2GB (Grade C devices supported)\n- **Storage space**  1.5GB to 8GB depending on model\n- **COOP/COEP headers**  Required for SharedArrayBuffer (see [FAQ](/faq))\n\n## Next Steps\n\n- [Device Detection](/examples/device-detection)  Check capabilities before loading\n- [Cache Management](/guides/cache-management)  Manage downloaded models\n- [Hybrid Mode](/examples/hybrid-mode)  Combine local and cloud for best of both worlds","src/content/docs/examples/local-only.mdx","c14894606038f0d7","examples/streaming-chat",{"id":212,"data":214,"body":220,"filePath":221,"digest":222,"deferredRender":16},{"title":215,"description":216,"editUrl":16,"head":217,"template":18,"sidebar":218,"pagefind":16,"draft":20},"Streaming Chat","Stream chat responses token by token for real-time user experience",[],{"hidden":20,"attrs":219},{},"Streaming responses provides a better user experience by displaying content as it's generated, rather than waiting for the complete response.\n\n## Complete Example\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = await createClient({\n  local: 'auto'\n});\n\n// Enable streaming with stream: true\nconst stream = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Explain how WebGPU enables in-browser AI inference.' }\n  ],\n  stream: true\n});\n\n// Process each chunk as it arrives\nfor await (const chunk of stream) {\n  const delta = chunk.choices[0]?.delta?.content;\n  if (delta) {\n    // Display the token immediately (e.g., append to UI)\n    process.stdout.write(delta);\n  }\n}\n```\n\n## Streaming to the DOM\n\nHere's a practical example of streaming to a web page:\n\n```typescript\nconst messageElement = document.getElementById('assistant-message');\nmessageElement.textContent = '';\n\nconst stream = await client.chat.completions.create({\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: userInput }\n  ],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const delta = chunk.choices[0]?.delta?.content;\n  if (delta) {\n    messageElement.textContent += delta;\n  }\n}\n```\n\n## Stream Chunk Format\n\nEach chunk follows the OpenAI streaming format:\n\n```typescript\n{\n  id: \"chatcmpl-123\",\n  object: \"chat.completion.chunk\",\n  created: 1234567890,\n  model: \"Llama-3.1-8B-Instruct-q4f16_1-MLC\",\n  choices: [\n    {\n      index: 0,\n      delta: {\n        role: \"assistant\",  // Only in first chunk\n        content: \"Hello\"    // Token content\n      },\n      finish_reason: null   // \"stop\" in final chunk\n    }\n  ]\n}\n```\n\n## Handling Stream Completion\n\n```typescript\nlet fullResponse = '';\n\nfor await (const chunk of stream) {\n  const choice = chunk.choices[0];\n\n  if (choice?.delta?.content) {\n    fullResponse += choice.delta.content;\n    updateUI(fullResponse);\n  }\n\n  if (choice?.finish_reason === 'stop') {\n    console.log('Generation complete!');\n    console.log('Model used:', chunk.model);\n  }\n}\n```\n\n## Error Handling\n\n```typescript\ntry {\n  const stream = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Hello!' }],\n    stream: true\n  });\n\n  for await (const chunk of stream) {\n    // Process chunks\n  }\n} catch (error) {\n  console.error('Streaming failed:', error);\n  // Handle error (e.g., show user-friendly message)\n}\n```\n\n## Performance Tips\n\n- **Use Web Workers**: By default, inference runs in a Web Worker to keep the main thread responsive\n- **Debounce UI updates**: If rendering is expensive, consider debouncing DOM updates\n- **Show loading states**: Display a loading indicator before the first chunk arrives\n\n## Next Steps\n\n- [Hybrid Mode](/examples/hybrid-mode)  Combine local and cloud streaming\n- [Abort Requests](/guides/abort-requests)  Cancel ongoing generation\n- [Custom Providers](/guides/custom-providers)  Implement custom streaming logic","src/content/docs/examples/streaming-chat.mdx","1f1657db6d91f190","concepts/architecture",{"id":223,"data":225,"body":231,"filePath":232,"digest":233,"deferredRender":16},{"title":226,"description":227,"editUrl":16,"head":228,"template":18,"sidebar":229,"pagefind":16,"draft":20},"Architecture","Overall architecture and module design of WebLLM.io SDK",[],{"hidden":20,"attrs":230},{},"WebLLM.io is designed as a dual-engine AI inference SDK that seamlessly combines local browser-based inference with cloud API fallback. This architecture enables progressive enhancement: applications work everywhere with cloud APIs, but leverage local GPU acceleration when available.\n\n## High-Level Architecture\n\n```\n\n                      Application Layer                       \n                  createClient()  WebLLMClient               \n\n                              \n\n                    Hardware Fingerprint                      \n              WebGPU Detection + VRAM Estimation              \n\n                              \n\n                      Device Scoring                          \n              S (8GB) / A (4GB) / B (2GB) / C (\u003C2GB)      \n\n                              \n\n                     Route Decision Engine                    \n   Consider: Device Grade, Battery, Network, Backend Status   \n\n                              \n                 \n                                          \n      \n        Local Backend             Cloud Backend       \n      WebGPU + MLC Engine        fetchSSE + OpenAI    \n       (in Web Worker)           Compatible API       \n      \n```\n\n## Module Layout\n\nThe SDK is organized into eight core modules:\n\n### 1. Core Module\n\n**Location:** `packages/sdk/src/core/`\n\nExports the primary `createClient()` factory function and fundamental types. Handles configuration resolution, normalization, and validation. Defines error classes and client lifecycle management.\n\n**Key responsibilities:**\n- Client factory and initialization\n- Configuration type definitions and resolution\n- Error handling abstractions\n- Core interfaces and contracts\n\n### 2. Capability Module\n\n**Location:** `packages/sdk/src/capability/`\n\nDetects WebGPU availability, estimates VRAM through `maxStorageBufferBindingSize`, and assigns device grades (S/A/B/C). This module acts as the hardware fingerprinting layer.\n\n**Key responsibilities:**\n- WebGPU feature detection\n- VRAM estimation via WebGPU limits\n- Device scoring algorithm\n- Compatibility checks\n\n### 3. Providers Module\n\n**Location:** `packages/sdk/src/providers/`\n\nContains provider implementations for local and cloud backends. The `mlc()` provider wraps `@mlc-ai/web-llm` for local inference. The `fetchSSE()` provider implements OpenAI-compatible server-sent events parsing without external dependencies.\n\n**Key responsibilities:**\n- `mlc()` provider for local MLC inference\n- `fetchSSE()` for cloud streaming APIs\n- Provider composition and wrapping logic\n- Custom provider interface definitions\n\n### 4. Inference Module\n\n**Location:** `packages/sdk/src/inference/`\n\nDefines the `InferenceBackend` interface and implements local/cloud backend adapters. Includes the `RequestQueue` that serializes concurrent requests to the single-threaded MLC engine.\n\n**Key responsibilities:**\n- `InferenceBackend` interface contract\n- Local backend (MLC) adapter\n- Cloud backend (fetch) adapter\n- Request queue for serialization\n\n### 5. Router Module\n\n**Location:** `packages/sdk/src/router/`\n\nImplements the route decision engine that determines whether to use local or cloud inference for each request. Considers device grade, backend readiness, battery status, and network conditions.\n\n**Key responsibilities:**\n- Route decision algorithm\n- Fallback strategy management\n- Backend health monitoring\n- Dynamic routing based on runtime conditions\n\n### 6. Chat Module\n\n**Location:** `packages/sdk/src/chat/`\n\nImplements the OpenAI-compatible `chat.completions` API with automatic fallback logic. Handles streaming and non-streaming responses, abort signals, and error recovery.\n\n**Key responsibilities:**\n- `chat.completions.create()` API\n- Streaming and non-streaming response handling\n- Automatic local-to-cloud fallback\n- Abort signal propagation\n\n### 7. Loader Module\n\n**Location:** `packages/sdk/src/loader/`\n\nManages progressive model loading with OPFS (Origin Private File System) caching. Tracks load state, emits progress events, and handles cache invalidation.\n\n**Key responsibilities:**\n- Progressive model download and initialization\n- OPFS cache management\n- Load state tracking and events\n- Cache hit/miss optimization\n\n### 8. Utils Module\n\n**Location:** `packages/sdk/src/utils/`\n\nProvides shared utilities including a lightweight `EventEmitter` for progress tracking, a self-implemented SSE parser (~30 lines, zero dependencies), and logging helpers.\n\n**Key responsibilities:**\n- EventEmitter for progress events\n- SSE (Server-Sent Events) parser\n- Logger abstraction\n- Common helper functions\n\n## Request Flow\n\n### Standard Request Path\n\n1. **Application** calls `client.chat.completions.create(messages)`\n2. **Chat module** receives request with messages and options\n3. **Router** evaluates:\n   - Is local backend ready?\n   - Is device grade sufficient?\n   - Is battery low?\n   - Is network available?\n4. **Route decision** selects local or cloud backend\n5. **Inference backend** processes request:\n   - **Local**: Queue request  MLC engine  stream response\n   - **Cloud**: Fetch SSE  parse stream  return response\n6. **Chat module** returns streaming iterator or response object\n7. **Application** consumes response chunks\n\n### Fallback Flow\n\nIf local inference fails (e.g., out of memory, WebGPU context lost):\n\n1. **Chat module** catches error\n2. **Router** marks local backend as unavailable\n3. **Request automatically retries** with cloud backend\n4. **Application** receives response transparently\n\nThis automatic fallback ensures resilience without manual error handling.\n\n## Dual Engine Design\n\nThe dual-engine architecture provides three key benefits:\n\n### 1. Progressive Enhancement\n\nApplications work everywhere with cloud APIs, but automatically accelerate with local GPU when available. No feature detection required in application code.\n\n### 2. Zero Configuration\n\n`createClient({ local: 'auto' })` automatically:\n- Detects WebGPU support\n- Estimates VRAM\n- Selects appropriate model\n- Falls back to cloud if needed\n\n### 3. Full Control\n\nAdvanced use cases can explicitly configure providers:\n\n```typescript\ncreateClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useCache: true\n  }),\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY\n  })\n})\n```\n\n## WebWorker Isolation\n\nBy default, local inference runs in a dedicated Web Worker to prevent UI freezing:\n\n```\nMain Thread                    Worker Thread\n              \ncreateClient()\n    \nInitialize Worker  Load MLC Engine\n                                  \nSend inference request  Enqueue in RequestQueue\n                                  \nReceive streaming chunks  MLCEngine.generate()\n                                  \nRender UI                      Continue processing\n```\n\nThis architecture ensures smooth user experience even during intensive inference operations.\n\n## Subpath Exports\n\nThe SDK uses Node.js subpath exports for selective importing:\n\n- `@webllm-io/sdk`  Main entry: createClient, checkCapability, types\n- `@webllm-io/sdk/providers/mlc`  mlc() provider function\n- `@webllm-io/sdk/providers/fetch`  fetchSSE() provider function\n- `@webllm-io/sdk/worker`  Web Worker entry for MLC inference\n\nThis design reduces bundle size by allowing tree-shaking of unused providers.\n\n## Next Steps\n\n- Learn about the [Three-Level API](/concepts/three-level-api) design\n- Understand [Device Scoring](/concepts/device-scoring) in detail\n- Explore [Provider Composition](/concepts/provider-composition) patterns","src/content/docs/concepts/architecture.mdx","b9a8a3fe0cd808d8","concepts/device-scoring",{"id":234,"data":236,"body":242,"filePath":243,"digest":244,"deferredRender":16},{"title":237,"description":238,"editUrl":16,"head":239,"template":18,"sidebar":240,"pagefind":16,"draft":20},"Device Scoring","Hardware capability detection and automatic model selection based on VRAM estimation",[],{"hidden":20,"attrs":241},{},"WebLLM.io automatically detects your device's hardware capabilities and assigns a score (S/A/B/C) to select the optimal model. This ensures the best possible user experience across devices ranging from high-end desktops to mobile phones.\n\n## Device Grade System\n\nThe SDK uses a four-tier grading system based on available GPU memory:\n\n| Grade | VRAM Range | Typical Devices | Default Model | Model Size |\n|-------|------------|-----------------|---------------|------------|\n| **S** | 8192 MB | High-end desktop GPUs (RTX 3080+, M2 Max+) | Llama-3.1-8B-Instruct-q4f16_1-MLC | ~5.5 GB |\n| **A** | 4096 MB | Mid-range GPUs (RTX 3060, M1/M2 Pro, iPad Pro) | Llama-3.1-8B-Instruct-q4f16_1-MLC | ~5.5 GB |\n| **B** | 2048 MB | Entry-level GPUs (Integrated Intel/AMD, M1 base) | Phi-3.5-mini-instruct-q4f16_1-MLC | ~2.2 GB |\n| **C** | &lt;2048 MB | Mobile devices, older laptops | Qwen2.5-1.5B-Instruct-q4f16_1-MLC | ~1.0 GB |\n\n**Important:** All grades support local inference. The C grade uses a lightweight but capable 1.5B parameter model, ensuring even low-end devices can run local AI.\n\n## VRAM Estimation Method\n\nUnlike traditional VRAM detection (which requires OS-level APIs), WebLLM.io uses a WebGPU-based estimation technique:\n\n```\nWebGPU Adapter\n    \nadapter.limits.maxStorageBufferBindingSize\n    \nProxy for VRAM capacity\n    \nDevice Grade (S/A/B/C)\n```\n\n### Why maxStorageBufferBindingSize?\n\nThe `maxStorageBufferBindingSize` limit indicates the maximum size of a single storage buffer binding in bytes. This value correlates strongly with total GPU memory:\n\n- **High VRAM devices** expose large buffer limits (1GB)\n- **Low VRAM devices** expose smaller limits (&lt;128MB)\n\nWhile not a perfect 1:1 mapping, this heuristic works reliably across browsers and platforms.\n\n### Detection Code Example\n\n```typescript\n// Simplified version of capability detection\nasync function detectDeviceGrade(): Promise\u003C'S' | 'A' | 'B' | 'C'> {\n  if (!navigator.gpu) {\n    throw new Error('WebGPU not supported')\n  }\n\n  const adapter = await navigator.gpu.requestAdapter()\n  if (!adapter) {\n    throw new Error('No WebGPU adapter available')\n  }\n\n  const maxBufferSize = adapter.limits.maxStorageBufferBindingSize\n  const estimatedVRAM_MB = Math.floor(maxBufferSize / (1024 * 1024))\n\n  if (estimatedVRAM_MB >= 8192) return 'S'\n  if (estimatedVRAM_MB >= 4096) return 'A'\n  if (estimatedVRAM_MB >= 2048) return 'B'\n  return 'C'\n}\n```\n\n## Model Selection Strategy\n\n### Automatic Selection (Zero Config)\n\nWith `local: 'auto'`, the SDK selects the best model for your device:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk'\n\nconst client = createClient({\n  local: 'auto'\n})\n\n// Automatically selects:\n// S/A grade  Llama-3.1-8B-Instruct-q4f16_1-MLC\n// B grade    Phi-3.5-mini-instruct-q4f16_1-MLC\n// C grade    Qwen2.5-1.5B-Instruct-q4f16_1-MLC\n```\n\n### Custom Tiers (Responsive Config)\n\nYou can override default models for each tier:\n\n```typescript\nconst client = createClient({\n  local: {\n    tiers: {\n      high: 'Llama-3.1-70B-Instruct-q3f16_1-MLC',    // For S/A grade\n      medium: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',   // For B grade\n      low: 'TinyLlama-1.1B-Chat-q4f16_1-MLC'         // For C grade\n    }\n  }\n})\n```\n\nThe SDK maps grades to tiers:\n- **S and A grades**  `high` tier\n- **B grade**  `medium` tier\n- **C grade**  `low` tier\n\n## Device Grade Examples\n\n### Grade S: High-End Desktop\n\n**Hardware:**\n- NVIDIA RTX 4090 (24GB VRAM)\n- Apple M2 Max (32GB unified memory)\n- AMD RX 7900 XTX (24GB VRAM)\n\n**Model Selection:**\n- Default: Llama-3.1-8B-Instruct (~5.5GB)\n- Can run: Any quantized model up to 13B parameters\n- Inference Speed: ~30-50 tokens/sec\n\n### Grade A: Mid-Range\n\n**Hardware:**\n- NVIDIA RTX 3060 (12GB VRAM)\n- Apple M1/M2 Pro (16GB unified memory)\n- iPad Pro M2 (8GB RAM)\n\n**Model Selection:**\n- Default: Llama-3.1-8B-Instruct (~5.5GB)\n- Can run: 7B-8B quantized models comfortably\n- Inference Speed: ~15-25 tokens/sec\n\n### Grade B: Entry-Level\n\n**Hardware:**\n- Intel Iris Xe (integrated GPU)\n- AMD Radeon 680M (integrated GPU)\n- Apple M1 base (8GB unified memory)\n\n**Model Selection:**\n- Default: Phi-3.5-mini-instruct (~2.2GB)\n- Can run: 3B-4B parameter models\n- Inference Speed: ~8-15 tokens/sec\n\n### Grade C: Mobile/Low-End\n\n**Hardware:**\n- iPhone 15 Pro\n- Android flagship phones\n- Low-power laptops with integrated graphics\n\n**Model Selection:**\n- Default: Qwen2.5-1.5B-Instruct (~1.0GB)\n- Can run: Lightweight models up to 1.5B parameters\n- Inference Speed: ~5-10 tokens/sec\n\n## Checking Device Capability\n\nUse the `checkCapability()` function to inspect your device grade:\n\n```typescript\nimport { checkCapability } from '@webllm-io/sdk'\n\nconst capability = await checkCapability()\n\nconsole.log(capability)\n// {\n//   webgpu: true,\n//   grade: 'A',\n//   estimatedVRAM_MB: 4096,\n//   recommendedModel: 'Llama-3.1-8B-Instruct-q4f16_1-MLC'\n// }\n```\n\nThis is useful for debugging or showing system information in your UI:\n\n```typescript\n// Show capability to user\nconst cap = await checkCapability()\ndocument.getElementById('gpu-info').textContent =\n  `Device Grade: ${cap.grade} (${cap.estimatedVRAM_MB}MB estimated VRAM)`\n```\n\n## Model Size vs Quality Trade-offs\n\nUnderstanding the trade-offs helps in custom tier configuration:\n\n```\nModel Parameters  Model Size  Quality   Speed      VRAM Required\n\n70B (q3f16_1)     ~40 GB      Excellent Very Slow  S grade only\n13B (q4f16_1)     ~8 GB       Very Good Slow       S grade\n8B  (q4f16_1)     ~5.5 GB     Good      Medium     S/A grade\n3.5B (q4f16_1)    ~2.2 GB     Fair      Fast       B grade\n1.5B (q4f16_1)    ~1.0 GB     Basic     Very Fast  C grade\n```\n\n**Quantization Levels:**\n- `q4f16_1`: 4-bit weights, 16-bit activations (good balance)\n- `q3f16_1`: 3-bit weights, 16-bit activations (higher compression)\n\n## Handling Detection Failures\n\nIf WebGPU is unavailable or detection fails, the SDK falls back to cloud:\n\n```typescript\nconst client = createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n})\n\n// If WebGPU unavailable  automatically uses cloud\nawait client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello' }]\n})\n```\n\n### Detection Edge Cases\n\n1. **Browser doesn't support WebGPU**\n   - Solution: Fall back to cloud backend\n   - Affected: Safari \u003C 18, Firefox \u003C 120 (partial support)\n\n2. **WebGPU disabled by user/policy**\n   - Solution: Cloud fallback\n   - Common in enterprise environments with strict security policies\n\n3. **Adapter request denied**\n   - Solution: Cloud fallback\n   - Rare, may occur on headless systems or virtual machines\n\n4. **Unusually low maxStorageBufferBindingSize**\n   - Solution: Assign C grade and use lightweight model\n   - May occur on very old integrated GPUs\n\n## Best Practices\n\n### 1. Trust Auto-Detection for Most Use Cases\n\nThe default model selection is optimized for each grade:\n\n```typescript\n// Recommended for most apps\nconst client = createClient({ local: 'auto' })\n```\n\n### 2. Customize Tiers for Specific Needs\n\nIf you need specific quality/speed trade-offs:\n\n```typescript\nconst client = createClient({\n  local: {\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',   // Quality focus\n      medium: 'Phi-3.5-mini-instruct-q4f16_1-MLC', // Balanced\n      low: 'TinyLlama-1.1B-Chat-q4f16_1-MLC'       // Speed focus\n    }\n  }\n})\n```\n\n### 3. Show Capability Info to Users\n\nLet users know what they're getting:\n\n```typescript\nconst cap = await checkCapability()\nshowNotification(\n  `Running ${cap.recommendedModel} locally (Grade ${cap.grade})`\n)\n```\n\n### 4. Test Across Device Grades\n\nUse browser DevTools to simulate different limits:\n\n```typescript\n// Mock lower-grade device for testing\nif (import.meta.env.DEV) {\n  Object.defineProperty(navigator.gpu, 'requestAdapter', {\n    value: async () => ({\n      limits: { maxStorageBufferBindingSize: 2048 * 1024 * 1024 } // Force B grade\n    })\n  })\n}\n```\n\n## Next Steps\n\n- Learn about the [Three-Level API](/concepts/three-level-api) for progressive configuration\n- Understand [Request Queue](/concepts/request-queue) for concurrent inference\n- Explore [Architecture](/concepts/architecture) for overall system design","src/content/docs/concepts/device-scoring.mdx","6b8fa931320487d6","concepts/three-level-api",{"id":245,"data":247,"body":253,"filePath":254,"digest":255,"deferredRender":16},{"title":248,"description":249,"editUrl":16,"head":250,"template":18,"sidebar":251,"pagefind":16,"draft":20},"Three-Level API","Progressive configuration from zero-config to full control",[],{"hidden":20,"attrs":252},{},"WebLLM.io provides three levels of API complexity, allowing you to start simple and progressively opt into more control as your needs evolve. Each level builds on the previous one, providing a smooth learning curve.\n\n## API Levels Overview\n\n```\nLevel 1: Zero Config\n    (add device-specific model tiers)\nLevel 2: Responsive\n    (add explicit provider functions)\nLevel 3: Full Control\n```\n\n## Level 1: Zero Config\n\n**When to use:** Prototyping, simple demos, or when you trust the SDK's defaults.\n\nThe simplest possible configuration. Just specify `'auto'` for local inference:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk'\n\nconst client = createClient({\n  local: 'auto'\n})\n\n// That's it! The SDK handles everything:\n// - WebGPU detection\n// - VRAM estimation\n// - Device scoring\n// - Model selection\n// - Progressive loading\n```\n\n### What Happens Automatically\n\n1. **Hardware Detection**\n   - Checks if WebGPU is available\n   - Reads `maxStorageBufferBindingSize` to estimate VRAM\n   - Assigns device grade (S/A/B/C)\n\n2. **Model Selection**\n   - **S grade (8GB):** `Llama-3.1-8B-Instruct-q4f16_1-MLC`\n   - **A grade (4GB):** `Llama-3.1-8B-Instruct-q4f16_1-MLC`\n   - **B grade (2GB):** `Phi-3.5-mini-instruct-q4f16_1-MLC`\n   - **C grade (&lt;2GB):** `Qwen2.5-1.5B-Instruct-q4f16_1-MLC`\n\n3. **Worker Initialization**\n   - Spawns Web Worker for non-blocking inference\n   - Configures OPFS cache by default\n\n4. **Fallback Strategy**\n   - If local inference unavailable, falls back to cloud (if configured)\n   - Transparent error recovery\n\n### Zero Config with Cloud Fallback\n\n```typescript\nconst client = createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n})\n```\n\nNow the SDK will:\n- Try local inference first (auto-selected model)\n- Fall back to `gpt-4o-mini` if local fails or is unavailable\n\n## Level 2: Responsive\n\n**When to use:** Production apps targeting diverse devices (desktop, tablet, mobile) with different hardware capabilities.\n\nResponsive configuration lets you define model tiers for different device grades while still letting the SDK handle detection and selection:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk'\n\nconst client = createClient({\n  local: {\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',      // S/A grade\n      medium: 'Phi-3.5-mini-instruct-q4f16_1-MLC',    // B grade\n      low: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'        // C grade\n    }\n  },\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n})\n```\n\n### Tier Mapping\n\nThe SDK maps device grades to tiers:\n\n| Device Grade | VRAM Range | Selected Tier |\n|--------------|------------|---------------|\n| S            | 8GB       | `high`        |\n| A            | 4GB       | `high`        |\n| B            | 2GB       | `medium`      |\n| C            | &lt;2GB       | `low`         |\n\n### Additional Configuration Options\n\nYou can customize more aspects at this level:\n\n```typescript\nconst client = createClient({\n  local: {\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      medium: 'Phi-3.5-mini-instruct-q4f16_1-MLC',\n      low: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n    },\n    useWorker: true,    // Run in Web Worker (default: true)\n    useCache: true,     // Enable OPFS cache (default: true)\n  },\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    timeout: 30000,     // Request timeout in ms\n    maxRetries: 2       // Retry count for network failures\n  }\n})\n```\n\n### When to Use Responsive Config\n\nResponsive configuration is ideal when:\n- Your app targets multiple device types (desktop, tablet, mobile)\n- You want to optimize for quality on high-end devices\n- You need fast inference on low-end devices\n- You want automatic device-appropriate model selection\n- You still want the SDK to handle hardware detection\n\n## Level 3: Full Control\n\n**When to use:** Advanced use cases requiring explicit control over providers, custom model selection, or integration with non-standard backends.\n\nFull control configuration uses explicit provider functions:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk'\nimport { mlc } from '@webllm-io/sdk/providers/mlc'\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch'\n\nconst client = createClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useWorker: true,\n    useCache: false,  // Disable cache for always-fresh downloads\n    workerUrl: '/custom-worker.js',  // Custom worker script\n    initProgressCallback: (progress) => {\n      console.log(`Loading: ${progress.text}`)\n    }\n  }),\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    headers: {\n      'X-Custom-Header': 'value'\n    },\n    fetch: customFetchImpl  // Custom fetch implementation\n  })\n})\n```\n\n### Provider Functions\n\n#### mlc() Provider\n\nThe `mlc()` provider wraps `@mlc-ai/web-llm` for local inference:\n\n```typescript\nimport { mlc } from '@webllm-io/sdk/providers/mlc'\n\nconst localProvider = mlc({\n  model: string,                    // Required: MLC model ID\n  useWorker?: boolean,              // Default: true\n  useCache?: boolean,               // Default: true\n  workerUrl?: string,               // Custom worker script\n  initProgressCallback?: (report) => void,\n  logLevel?: 'DEBUG' | 'INFO' | 'WARN' | 'ERROR'\n})\n```\n\n#### fetchSSE() Provider\n\nThe `fetchSSE()` provider implements OpenAI-compatible streaming:\n\n```typescript\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch'\n\nconst cloudProvider = fetchSSE({\n  baseURL: string,                  // Required: API endpoint\n  apiKey?: string,                  // Authentication key\n  model?: string,                   // Default model\n  timeout?: number,                 // Request timeout (ms)\n  maxRetries?: number,              // Retry count\n  headers?: Record\u003Cstring, string>, // Custom headers\n  fetch?: typeof fetch              // Custom fetch impl\n})\n```\n\n### Custom Provider Function\n\nYou can implement custom providers for non-standard backends:\n\n```typescript\nimport { CloudFn } from '@webllm-io/sdk'\n\nconst customCloudProvider: CloudFn = async ({ messages, options }) => {\n  // Custom implementation\n  const response = await fetch('https://my-api.com/chat', {\n    method: 'POST',\n    body: JSON.stringify({ messages, ...options })\n  })\n\n  // Return AsyncIterable\u003Cstring> for streaming\n  return {\n    async *[Symbol.asyncIterator]() {\n      const reader = response.body.getReader()\n      while (true) {\n        const { done, value } = await reader.read()\n        if (done) break\n        yield new TextDecoder().decode(value)\n      }\n    }\n  }\n}\n\nconst client = createClient({\n  cloud: customCloudProvider\n})\n```\n\n### Disabling Local or Cloud\n\nYou can explicitly disable one engine:\n\n```typescript\n// Cloud-only (no local inference)\nconst client = createClient({\n  local: false,\n  cloud: fetchSSE({ /* ... */ })\n})\n\n// Local-only (no cloud fallback)\nconst client = createClient({\n  local: mlc({ /* ... */ }),\n  cloud: false\n})\n```\n\n### When to Use Full Control\n\nFull control configuration is necessary when:\n- You need a specific model regardless of device grade\n- You're integrating with a custom or self-hosted backend\n- You want to disable caching for development\n- You need custom progress callbacks or logging\n- You're implementing a non-standard provider\n- You want to benchmark different configurations\n\n## Choosing the Right Level\n\n```\n\n Use Case                            Recommended Level   \n\n Quick prototype                     Zero Config         \n Simple demo                         Zero Config         \n Multi-device production app         Responsive          \n Device-specific optimization        Responsive          \n Custom backend integration          Full Control        \n Specific model requirement          Full Control        \n Advanced debugging                  Full Control        \n Non-standard provider               Full Control        \n\n```\n\n## Migration Path\n\nYou can smoothly migrate between levels as your needs evolve:\n\n### From Zero Config to Responsive\n\n```typescript\n// Before (Zero Config)\nconst client = createClient({ local: 'auto' })\n\n// After (Responsive)\nconst client = createClient({\n  local: {\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      medium: 'Phi-3.5-mini-instruct-q4f16_1-MLC',\n      low: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n    }\n  }\n})\n```\n\n### From Responsive to Full Control\n\n```typescript\n// Before (Responsive)\nconst client = createClient({\n  local: {\n    tiers: { high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC' }\n  }\n})\n\n// After (Full Control)\nconst client = createClient({\n  local: mlc({ model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC' })\n})\n```\n\n## Next Steps\n\n- Learn about [Device Scoring](/concepts/device-scoring) for tier selection\n- Understand [Provider Composition](/concepts/provider-composition) patterns\n- Explore the [Request Queue](/concepts/request-queue) for concurrent requests","src/content/docs/concepts/three-level-api.mdx","26a307ef318ad397","concepts/request-queue",{"id":256,"data":258,"body":264,"filePath":265,"digest":266,"deferredRender":16},{"title":259,"description":260,"editUrl":16,"head":261,"template":18,"sidebar":262,"pagefind":16,"draft":20},"Request Queue","Serializing concurrent requests for single-threaded MLC inference engine",[],{"hidden":20,"attrs":263},{},"The MLC inference engine underlying WebLLM.io's local backend is single-threaded. It can only process one inference request at a time. To handle concurrent requests safely, the SDK implements a `RequestQueue` that serializes requests in FIFO (first-in, first-out) order.\n\n## The Single-Threading Constraint\n\nUnlike cloud APIs that can handle parallel requests, WebGPU-based inference engines run on a single execution thread:\n\n```\nCloud API (Parallel)              Local MLC Engine (Serial)\n              \nRequest 1  Response 1      Request 1  Response 1\nRequest 2  Response 2                       \nRequest 3  Response 3      Request 2  Response 2\n                                                   \n(All process simultaneously)      Request 3  Response 3\n\n                                  (One at a time)\n```\n\n**Why single-threaded?**\n- WebGPU contexts are not thread-safe\n- Model weights are loaded into GPU memory once\n- Inference uses mutable state (KV cache, attention heads)\n\nAttempting concurrent inference would cause:\n- Race conditions in GPU memory access\n- Corrupted KV cache state\n- Incorrect or garbled output\n- Potential crashes\n\n## RequestQueue Architecture\n\nThe `RequestQueue` solves this by serializing requests:\n\n```\nApplication Layer (Multiple concurrent calls)\n    \nchat.completions.create()  3\n    \n\n          RequestQueue               \n                                     \n              \n   Req   Req   Req   FIFO  \n    1      2      3    Queue \n              \n\n     (dequeue one at a time)\nMLCEngine.generate()\n    \nResponse chunks\n```\n\n### How It Works\n\n1. **Request arrives** at `chat.completions.create()`\n2. **Enqueued** into the RequestQueue\n3. **Queue checks** if engine is idle\n   - If idle: dequeue and start inference immediately\n   - If busy: wait until current request completes\n4. **Inference runs** on the MLC engine\n5. **Streaming chunks** are yielded back to caller\n6. **Request completes**, queue dequeues next request\n7. **Repeat** until queue is empty\n\n## Code Example\n\n### Without Queue (Incorrect)\n\nThis code would fail or produce corrupted results:\n\n```typescript\n//  WRONG: Direct concurrent calls to MLC engine\nconst engine = await CreateMLCEngine(/* ... */)\n\n// These requests race and corrupt each other\nconst response1 = engine.chat.completions.create({ messages: [/* ... */] })\nconst response2 = engine.chat.completions.create({ messages: [/* ... */] })\nconst response3 = engine.chat.completions.create({ messages: [/* ... */] })\n```\n\n### With Queue (Correct)\n\nWebLLM.io's internal queue handles serialization automatically:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk'\n\nconst client = createClient({ local: 'auto' })\n\n//  CORRECT: Queue automatically serializes these\nconst [response1, response2, response3] = await Promise.all([\n  client.chat.completions.create({ messages: [{ role: 'user', content: 'Query 1' }] }),\n  client.chat.completions.create({ messages: [{ role: 'user', content: 'Query 2' }] }),\n  client.chat.completions.create({ messages: [{ role: 'user', content: 'Query 3' }] })\n])\n\n// Requests execute in order: 1  2  3\n// Each waits for the previous to complete\n```\n\n## Queue Behavior\n\n### FIFO Ordering\n\nRequests are processed in the order they arrive:\n\n```typescript\nconsole.log('Starting 3 requests...')\n\n// Request 1 starts immediately\nconst promise1 = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'First' }]\n})\n\n// Request 2 queued, waits for #1\nconst promise2 = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Second' }]\n})\n\n// Request 3 queued, waits for #2\nconst promise3 = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Third' }]\n})\n\n// Output order is guaranteed: First  Second  Third\n```\n\n### Streaming and Queue\n\nStreaming requests hold the queue until completion:\n\n```typescript\n// Request 1: streaming (holds queue for entire duration)\nconst stream1 = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write an essay' }],\n  stream: true\n})\n\n// Request 2: waits until stream1 is fully consumed\nconst stream2 = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Another query' }],\n  stream: true\n})\n\n// Consume stream1 chunks\nfor await (const chunk of stream1) {\n  console.log(chunk.choices[0]?.delta?.content)\n}\n// Only after stream1 completes does stream2 start\n```\n\n### Abort and Queue\n\nAborting a request releases the queue immediately:\n\n```typescript\nconst controller = new AbortController()\n\n// Request 1: starts immediately\nconst promise1 = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Long response' }],\n  signal: controller.signal\n})\n\n// Request 2: queued\nconst promise2 = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Quick query' }]\n})\n\n// Abort request 1 after 1 second\nsetTimeout(() => controller.abort(), 1000)\n\n// Request 2 starts immediately after abort\n```\n\n## Performance Implications\n\n### Queueing Latency\n\nIf multiple requests arrive simultaneously, later requests experience queueing delay:\n\n```\nRequest 1: 0s wait + 5s inference = 5s total\nRequest 2: 5s wait + 3s inference = 8s total\nRequest 3: 8s wait + 4s inference = 12s total\n```\n\n**Mitigation strategies:**\n1. Batch related queries into a single request\n2. Use cloud backend for parallel requests\n3. Show queueing status in UI\n\n### Batching Example\n\nInstead of multiple requests:\n\n```typescript\n//  Slow: 3 sequential requests\nconst joke = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Tell me a joke' }]\n})\nconst fact = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Tell me a fact' }]\n})\nconst poem = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write a haiku' }]\n})\n```\n\nCombine into one:\n\n```typescript\n//  Fast: Single request\nconst response = await client.chat.completions.create({\n  messages: [{\n    role: 'user',\n    content: `Please provide three things:\n1. A joke\n2. An interesting fact\n3. A haiku`\n  }]\n})\n```\n\n## Multi-User Scenarios\n\nFor apps with multiple concurrent users, the queue ensures fair access:\n\n```typescript\n// User A sends a message\nuserA.sendMessage('Hello') // Request 1, starts immediately\n\n// User B sends a message (queued)\nuserB.sendMessage('Hi there') // Request 2, waits\n\n// User C sends a message (queued)\nuserC.sendMessage('What is AI?') // Request 3, waits\n\n// Execution order: A  B  C\n```\n\n### Queue Status UI\n\nShow users their position in queue:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk'\n\nconst client = createClient({ local: 'auto' })\n\n// Hypothetical queue status API (not currently exposed)\nclient.on('queueStatus', ({ position, length }) => {\n  if (position > 0) {\n    showNotification(`Your request is #${position} in queue`)\n  }\n})\n```\n\n## Queue vs Cloud Parallelism\n\nConsider using cloud backend for truly parallel workloads:\n\n```typescript\nconst client = createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY\n  }\n})\n\n// For single-user sequential chat, local is fine\nawait client.chat.completions.create({ messages: [/* ... */] })\n\n// For bulk parallel processing, force cloud mode\nconst summaries = await Promise.all(\n  documents.map(doc =>\n    client.chat.completions.create({\n      messages: [{ role: 'user', content: `Summarize: ${doc}` }],\n      // Force cloud for parallel execution\n      __forceBackend: 'cloud'\n    })\n  )\n)\n```\n\n## Queue Internals\n\n### Implementation Pseudocode\n\n```typescript\nclass RequestQueue {\n  private queue: Request[] = []\n  private processing = false\n\n  async enqueue(request: Request): Promise\u003CResponse> {\n    this.queue.push(request)\n\n    if (!this.processing) {\n      this.processNext()\n    }\n\n    return request.promise\n  }\n\n  private async processNext() {\n    if (this.queue.length === 0) {\n      this.processing = false\n      return\n    }\n\n    this.processing = true\n    const request = this.queue.shift()!\n\n    try {\n      const response = await this.executeOnEngine(request)\n      request.resolve(response)\n    } catch (error) {\n      request.reject(error)\n    }\n\n    // Process next request\n    this.processNext()\n  }\n\n  private async executeOnEngine(request: Request): Promise\u003CResponse> {\n    // Actual MLC engine inference\n    return await mlcEngine.chat.completions.create(request.params)\n  }\n}\n```\n\n### Thread Safety\n\nThe queue itself runs on the JavaScript main thread (or worker thread if using `useWorker: true`). JavaScript's single-threaded execution model ensures the queue's own operations are atomic.\n\n## Best Practices\n\n### 1. Avoid Unnecessary Concurrent Requests\n\nIf requests are sequential, use `await`:\n\n```typescript\n//  Unnecessarily concurrent\nconst promises = messages.map(msg =>\n  client.chat.completions.create({ messages: [{ role: 'user', content: msg }] })\n)\nawait Promise.all(promises)\n\n//  Sequential is clearer and has same result\nfor (const msg of messages) {\n  await client.chat.completions.create({\n    messages: [{ role: 'user', content: msg }]\n  })\n}\n```\n\n### 2. Batch Related Queries\n\nCombine multiple questions into one prompt when possible.\n\n### 3. Use Abort Signals for User Cancellations\n\nAllow users to cancel queued requests:\n\n```typescript\nconst controller = new AbortController()\n\nconst responsePromise = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Long task' }],\n  signal: controller.signal\n})\n\n// User clicks \"Cancel\"\ncancelButton.onclick = () => controller.abort()\n```\n\n### 4. Show Queue Status\n\nInform users when requests are queued, especially in multi-user apps.\n\n### 5. Consider Cloud for Parallel Workloads\n\nIf you need true parallelism (e.g., batch document processing), use cloud backend.\n\n## Next Steps\n\n- Learn about [Architecture](/concepts/architecture) for overall system design\n- Understand [Provider Composition](/concepts/provider-composition) for backend selection\n- Explore [Three-Level API](/concepts/three-level-api) for configuration patterns","src/content/docs/concepts/request-queue.mdx","8a6ee1db4a1b8bf6","concepts/provider-composition",{"id":267,"data":269,"body":275,"filePath":276,"digest":277,"deferredRender":16},{"title":270,"description":271,"editUrl":16,"head":272,"template":18,"sidebar":273,"pagefind":16,"draft":20},"Provider Composition","How configuration values transform into provider functions for local and cloud backends",[],{"hidden":20,"attrs":274},{},"WebLLM.io uses a flexible provider composition system that allows you to configure backends using simple strings or objects, while internally converting them to standardized provider functions. This design enables progressive disclosure: start with simple configs and opt into explicit providers when you need more control.\n\n## Configuration to Provider Pipeline\n\nEvery backend configuration goes through a resolution pipeline:\n\n```\nUser Input                    Resolution               Internal Provider\n\n'auto'                       auto-wrap               mlc()\n{ model: 'Llama-3.1' }       auto-wrap               mlc({ model: '...' })\nmlc({ ... })                 pass-through            mlc({ ... })\nCustomFunction               pass-through            CustomFunction\n\nstring/object                 normalize                 ResolvedProvider\n(Plain Config)                                          (Provider Function)\n```\n\n## Local Provider Resolution\n\n### String Inputs\n\nString inputs are shorthand for common configurations:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk'\n\n// Input: 'auto'\nconst client1 = createClient({\n  local: 'auto'\n})\n// Resolves to: mlc() with automatic device-based model selection\n\n// Input: 'Llama-3.1-8B-Instruct-q4f16_1-MLC'\nconst client2 = createClient({\n  local: 'Llama-3.1-8B-Instruct-q4f16_1-MLC'\n})\n// Resolves to: mlc({ model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC' })\n```\n\n### Object Inputs\n\nObject inputs provide structured configuration:\n\n```typescript\n// Input: LocalObjectConfig\nconst client = createClient({\n  local: {\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useWorker: true,\n    useCache: true\n  }\n})\n// Resolves to: mlc({ model: '...', useWorker: true, useCache: true })\n```\n\n### Tiered Object Inputs\n\nThe responsive API uses a `tiers` object:\n\n```typescript\n// Input: LocalTierConfig\nconst client = createClient({\n  local: {\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      medium: 'Phi-3.5-mini-instruct-q4f16_1-MLC',\n      low: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'\n    }\n  }\n})\n// Resolves to: mlc() with device-grade-based tier selection\n```\n\n### Function Inputs\n\nExplicit provider functions pass through unchanged:\n\n```typescript\nimport { mlc } from '@webllm-io/sdk/providers/mlc'\n\n// Input: mlc() provider function\nconst client = createClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useWorker: false,\n    logLevel: 'DEBUG'\n  })\n})\n// Resolves to: mlc({ ... }) (no transformation)\n```\n\n### Disabling Local\n\nSet `local: false` to disable local inference:\n\n```typescript\nconst client = createClient({\n  local: false,\n  cloud: { /* ... */ }\n})\n// Resolves to: null (no local backend)\n```\n\n## Cloud Provider Resolution\n\n### String Inputs\n\nString inputs are treated as API keys with default OpenAI endpoint:\n\n```typescript\n// Input: API key string\nconst client = createClient({\n  cloud: 'sk-...'\n})\n// Resolves to: fetchSSE({\n//   baseURL: 'https://api.openai.com/v1',\n//   apiKey: 'sk-...',\n//   model: 'gpt-4o-mini'\n// })\n```\n\n### Object Inputs\n\nObject inputs provide full configuration:\n\n```typescript\n// Input: CloudObjectConfig\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    timeout: 30000,\n    maxRetries: 2\n  }\n})\n// Resolves to: fetchSSE({ ... })\n```\n\n### Function Inputs\n\nExplicit provider functions pass through:\n\n```typescript\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch'\n\n// Input: fetchSSE() provider function\nconst client = createClient({\n  cloud: fetchSSE({\n    baseURL: 'https://api.anthropic.com/v1',\n    apiKey: process.env.ANTHROPIC_API_KEY,\n    model: 'claude-3-5-sonnet-20241022',\n    headers: {\n      'anthropic-version': '2023-06-01'\n    }\n  })\n})\n// Resolves to: fetchSSE({ ... }) (no transformation)\n```\n\n### Custom Cloud Functions\n\nYou can provide a custom `CloudFn` implementation:\n\n```typescript\nimport type { CloudFn } from '@webllm-io/sdk'\n\nconst customCloudProvider: CloudFn = async ({ messages, options }) => {\n  const response = await fetch('https://my-custom-api.com/chat', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ messages, ...options })\n  })\n\n  // Return AsyncIterable\u003CChatCompletionChunk>\n  return parseSSEStream(response.body)\n}\n\nconst client = createClient({\n  cloud: customCloudProvider\n})\n// Resolves to: customCloudProvider (no transformation)\n```\n\n### Disabling Cloud\n\nSet `cloud: false` to disable cloud fallback:\n\n```typescript\nconst client = createClient({\n  local: 'auto',\n  cloud: false\n})\n// Resolves to: null (no cloud backend)\n```\n\n## Provider Types\n\n### ResolvedLocalBackend\n\nAfter resolution, local configs become `ResolvedLocalBackend`:\n\n```typescript\ntype ResolvedLocalBackend = {\n  type: 'mlc'\n  model: string\n  useWorker: boolean\n  useCache: boolean\n  workerUrl?: string\n  initProgressCallback?: (report: InitProgressReport) => void\n  logLevel?: 'DEBUG' | 'INFO' | 'WARN' | 'ERROR'\n}\n```\n\n### ResolvedCloudBackend\n\nAfter resolution, cloud configs become `ResolvedCloudBackend`:\n\n```typescript\ntype ResolvedCloudBackend = {\n  type: 'fetchSSE' | 'custom'\n  baseURL: string\n  apiKey?: string\n  model?: string\n  timeout?: number\n  maxRetries?: number\n  headers?: Record\u003Cstring, string>\n  fetch?: typeof fetch\n}\n```\n\n## Resolution Flow Diagram\n\n```\ncreateClient({ local, cloud })\n    \n\n      Configuration Resolution             \n\n  Local Input       Normalize    mlc()   \n  Cloud Input       Normalize    fetchSSE()\n\n    \n\n      Provider Initialization              \n\n  mlc() creates MLCEngine instance         \n  fetchSSE() creates fetch wrapper         \n\n    \n\n      Backend Registration                 \n\n  Register with InferenceBackend manager   \n  Register with Router for decision logic  \n\n    \nWebLLMClient ready\n```\n\n## Auto-Wrapping Examples\n\n### Example 1: Zero Config to mlc()\n\n```typescript\n// User writes:\ncreateClient({ local: 'auto' })\n\n// SDK transforms to:\ncreateClient({\n  local: mlc({\n    model: detectDeviceGrade() === 'S' ? 'Llama-3.1-8B-...' :\n           detectDeviceGrade() === 'A' ? 'Llama-3.1-8B-...' :\n           detectDeviceGrade() === 'B' ? 'Phi-3.5-mini-...' :\n                                          'Qwen2.5-1.5B-...',\n    useWorker: true,\n    useCache: true\n  })\n})\n```\n\n### Example 2: Object Config to mlc()\n\n```typescript\n// User writes:\ncreateClient({\n  local: {\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useCache: false\n  }\n})\n\n// SDK transforms to:\ncreateClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useWorker: true,  // Default\n    useCache: false\n  })\n})\n```\n\n### Example 3: String API Key to fetchSSE()\n\n```typescript\n// User writes:\ncreateClient({\n  cloud: process.env.OPENAI_API_KEY\n})\n\n// SDK transforms to:\ncreateClient({\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    timeout: 30000,\n    maxRetries: 1\n  })\n})\n```\n\n### Example 4: Cloud Object to fetchSSE()\n\n```typescript\n// User writes:\ncreateClient({\n  cloud: {\n    baseURL: 'https://api.together.xyz/v1',\n    apiKey: process.env.TOGETHER_API_KEY,\n    model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'\n  }\n})\n\n// SDK transforms to:\ncreateClient({\n  cloud: fetchSSE({\n    baseURL: 'https://api.together.xyz/v1',\n    apiKey: process.env.TOGETHER_API_KEY,\n    model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',\n    timeout: 30000,    // Default\n    maxRetries: 1      // Default\n  })\n})\n```\n\n## Custom Provider Implementation\n\n### Local Provider Example\n\nImplementing a custom local provider (hypothetical):\n\n```typescript\nimport type { LocalFn } from '@webllm-io/sdk'\n\nconst customLocalProvider: LocalFn = async ({ messages, options }) => {\n  // Initialize custom WebGPU inference engine\n  const engine = await initCustomEngine()\n\n  // Generate response\n  const stream = await engine.generate(messages, options)\n\n  // Return AsyncIterable\u003CChatCompletionChunk>\n  return {\n    async *[Symbol.asyncIterator]() {\n      for await (const token of stream) {\n        yield {\n          id: generateId(),\n          object: 'chat.completion.chunk',\n          created: Date.now(),\n          model: 'custom-model',\n          choices: [{\n            index: 0,\n            delta: { content: token },\n            finish_reason: null\n          }]\n        }\n      }\n    }\n  }\n}\n\nconst client = createClient({\n  local: customLocalProvider\n})\n```\n\n### Cloud Provider Example\n\nImplementing a custom cloud provider for Anthropic:\n\n```typescript\nimport type { CloudFn } from '@webllm-io/sdk'\n\nconst anthropicProvider: CloudFn = async ({ messages, options }) => {\n  const response = await fetch('https://api.anthropic.com/v1/messages', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      'x-api-key': process.env.ANTHROPIC_API_KEY,\n      'anthropic-version': '2023-06-01'\n    },\n    body: JSON.stringify({\n      model: 'claude-3-5-sonnet-20241022',\n      messages: messages,\n      stream: true,\n      max_tokens: options.max_tokens || 4096\n    })\n  })\n\n  // Parse Anthropic's SSE format (different from OpenAI)\n  return parseAnthropicSSE(response.body)\n}\n\nconst client = createClient({\n  cloud: anthropicProvider\n})\n```\n\n## Benefits of Provider Composition\n\n### 1. Progressive Disclosure\n\nStart simple, add complexity only when needed:\n\n```typescript\n// Beginner: simple string\ncreateClient({ local: 'auto' })\n\n// Intermediate: object config\ncreateClient({ local: { model: '...', useCache: false } })\n\n// Advanced: explicit provider\ncreateClient({ local: mlc({ model: '...', logLevel: 'DEBUG' }) })\n```\n\n### 2. Type Safety\n\nTypeScript ensures valid configurations at compile time:\n\n```typescript\n//  Valid\ncreateClient({ local: 'auto' })\ncreateClient({ local: { model: 'Llama-3.1-8B' } })\ncreateClient({ local: mlc({ model: 'Llama-3.1-8B' }) })\n\n//  Type error\ncreateClient({ local: 123 })\ncreateClient({ local: { invalidKey: true } })\n```\n\n### 3. Extensibility\n\nCustom providers integrate seamlessly:\n\n```typescript\nconst client = createClient({\n  local: myCustomLocalProvider,\n  cloud: myCustomCloudProvider\n})\n```\n\n### 4. Default Optimizations\n\nAuto-wrapped providers get sensible defaults:\n\n- `useWorker: true` prevents UI freezing\n- `useCache: true` speeds up subsequent loads\n- `timeout: 30000` prevents hung requests\n- `maxRetries: 1` handles transient network errors\n\n## Best Practices\n\n### 1. Use Auto-Wrapping for Simplicity\n\nLet the SDK handle provider creation:\n\n```typescript\n//  Recommended for most use cases\ncreateClient({\n  local: 'auto',\n  cloud: { baseURL: '...', apiKey: '...' }\n})\n```\n\n### 2. Use Explicit Providers for Advanced Control\n\nWhen you need debugging or non-standard configs:\n\n```typescript\n//  Use explicit providers for advanced scenarios\nimport { mlc } from '@webllm-io/sdk/providers/mlc'\n\ncreateClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    logLevel: 'DEBUG',\n    initProgressCallback: (report) => {\n      console.log(`Loading: ${report.text}`)\n    }\n  })\n})\n```\n\n### 3. Validate Custom Providers\n\nEnsure custom providers match the expected interface:\n\n```typescript\nimport type { CloudFn } from '@webllm-io/sdk'\n\nconst myProvider: CloudFn = async ({ messages, options }) => {\n  // Implementation must return AsyncIterable\u003CChatCompletionChunk>\n  // ...\n}\n```\n\n### 4. Document Provider Assumptions\n\nIf you're wrapping a non-standard API, document the assumptions:\n\n```typescript\n/**\n * Custom provider for XYZ API\n * Assumes:\n * - OpenAI-compatible message format\n * - SSE streaming with 'data:' prefix\n * - Authentication via 'X-API-Key' header\n */\nconst xyzProvider = async ({ messages, options }) => {\n  // ...\n}\n```\n\n## Next Steps\n\n- Learn about [Architecture](/concepts/architecture) for overall system design\n- Understand [Three-Level API](/concepts/three-level-api) for configuration patterns\n- Explore [Device Scoring](/concepts/device-scoring) for automatic model selection","src/content/docs/concepts/provider-composition.mdx","88abc28fe1e662bf","getting-started/installation",{"id":278,"data":280,"body":286,"filePath":287,"digest":288,"deferredRender":16},{"title":281,"description":282,"editUrl":16,"head":283,"template":18,"sidebar":284,"pagefind":16,"draft":20},"Installation","Install @webllm-io/sdk and optional dependencies for local and cloud AI inference",[],{"hidden":20,"attrs":285},{},"# Installation\n\nGet started with WebLLM.io by installing the core SDK package.\n\n## Core Package\n\nInstall `@webllm-io/sdk` using your preferred package manager:\n\n```bash\n# npm\nnpm install @webllm-io/sdk\n\n# pnpm\npnpm add @webllm-io/sdk\n\n# yarn\nyarn add @webllm-io/sdk\n```\n\n## Optional Peer Dependency\n\nFor **local inference** support, you need to install `@mlc-ai/web-llm` as a peer dependency:\n\n```bash\n# npm\nnpm install @mlc-ai/web-llm\n\n# pnpm\npnpm add @mlc-ai/web-llm\n\n# yarn\nyarn add @mlc-ai/web-llm\n```\n\n:::note\nIf you only plan to use cloud inference, you can skip installing `@mlc-ai/web-llm`. The SDK will automatically disable local inference when the peer dependency is not available.\n:::\n\n## TypeScript Support\n\nTypeScript definitions are included out of the box. No additional `@types` packages are needed.\n\n## Browser Requirements\n\n### Local Inference (WebGPU)\n\nTo run models locally in the browser, you need:\n\n- **Chrome 113+** or **Edge 113+**\n- **WebGPU support** enabled (usually on by default)\n- **Sufficient VRAM**  Device scoring adapts model selection to available resources\n\n:::tip[Check Device Capability]\nUse the built-in capability checker to verify WebGPU support:\n\n```ts\nimport { checkCapability } from '@webllm-io/sdk';\n\nconst capability = await checkCapability();\nconsole.log(capability.grade); // S, A, B, or C\nconsole.log(capability.canRunLocal); // true if WebGPU is available\n```\n:::\n\n### Cloud Mode\n\nCloud inference works in **all modern browsers** with no special requirements:\n\n- Chrome, Firefox, Safari, Edge (latest versions)\n- Mobile browsers (iOS Safari, Chrome Mobile)\n\n## Verification\n\nAfter installation, verify everything is set up correctly:\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient({\n  local: 'auto',\n  cloud: { baseURL: 'https://api.openai.com/v1', apiKey: 'sk-...' },\n});\n\nconsole.log('WebLLM client created successfully!');\n```\n\n## Next Steps\n\n- [Quick Start](/getting-started/quick-start)  Make your first completion\n- [Playground](/getting-started/playground)  Try the interactive demo\n- [Configuration](/guides/configuration)  Learn about advanced options","src/content/docs/getting-started/installation.mdx","cdcf38ddc3aff042","guides/abort-requests",{"id":289,"data":291,"body":297,"filePath":298,"digest":299,"deferredRender":16},{"title":292,"description":293,"editUrl":16,"head":294,"template":18,"sidebar":295,"pagefind":16,"draft":20},"Abort Requests","Cancel in-flight inference requests using AbortSignal.",[],{"hidden":20,"attrs":296},{},"WebLLM SDK supports request cancellation via the standard `AbortController` / `AbortSignal` pattern. This works for both local and cloud inference.\n\n## Basic Usage\n\n```ts\nconst controller = new AbortController();\n\n// Cancel after 5 seconds\nsetTimeout(() => controller.abort(), 5000);\n\ntry {\n  const res = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Write a long essay about AI' }],\n    signal: controller.signal,\n  });\n} catch (err) {\n  if (err.code === 'ABORTED') {\n    console.log('Request was cancelled');\n  }\n}\n```\n\n## Aborting Streams\n\nAbort works naturally with streaming:\n\n```ts\nconst controller = new AbortController();\n\nconst stream = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Tell me a story' }],\n  stream: true,\n  signal: controller.signal,\n});\n\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.choices[0]?.delta?.content ?? '');\n\n  // Cancel mid-stream based on some condition\n  if (shouldStop()) {\n    controller.abort();\n    break;\n  }\n}\n```\n\n## How It Works\n\nThe abort mechanism differs by backend:\n\n| Backend | Mechanism |\n|---|---|\n| **Local (MLC)** | Calls `interruptGenerate()` on the MLC engine to stop token generation |\n| **Cloud (fetchSSE)** | Passes the `AbortSignal` to the underlying `fetch()` call |\n\nIn both cases, the SDK throws a `WebLLMError` with code `'ABORTED'`.\n\n## Error Handling\n\nAborted requests throw a `WebLLMError` with a specific error code:\n\n```ts\nimport { WebLLMError } from '@webllm-io/sdk';\n\ntry {\n  const res = await client.chat.completions.create({\n    messages: [...],\n    signal: controller.signal,\n  });\n} catch (err) {\n  if (err instanceof WebLLMError && err.code === 'ABORTED') {\n    // User cancelled  not a real error\n    return;\n  }\n  // Handle actual errors\n  throw err;\n}\n```\n\n## Fallback Behavior\n\nWhen a request is aborted, the SDK does **not** attempt a fallback to the other backend. This is intentional  if the user cancelled, they don't want the request to continue on a different provider.\n\n## UI Pattern: Stop Button\n\nA common pattern for chat UIs:\n\n```ts\nlet activeController: AbortController | null = null;\n\nasync function sendMessage(content: string) {\n  // Cancel any in-flight request\n  activeController?.abort();\n  activeController = new AbortController();\n\n  const stream = client.chat.completions.create({\n    messages: [{ role: 'user', content }],\n    stream: true,\n    signal: activeController.signal,\n  });\n\n  for await (const chunk of stream) {\n    appendToUI(chunk.choices[0]?.delta?.content ?? '');\n  }\n\n  activeController = null;\n}\n\nfunction stopGeneration() {\n  activeController?.abort();\n}\n```","src/content/docs/guides/abort-requests.mdx","3fc382114d87de42","getting-started/playground",{"id":300,"data":302,"body":308,"filePath":309,"digest":310,"deferredRender":16},{"title":303,"description":304,"editUrl":16,"head":305,"template":18,"sidebar":306,"pagefind":16,"draft":20},"Playground","Try WebLLM.io in your browser with the interactive playground - test local and cloud inference",[],{"hidden":20,"attrs":307},{},"# Playground\n\nThe WebLLM.io Playground is an interactive demo application where you can test local and cloud inference capabilities in real-time.\n\n## Accessing the Playground\n\n### Online\n\nVisit the hosted playground at:\n\n```\nhttps://webllm.io/playground\n```\n\n### Local Development\n\nRun the playground locally:\n\n```bash\n# From the monorepo root\npnpm --filter @webllm-io/playground dev\n```\n\nThe playground will start at `http://localhost:5173`.\n\n:::note[COOP/COEP Headers Required]\nThe playground requires Cross-Origin-Opener-Policy and Cross-Origin-Embedder-Policy headers for `SharedArrayBuffer` support. These are automatically configured in the Vite development server.\n:::\n\n## Features\n\n### Device Capability Display\n\nWhen the playground loads, it automatically detects your device's WebGPU capability and displays:\n\n- **Device Grade**  S, A, B, or C based on estimated VRAM\n- **WebGPU Support**  Whether local inference is available\n- **Recommended Model**  Auto-selected model for your device tier\n\n### Mode Selection\n\nChoose how WebLLM.io routes your requests:\n\n- **Local**  Force local inference (requires WebGPU)\n- **Cloud**  Use cloud provider only\n- **Hybrid**  Smart routing based on device capability (default)\n\n### Settings Panel\n\nClick the **Settings** button to configure local and cloud options.\n\n#### Local Settings\n\n- **Model**  Override auto-selection with a specific model name\n  - Leave empty for automatic device-based selection\n  - Example: `Llama-3.1-8B-Instruct-q4f16_1-MLC`\n- **WebWorker**  Run inference in a Web Worker (default: **Enabled**)\n  - Keeps UI responsive during inference\n  - Recommended for production use\n- **Cache (OPFS)**  Enable model caching in Origin Private File System (default: **Enabled**)\n  - Dramatically improves load times on subsequent visits\n  - Models are cached locally and reused\n\n#### Cloud Settings\n\n- **Base URL**  Cloud API endpoint\n  - Example: `https://api.openai.com/v1`\n  - Compatible with OpenAI-style APIs\n- **API Key**  Authentication token\n  - Stored securely in `localStorage`\n  - Displayed as password-masked input\n- **Model**  Cloud model identifier\n  - Example: `gpt-4o-mini`, `gpt-4-turbo`\n- **Timeout**  Request timeout in milliseconds\n  - Default: 30000 (30 seconds)\n- **Retries**  Number of retry attempts on failure\n  - Default: 2\n\n### Settings Persistence\n\nAll settings are automatically saved to `localStorage` under the key `webllm-playground-config` and restored when you reload the page.\n\nClick **Apply & Reinitialize** to apply changes and restart the client.\n\n### Model Tag\n\nEach assistant message displays the model that generated the response:\n\n- For local inference: `Llama-3.1-8B-Instruct-q4f16_1-MLC` (example)\n- For cloud inference: `gpt-4o-mini` (example)\n\nThe model name is extracted from the first streaming chunk's `model` field and displayed as an italic tag below the message.\n\n## Chat Interface\n\n### Sending Messages\n\nType your message in the input box and press **Send** or hit **Enter**.\n\n### Streaming Responses\n\nResponses stream in real-time, displaying tokens as they're generated.\n\n### Message History\n\nThe playground maintains conversation context automatically. Previous messages are included in subsequent requests.\n\n### Clear Conversation\n\nClick **Clear** to reset the conversation and start fresh.\n\n## Browser Requirements\n\n- **Chrome 113+** or **Edge 113+** for local inference (WebGPU required)\n- Any modern browser for cloud-only mode\n\n## Troubleshooting\n\n### Local Inference Not Working\n\n1. **Check WebGPU Support**  Ensure your browser supports WebGPU\n2. **Verify MLC Dependency**  The playground includes `@mlc-ai/web-llm` by default\n3. **Check Console**  Look for detailed error messages in the browser console\n\n### Cloud Inference Failing\n\n1. **Verify API Key**  Ensure your API key is correct\n2. **Check Base URL**  Confirm the endpoint URL format\n3. **Network Issues**  Check browser network tab for CORS or connection errors\n\n### Slow Initial Load\n\nThe first time you use local inference, the model needs to download (1-4 GB depending on device). Subsequent loads use the OPFS cache and are much faster.\n\n## Next Steps\n\n- [Quick Start](/getting-started/quick-start)  Integrate WebLLM.io into your app\n- [Configuration](/guides/configuration)  Advanced configuration options\n- [Local Inference](/guides/local-inference)  Learn about WebGPU models\n- [Examples](/examples/basic-chat)  More code examples","src/content/docs/getting-started/playground.mdx","74985e29e320aaf1","getting-started/quick-start",{"id":311,"data":313,"body":319,"filePath":320,"digest":321,"deferredRender":16},{"title":314,"description":315,"editUrl":16,"head":316,"template":18,"sidebar":317,"pagefind":16,"draft":20},"Quick Start","Get up and running with WebLLM.io in minutes - from installation to your first completion",[],{"hidden":20,"attrs":318},{},"# Quick Start\n\nThis guide will walk you through creating your first AI completions with WebLLM.io.\n\n## Step 1: Import and Create Client\n\nFirst, import the SDK and create a client instance:\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient({\n  local: 'auto', // Auto-detect device capability and select model\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: 'sk-your-api-key-here',\n  },\n});\n```\n\n### Configuration Options\n\n- **`local: 'auto'`**  Automatically selects the best local model based on device capability\n- **`cloud: { ... }`**  Cloud provider configuration (OpenAI compatible)\n\n:::tip[Cloud Only Mode]\nTo use cloud inference only, omit the `local` option:\n\n```ts\nconst client = createClient({\n  cloud: { baseURL: '...', apiKey: '...' },\n});\n```\n:::\n\n## Step 2: Make Your First Completion\n\nUse the OpenAI-compatible Chat Completions API:\n\n```ts\nconst result = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello, world!' }],\n});\n\nconsole.log(result.choices[0].message.content);\n// Output: \"Hello! How can I help you today?\"\n```\n\n### Request Options\n\n```ts\nconst result = await client.chat.completions.create({\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n  temperature: 0.7,\n  max_tokens: 100,\n});\n```\n\n## Step 3: Streaming Completions\n\nFor real-time responses, use streaming mode:\n\n```ts\nconst stream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write a short poem about coding.' }],\n  stream: true,\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content ?? '';\n  process.stdout.write(content);\n}\n```\n\n### Browser Example (Streaming)\n\n```ts\nconst stream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Explain quantum computing.' }],\n  stream: true,\n});\n\nconst container = document.getElementById('output');\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content ?? '';\n  container.textContent += content;\n}\n```\n\n## Step 4: Abort/Interrupt Requests\n\nCancel ongoing completions:\n\n```ts\nconst controller = new AbortController();\n\n// Start completion\nconst promise = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Count to 1000.' }],\n  signal: controller.signal,\n});\n\n// Abort after 2 seconds\nsetTimeout(() => controller.abort(), 2000);\n\ntry {\n  await promise;\n} catch (error) {\n  console.log('Request aborted');\n}\n```\n\n## Step 5: Clean Up\n\nWhen you're done, dispose of the client to free resources:\n\n```ts\nawait client.dispose();\n```\n\n:::caution\nAlways call `dispose()` when you no longer need the client, especially when using local inference. This releases WebGPU resources and terminates Web Workers.\n:::\n\n## Complete Example\n\nPutting it all together:\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\n\nasync function main() {\n  // Create client\n  const client = createClient({\n    local: 'auto',\n    cloud: {\n      baseURL: 'https://api.openai.com/v1',\n      apiKey: process.env.OPENAI_API_KEY,\n    },\n  });\n\n  // Non-streaming completion\n  const result = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Hello!' }],\n  });\n  console.log('Non-streaming:', result.choices[0].message.content);\n\n  // Streaming completion\n  const stream = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Count to 5.' }],\n    stream: true,\n  });\n\n  process.stdout.write('Streaming: ');\n  for await (const chunk of stream) {\n    process.stdout.write(chunk.choices[0]?.delta?.content ?? '');\n  }\n  console.log('\\n');\n\n  // Clean up\n  await client.dispose();\n}\n\nmain().catch(console.error);\n```\n\n## Next Steps\n\n- [Playground](/getting-started/playground)  Try the interactive demo\n- [Configuration](/guides/configuration)  Learn about advanced options\n- [Local Inference](/guides/local-inference)  Deep dive into WebGPU models\n- [Cloud Providers](/guides/cloud-providers)  Configure cloud fallback","src/content/docs/getting-started/quick-start.mdx","adbb0d6d1de2e97c","guides/custom-providers",{"id":322,"data":324,"body":330,"filePath":331,"digest":332,"deferredRender":16},{"title":325,"description":326,"editUrl":16,"head":327,"template":18,"sidebar":328,"pagefind":16,"draft":20},"Custom Providers","Use mlc() and fetchSSE() provider factories, or write your own.",[],{"hidden":20,"attrs":329},{},"WebLLM SDK uses a provider system to abstract inference backends. You can use the built-in `mlc()` and `fetchSSE()` factories, or provide a completely custom function.\n\n## Built-in Providers\n\n### mlc()  Local Provider\n\nThe `mlc()` function creates a local inference provider using the MLC engine:\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\nimport { mlc } from '@webllm-io/sdk/providers/mlc';\n\nconst client = createClient({\n  local: mlc({\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useWebWorker: true,\n    useCache: true,\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      medium: 'Llama-3.2-3B-Instruct-q4f16_1-MLC',\n      low: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC',\n    },\n  }),\n});\n```\n\n#### Options\n\n| Option | Type | Default | Description |\n|---|---|---|---|\n| `model` | `string` |  | Fixed model ID. Overrides tier selection. |\n| `tiers` | `TiersConfig` |  | Model per device grade (high/medium/low) |\n| `useCache` | `boolean` | `true` | Enable OPFS model caching |\n| `useWebWorker` | `boolean` | `true` | Run in Web Worker |\n\n### fetchSSE()  Cloud Provider\n\nThe `fetchSSE()` function creates a cloud inference provider using SSE streaming:\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch';\n\nconst client = createClient({\n  cloud: fetchSSE({\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: 'sk-...',\n    model: 'gpt-4o-mini',\n    timeout: 30000,\n    retries: 2,\n  }),\n});\n```\n\nYou can also pass just a URL string:\n\n```ts\nconst client = createClient({\n  cloud: fetchSSE('https://api.openai.com/v1'),\n});\n```\n\n#### Options\n\n| Option | Type | Default | Description |\n|---|---|---|---|\n| `baseURL` | `string` |  | API endpoint URL (required) |\n| `apiKey` | `string` |  | Authorization bearer token |\n| `model` | `string` |  | Model identifier |\n| `headers` | `Record\u003Cstring, string>` |  | Additional request headers |\n| `timeout` | `number` |  | Request timeout in milliseconds |\n| `retries` | `number` |  | Number of retry attempts |\n\n## Custom Cloud Function\n\nFor full control, pass a function as the `cloud` config. It receives messages and a route context, and must return either a `ChatCompletion` or an `AsyncIterable\u003CChatCompletionChunk>`:\n\n```ts\nimport type { CloudFn } from '@webllm-io/sdk';\n\nconst myProvider: CloudFn = async (messages, context) => {\n  const response = await fetch('https://my-api.example.com/chat', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ messages }),\n  });\n\n  return response.json();\n};\n\nconst client = createClient({\n  cloud: myProvider,\n});\n```\n\n### CloudFn Type\n\n```ts\ntype CloudFn = (\n  messages: Message[],\n  context: RouteContext,\n) => Promise\u003CChatCompletion> | AsyncIterable\u003CChatCompletionChunk>;\n```\n\n## Config vs Explicit Provider\n\nThere are two ways to configure each backend:\n\n**Plain config**  the SDK wraps it into the default provider automatically:\n\n```ts\ncreateClient({\n  local: 'auto',\n  cloud: { baseURL: '...', apiKey: '...' },\n});\n```\n\n**Explicit provider**  you call the factory yourself for more control:\n\n```ts\ncreateClient({\n  local: mlc({ useWebWorker: false }),\n  cloud: fetchSSE({ baseURL: '...', retries: 5 }),\n});\n```\n\nBoth approaches produce the same result. Use explicit providers when you need options that aren't exposed in the plain config shorthand.","src/content/docs/guides/custom-providers.mdx","ddaa48021a1e35ee","guides/cloud-fallback",{"id":333,"data":335,"body":341,"filePath":342,"digest":343,"deferredRender":16},{"title":336,"description":337,"editUrl":16,"head":338,"template":18,"sidebar":339,"pagefind":16,"draft":20},"Cloud Fallback","Configure cloud API endpoints as fallback or primary inference backend",[],{"hidden":20,"attrs":340},{},"WebLLM.io supports cloud-based inference using OpenAI-compatible APIs. This enables instant responses, server-side model execution, and zero local dependencies for pure cloud mode.\n\n## Zero Dependencies for Cloud-Only Mode\n\nWhen using cloud-only configuration (no local inference), WebLLM.io has **zero external dependencies**. The SDK includes a self-implemented SSE (Server-Sent Events) parser, eliminating the need for the OpenAI SDK or any other HTTP client library.\n\n```bash\n# Cloud-only mode requires NO peer dependencies\nnpm install @webllm-io/sdk\n```\n\n## Basic Usage\n\nConfigure cloud inference with an API endpoint and key:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: 'sk-your-api-key-here',\n    model: 'gpt-4o-mini'\n  }\n});\n\nconst completion = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Explain quantum computing' }\n  ]\n});\n\nconsole.log(completion.choices[0].message.content);\n```\n\n## Shorthand String Configuration\n\nFor quick setup, pass the API key as a string (assumes OpenAI):\n\n```typescript\nconst client = createClient({\n  cloud: 'sk-your-api-key-here'\n});\n\n// Specify model in each request\nconst completion = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }],\n  model: 'gpt-4o-mini'\n});\n```\n\n## OpenAI-Compatible APIs\n\nWebLLM.io works with any OpenAI-compatible API endpoint. Examples include:\n\n### OpenAI\n\n```typescript\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n```\n\n### Azure OpenAI\n\n```typescript\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://your-resource.openai.azure.com/openai/deployments/your-deployment',\n    apiKey: process.env.AZURE_OPENAI_KEY,\n    headers: {\n      'api-version': '2024-02-01'\n    },\n    model: 'gpt-4o'\n  }\n});\n```\n\n### Groq\n\n```typescript\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://api.groq.com/openai/v1',\n    apiKey: process.env.GROQ_API_KEY,\n    model: 'llama-3.1-70b-versatile'\n  }\n});\n```\n\n### Local OpenAI-Compatible Server\n\n```typescript\nconst client = createClient({\n  cloud: {\n    baseURL: 'http://localhost:1234/v1',\n    model: 'local-model'\n    // No API key required for local servers\n  }\n});\n```\n\n## Custom Headers\n\nAdd custom headers for authentication or API versioning:\n\n```typescript\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://api.example.com/v1',\n    apiKey: 'your-key',\n    model: 'custom-model',\n    headers: {\n      'X-Custom-Header': 'value',\n      'Authorization': 'Bearer custom-token'  // Override default auth\n    }\n  }\n});\n```\n\n## Timeout and Retries\n\nConfigure request timeout and automatic retries:\n\n```typescript\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    timeout: 30000,  // 30 seconds (default: 60000)\n    retries: 3       // Retry failed requests 3 times (default: 2)\n  }\n});\n```\n\nTimeouts apply to the entire request (including streaming). Retries only trigger on network errors, not on API errors (4xx/5xx status codes).\n\n## Using the fetchSSE Provider\n\nFor advanced use cases, you can create a cloud provider explicitly using `fetchSSE()`:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\nimport { fetchSSE } from '@webllm-io/sdk/providers/fetch';\n\nconst cloudProvider = fetchSSE({\n  baseURL: 'https://api.openai.com/v1',\n  apiKey: process.env.OPENAI_API_KEY,\n  model: 'gpt-4o-mini',\n  timeout: 45000,\n  retries: 5\n});\n\nconst client = createClient({\n  cloud: cloudProvider\n});\n```\n\nThis is equivalent to passing the object configuration directly, but gives you a reusable provider instance.\n\n## Environment Variables\n\nStore API keys securely using environment variables:\n\n```typescript\n// .env\nOPENAI_API_KEY=sk-your-api-key-here\nGROQ_API_KEY=gsk_your-groq-key-here\n```\n\n```typescript\n// app.ts\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: import.meta.env.OPENAI_API_KEY,  // Vite\n    // apiKey: process.env.OPENAI_API_KEY,   // Node.js\n    model: 'gpt-4o-mini'\n  }\n});\n```\n\n**Never commit API keys to version control.**\n\n## Model Override per Request\n\nOverride the default model for individual requests:\n\n```typescript\nconst client = createClient({\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'  // default\n  }\n});\n\n// Use default model\nawait client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Quick question' }]\n});\n\n// Override with a different model\nawait client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Complex reasoning task' }],\n  model: 'gpt-4o'\n});\n```\n\n## Error Handling\n\nHandle API errors gracefully:\n\n```typescript\ntry {\n  const completion = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Hello!' }]\n  });\n  console.log(completion.choices[0].message.content);\n} catch (error) {\n  if (error.status === 401) {\n    console.error('Invalid API key');\n  } else if (error.status === 429) {\n    console.error('Rate limit exceeded');\n  } else if (error.status === 500) {\n    console.error('Server error');\n  } else {\n    console.error('Request failed:', error.message);\n  }\n}\n```\n\n## Next Steps\n\n- Learn about [Hybrid Routing](/guides/hybrid-routing) to combine local and cloud inference\n- Explore [Streaming](/guides/streaming) for real-time token-by-token responses\n- See [Abort Requests](/guides/abort-requests) to cancel in-flight API calls","src/content/docs/guides/cloud-fallback.mdx","afa41a86a71ebdd3","guides/device-capability",{"id":344,"data":346,"body":352,"filePath":353,"digest":354,"deferredRender":16},{"title":347,"description":348,"editUrl":16,"head":349,"template":18,"sidebar":350,"pagefind":16,"draft":20},"Device Capability","Detect device capabilities and adapt your UI before loading any models.",[],{"hidden":20,"attrs":351},{},"The `checkCapability()` function probes the device's hardware in milliseconds, letting you make UI decisions before loading any models.\n\n## Basic Usage\n\n```ts\nimport { checkCapability } from '@webllm-io/sdk';\n\nconst report = await checkCapability();\n\nif (report.webgpu && report.grade !== 'C') {\n  showLocalAIToggle();\n} else {\n  showCloudOnlyBadge();\n}\n```\n\n## CapabilityReport\n\n`checkCapability()` returns a `CapabilityReport` with these fields:\n\n```ts\ninterface CapabilityReport {\n  webgpu: boolean;          // WebGPU API available\n  gpu: GpuInfo | null;      // GPU vendor, name, estimated VRAM\n  grade: DeviceGrade;       // 'S' | 'A' | 'B' | 'C'\n  connection: ConnectionInfo;\n  battery: BatteryInfo | null;\n  memory: number;           // navigator.deviceMemory (GB), 0 if unavailable\n}\n```\n\n### GpuInfo\n\n```ts\ninterface GpuInfo {\n  vendor: string;  // e.g. 'nvidia', 'apple', 'intel'\n  name: string;    // GPU adapter name\n  vram: number;    // Estimated VRAM in MB\n}\n```\n\n### ConnectionInfo\n\n```ts\ninterface ConnectionInfo {\n  type: string;     // 'wifi', '4g', 'unknown', etc.\n  downlink: number; // Estimated bandwidth in Mbps\n  saveData: boolean; // Data saver mode\n}\n```\n\n### BatteryInfo\n\n```ts\ninterface BatteryInfo {\n  level: number;    // 0 to 1\n  charging: boolean;\n}\n```\n\n## Device Grades\n\nThe grade is based on estimated VRAM:\n\n| Grade | VRAM Threshold | Typical Device |\n|---|---|---|\n| **S** | 8192 MB | Desktop with dedicated GPU (RTX 3060+, M1 Pro+) |\n| **A** | 4096 MB | Gaming laptop, M1 MacBook |\n| **B** | 2048 MB | Integrated GPU, mid-range mobile |\n| **C** | &lt;2048 MB | Low-end mobile, older hardware |\n\nAll grades support local inference  grade C uses a lightweight model (Qwen2.5-1.5B).\n\n## Conditional Features\n\nUse the report to progressively enable features:\n\n```ts\nconst report = await checkCapability();\n\n// Only offer local AI on capable devices\nif (!report.webgpu) {\n  config.local = false;\n}\n\n// Warn on metered connections\nif (report.connection.saveData || report.connection.downlink \u003C 2) {\n  showDataWarning();\n}\n\n// Skip local on low battery\nif (report.battery && report.battery.level \u003C 0.2 && !report.battery.charging) {\n  config.local = false;\n}\n```\n\n## Pre-flight Pattern\n\nCheck capabilities before creating the client:\n\n```ts\nimport { checkCapability, createClient } from '@webllm-io/sdk';\n\nconst cap = await checkCapability();\n\nconst client = createClient({\n  local: cap.webgpu ? 'auto' : false,\n  cloud: { baseURL: 'https://api.example.com/v1' },\n});\n```\n\nThis avoids unnecessary WebGPU initialization on unsupported devices.","src/content/docs/guides/device-capability.mdx","6110b685f53c11d6","guides/model-loading",{"id":355,"data":357,"body":363,"filePath":364,"digest":365,"deferredRender":16},{"title":358,"description":359,"editUrl":16,"head":360,"template":18,"sidebar":361,"pagefind":16,"draft":20},"Model Loading","Progressive model loading, OPFS caching, and load state management.",[],{"hidden":20,"attrs":362},{},"WebLLM SDK handles model downloading, compilation, and caching automatically. This guide covers the loading lifecycle and how to monitor or customize it.\n\n## Progressive Loading\n\nWhen both `local` and `cloud` are configured, the SDK uses a progressive loading strategy:\n\n1. **Cloud responds immediately**  no waiting for model download\n2. **Local model downloads in background**  via the MLC engine\n3. **Hot-switch to local**  once the model is ready, subsequent requests use local inference\n\nThis happens transparently. Your code stays the same:\n\n```ts\nconst client = createClient({\n  local: 'auto',\n  cloud: { baseURL: 'https://api.example.com/v1' },\n});\n\n// First call  cloud (instant)\n// After model loads  local (automatic)\nconst res = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello' }],\n});\n```\n\n## Monitoring Progress\n\nUse the `onProgress` callback to track download, compilation, and warmup stages:\n\n```ts\nconst client = createClient({\n  local: 'auto',\n  cloud: { baseURL: 'https://api.example.com/v1' },\n  onProgress(progress) {\n    console.log(`[${progress.stage}] ${progress.model}: ${(progress.progress * 100).toFixed(1)}%`);\n    if (progress.bytesLoaded && progress.bytesTotal) {\n      const mb = (progress.bytesLoaded / 1024 / 1024).toFixed(1);\n      const total = (progress.bytesTotal / 1024 / 1024).toFixed(1);\n      console.log(`  ${mb} MB / ${total} MB`);\n    }\n  },\n});\n```\n\n### LoadProgress Interface\n\n```ts\ninterface LoadProgress {\n  stage: 'download' | 'compile' | 'warmup';\n  progress: number;       // 0 to 1\n  model: string;          // Model ID\n  bytesLoaded?: number;   // Bytes downloaded so far\n  bytesTotal?: number;    // Total bytes to download\n}\n```\n\n### Load States\n\nEach model goes through these states:\n\n| Status | Description |\n|---|---|\n| `idle` | Not started |\n| `downloading` | Fetching model weights |\n| `compiling` | Compiling model for WebGPU |\n| `ready` | Model loaded and ready for inference |\n| `error` | Loading failed |\n\n## OPFS Caching\n\nBy default, models are cached in the browser's [Origin Private File System](https://developer.mozilla.org/en-US/docs/Web/API/File_System_API/Origin_private_file_system) (OPFS). Unlike HTTP cache or localStorage, OPFS is:\n\n- **Persistent**  not cleared by \"Clear browsing data\"\n- **Large capacity**  can store multi-GB model files\n- **Per-origin**  isolated to your domain\n\n### Check Cache Status\n\n```ts\nimport { hasModelInCache } from '@webllm-io/sdk';\n\nconst cached = await hasModelInCache('Llama-3.1-8B-Instruct-q4f16_1-MLC');\nif (cached) {\n  console.log('Model is cached  loading will be fast');\n}\n```\n\n### Delete Cached Models\n\n```ts\nimport { deleteModelFromCache } from '@webllm-io/sdk';\n\nawait deleteModelFromCache('Llama-3.1-8B-Instruct-q4f16_1-MLC');\n```\n\n### Disable Caching\n\n```ts\nconst client = createClient({\n  local: {\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useCache: false,  // Always re-download\n  },\n});\n```\n\n## Model Selection by Device Tier\n\nWhen `local: 'auto'`, the SDK selects a model based on the device grade:\n\n| Grade | VRAM | Default Model |\n|---|---|---|\n| S | 8 GB | Large model (8B+) |\n| A | 4 GB | 8B quantized model |\n| B | 2 GB | Small model (3B or less) |\n| C | &lt;2 GB | Lightweight model (Qwen2.5-1.5B) |\n\nOverride with explicit tiers:\n\n```ts\nconst client = createClient({\n  local: {\n    tiers: {\n      high: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      medium: 'Llama-3.2-3B-Instruct-q4f16_1-MLC',\n      low: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC',\n    },\n  },\n});\n```","src/content/docs/guides/model-loading.mdx","5d179561627073a5","guides/local-inference",{"id":366,"data":368,"body":374,"filePath":375,"digest":376,"deferredRender":16},{"title":369,"description":370,"editUrl":16,"head":371,"template":18,"sidebar":372,"pagefind":16,"draft":20},"Local Inference","Run AI models locally in the browser using WebGPU and MLC",[],{"hidden":20,"attrs":373},{},"WebLLM.io enables running large language models entirely in the browser using WebGPU and the MLC inference engine. This eliminates server costs and ensures complete privacy since all data stays on the user's device.\n\n## Prerequisites\n\nTo use local inference, you need to install the `@mlc-ai/web-llm` peer dependency:\n\n```bash\nnpm install @mlc-ai/web-llm\n# or\npnpm add @mlc-ai/web-llm\n# or\nyarn add @mlc-ai/web-llm\n```\n\nYour browser must also support WebGPU. Use `checkCapability()` to verify device compatibility before enabling local features.\n\n## Basic Usage\n\nThe simplest way to enable local inference is with `local: 'auto'`:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient({\n  local: 'auto'\n});\n\nconst completion = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'What is WebGPU?' }\n  ]\n});\n\nconsole.log(completion.choices[0].message.content);\n```\n\nWith `'auto'`, the SDK automatically selects the optimal model based on your device's capabilities (VRAM grade).\n\n## Explicit Model Selection\n\nYou can specify a model explicitly:\n\n```typescript\nconst client = createClient({\n  local: 'Llama-3.1-8B-Instruct-q4f16_1-MLC'\n});\n```\n\nOr use an object configuration for more control:\n\n```typescript\nconst client = createClient({\n  local: {\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useCache: true,        // Enable OPFS caching (default: true)\n    useWebWorker: true     // Run in Web Worker (default: true)\n  }\n});\n```\n\n## Device-Aware Model Selection with Tiers\n\nFor adaptive model selection based on device capabilities, use the `tiers` configuration:\n\n```typescript\nconst client = createClient({\n  local: {\n    tiers: {\n      S: 'Llama-3.1-70B-Instruct-q4f16_1-MLC',  // 8GB VRAM\n      A: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',   // 4GB VRAM\n      B: 'Qwen2.5-3B-Instruct-q4f16_1-MLC',     // 2GB VRAM\n      C: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'    // \u003C2GB VRAM\n    }\n  }\n});\n```\n\nThe SDK will check the device's VRAM estimate and select the appropriate model tier automatically. All grades (S/A/B/C) support local inference.\n\n## WebWorker Execution\n\nBy default, inference runs in a Web Worker to prevent UI freezing during model loading and inference:\n\n```typescript\nconst client = createClient({\n  local: {\n    model: 'auto',\n    useWebWorker: true  // default\n  }\n});\n```\n\n**Important**: WebWorker mode requires proper COOP/COEP headers for SharedArrayBuffer support. See the [Web Worker guide](/guides/web-worker) for configuration details.\n\nTo disable WebWorker (runs on main thread):\n\n```typescript\nconst client = createClient({\n  local: {\n    model: 'auto',\n    useWebWorker: false\n  }\n});\n```\n\n## OPFS Model Caching\n\nModels are automatically cached using the Origin Private File System (OPFS) to avoid re-downloading on subsequent visits:\n\n```typescript\nconst client = createClient({\n  local: {\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useCache: true  // default\n  }\n});\n```\n\nTo disable caching:\n\n```typescript\nconst client = createClient({\n  local: {\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useCache: false\n  }\n});\n```\n\nCheck cache status and manage cached models:\n\n```typescript\n// Check if model is cached\nconst isCached = await client.hasModelInCache('Llama-3.1-8B-Instruct-q4f16_1-MLC');\n\n// Delete model from cache\nawait client.deleteModelFromCache('Llama-3.1-8B-Instruct-q4f16_1-MLC');\n```\n\n## Dynamic Model Selection\n\nUse a function to decide the model at runtime based on device statistics:\n\n```typescript\nconst client = createClient({\n  local: (stats) => {\n    if (stats.grade === 'S' || stats.grade === 'A') {\n      return 'Llama-3.1-8B-Instruct-q4f16_1-MLC';\n    }\n    if (stats.grade === 'B') {\n      return 'Qwen2.5-3B-Instruct-q4f16_1-MLC';\n    }\n    // Grade C or unsupported\n    return 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC';\n  }\n});\n```\n\nThe `stats` object contains `{ grade, webgpu, gpu, connection, battery, memory }`.\n\n## Progress Tracking\n\nMonitor model download and loading progress:\n\n```typescript\nconst client = createClient({\n  local: 'auto',\n  onProgress: (progress) => {\n    console.log(`Stage: ${progress.stage}`);\n    console.log(`Progress: ${progress.progress}%`);\n    console.log(`Model: ${progress.model}`);\n\n    if (progress.bytesLoaded && progress.bytesTotal) {\n      const mb = (progress.bytesLoaded / 1024 / 1024).toFixed(1);\n      const totalMb = (progress.bytesTotal / 1024 / 1024).toFixed(1);\n      console.log(`Downloaded: ${mb}MB / ${totalMb}MB`);\n    }\n  }\n});\n```\n\n## Next Steps\n\n- Learn about [Cloud Fallback](/guides/cloud-fallback) for OpenAI-compatible API support\n- Explore [Hybrid Routing](/guides/hybrid-routing) to combine local and cloud inference\n- See [Model Loading](/guides/model-loading) for progressive loading strategies","src/content/docs/guides/local-inference.mdx","807bda0e537577ea","guides/hybrid-routing",{"id":377,"data":379,"body":385,"filePath":386,"digest":387,"deferredRender":16},{"title":380,"description":381,"editUrl":16,"head":382,"template":18,"sidebar":383,"pagefind":16,"draft":20},"Hybrid Routing","Intelligently route requests between local and cloud inference backends",[],{"hidden":20,"attrs":384},{},"Hybrid routing combines the privacy and cost benefits of local inference with the instant availability and power of cloud APIs. WebLLM.io automatically decides which backend to use based on device capabilities, model availability, and loading state.\n\n## Basic Hybrid Configuration\n\nEnable both local and cloud backends:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient({\n  local: 'auto',  // Device-aware model selection\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\n// SDK automatically routes to the best available backend\nconst completion = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Explain hybrid inference' }\n  ]\n});\n```\n\n## Automatic Routing Logic\n\nThe SDK decides which backend to use based on:\n\n1. **Device capability**  If WebGPU is unavailable or device grade is too low, cloud is used\n2. **Model loading state**  Cloud responds instantly while local model downloads\n3. **Backend availability**  If one backend fails, the other is tried automatically\n4. **Explicit provider override**  Request-level `provider` parameter forces a specific backend\n\n### Routing by Device Grade\n\n```typescript\nconst client = createClient({\n  local: {\n    tiers: {\n      S: 'Llama-3.1-70B-Instruct-q4f16_1-MLC',\n      A: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      B: 'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n      C: null  // Grade C devices fall back to cloud\n    }\n  },\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n```\n\nOn low-end devices (Grade C), cloud is used exclusively. Higher-grade devices run locally.\n\n## Progressive Loading with Cloud Fallback\n\nDuring model download, the cloud API serves requests immediately:\n\n```typescript\nconst client = createClient({\n  local: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  },\n  onProgress: (progress) => {\n    console.log(`Loading model: ${progress.progress}% (${progress.stage})`);\n  }\n});\n\n// First request uses cloud (local model still downloading)\nconst completion1 = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'First message' }]\n});\nconsole.log('Backend:', completion1.model);  // 'gpt-4o-mini'\n\n// Wait for model to finish loading...\n// Subsequent requests automatically use local model\n\nconst completion2 = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Second message' }]\n});\nconsole.log('Backend:', completion2.model);  // 'Llama-3.1-8B-Instruct-q4f16_1-MLC'\n```\n\nThis provides instant responsiveness while still leveraging local inference once available.\n\n## Automatic Fallback on Errors\n\nIf the primary backend fails, the SDK automatically tries the fallback:\n\n```typescript\nconst client = createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\n// If local inference fails (e.g., out of memory), cloud is used\nconst completion = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Complex task requiring lots of VRAM' }]\n});\n```\n\nFallback happens transparently without application code changes.\n\n## Force a Specific Provider\n\nOverride automatic routing with the `provider` parameter:\n\n```typescript\nconst client = createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\n// Force local inference\nconst localCompletion = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Private data' }],\n  provider: 'local'\n});\n\n// Force cloud API\nconst cloudCompletion = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Complex reasoning' }],\n  provider: 'cloud'\n});\n```\n\nIf the forced provider is unavailable, the request will fail instead of falling back.\n\n## User-Controlled Provider Selection\n\nLet users choose their preferred inference backend:\n\n```typescript\nfunction createClientFromUserSettings(settings) {\n  const config = {};\n\n  if (settings.enableLocal) {\n    config.local = settings.localModel || 'auto';\n  }\n\n  if (settings.enableCloud && settings.apiKey) {\n    config.cloud = {\n      baseURL: settings.cloudEndpoint,\n      apiKey: settings.apiKey,\n      model: settings.cloudModel\n    };\n  }\n\n  return createClient(config);\n}\n\n// User preferences from UI\nconst userSettings = {\n  enableLocal: true,\n  localModel: 'auto',\n  enableCloud: true,\n  cloudEndpoint: 'https://api.openai.com/v1',\n  apiKey: localStorage.getItem('openai_api_key'),\n  cloudModel: 'gpt-4o-mini'\n};\n\nconst client = createClientFromUserSettings(userSettings);\n```\n\n## Cost Optimization\n\nUse local inference by default and cloud as a fallback to minimize API costs:\n\n```typescript\nconst client = createClient({\n  local: {\n    tiers: {\n      S: 'Llama-3.1-70B-Instruct-q4f16_1-MLC',\n      A: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n      B: 'Qwen2.5-3B-Instruct-q4f16_1-MLC',\n      C: 'Qwen2.5-1.5B-Instruct-q4f16_1-MLC'  // Even low-end devices use local\n    }\n  },\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\n// Use local when possible to save API costs\n// Cloud only used during model loading or on errors\n```\n\n## Privacy-First Routing\n\nEnforce local-only inference for sensitive data:\n\n```typescript\nconst client = createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\nfunction createCompletion(messages, isPrivate) {\n  return client.chat.completions.create({\n    messages,\n    provider: isPrivate ? 'local' : undefined  // Force local for private data\n  });\n}\n\n// Private data stays on device\nawait createCompletion([\n  { role: 'user', content: 'Analyze my medical records: ...' }\n], true);\n\n// Non-sensitive data can use cloud\nawait createCompletion([\n  { role: 'user', content: 'What is the capital of France?' }\n], false);\n```\n\n## Detecting Active Backend\n\nCheck which model responded:\n\n```typescript\nconst completion = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log('Model used:', completion.model);\n\nif (completion.model.includes('MLC')) {\n  console.log('Local inference was used');\n} else {\n  console.log('Cloud API was used');\n}\n```\n\nFor streaming responses, check the first chunk:\n\n```typescript\nconst stream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  if (chunk.model) {\n    console.log('Backend:', chunk.model);\n    break;  // Model name is in the first chunk\n  }\n}\n```\n\n## Next Steps\n\n- Learn about [Streaming](/guides/streaming) for real-time responses\n- Explore [Model Loading](/guides/model-loading) for progressive loading strategies\n- See [Device Capability](/guides/device-capability) to check device compatibility","src/content/docs/guides/hybrid-routing.mdx","9ab5e4febed288bc","guides/structured-output",{"id":388,"data":390,"body":396,"filePath":397,"digest":398,"deferredRender":16},{"title":391,"description":392,"editUrl":16,"head":393,"template":18,"sidebar":394,"pagefind":16,"draft":20},"Structured Output","Get JSON-formatted responses using withJsonOutput().",[],{"hidden":20,"attrs":395},{},"The `withJsonOutput()` helper configures a chat completion request for JSON structured output. It works with both local (MLC) and cloud providers.\n\n## Usage\n\n```ts\nimport { createClient } from '@webllm-io/sdk';\nimport { withJsonOutput } from '@webllm-io/sdk';\n\nconst client = createClient({\n  local: 'auto',\n  cloud: { baseURL: 'https://api.example.com/v1', apiKey: 'sk-...' },\n});\n\nconst res = await client.chat.completions.create(\n  withJsonOutput({\n    messages: [\n      {\n        role: 'user',\n        content: 'Return a JSON object with fields: name (string), age (number)',\n      },\n    ],\n  }),\n);\n\nconst data = JSON.parse(res.choices[0].message.content);\nconsole.log(data.name, data.age);\n```\n\n## How It Works\n\n`withJsonOutput()` sets `response_format: { type: 'json_object' }` on the request object. This is supported by both the MLC engine (local) and OpenAI-compatible cloud APIs.\n\n```ts\nfunction withJsonOutput\u003CT extends ChatCompletionRequest>(req: T): T {\n  return {\n    ...req,\n    response_format: { type: 'json_object' },\n  };\n}\n```\n\n## With Streaming\n\nStructured output works with streaming too. Accumulate chunks and parse the final result:\n\n```ts\nconst stream = client.chat.completions.create(\n  withJsonOutput({\n    messages: [{ role: 'user', content: 'Return JSON: { \"items\": [\"a\",\"b\",\"c\"] }' }],\n    stream: true,\n  }),\n);\n\nlet content = '';\nfor await (const chunk of stream) {\n  content += chunk.choices[0]?.delta?.content ?? '';\n}\n\nconst result = JSON.parse(content);\n```\n\n## Tips\n\n- Always instruct the model to return JSON in the message content  `response_format` alone may not be sufficient for all models.\n- Wrap `JSON.parse()` in a try/catch  models can occasionally produce invalid JSON.\n- Smaller local models (grade C devices) may produce less reliable JSON. Consider using cloud fallback for critical structured output tasks.","src/content/docs/guides/structured-output.mdx","8083d34a66f27904","guides/web-worker",{"id":399,"data":401,"body":407,"filePath":408,"digest":409,"deferredRender":16},{"title":402,"description":403,"editUrl":16,"head":404,"template":18,"sidebar":405,"pagefind":16,"draft":20},"Web Worker","Run local inference in a Web Worker to keep the UI responsive.",[],{"hidden":20,"attrs":406},{},"By default, WebLLM SDK runs local MLC inference in a Web Worker. This prevents model loading and token generation from blocking the main thread.\n\n## Default Behavior\n\nWorker mode is enabled by default  no configuration needed:\n\n```ts\nconst client = createClient({ local: 'auto' });\n// Inference runs in a Web Worker automatically\n```\n\n## Disabling Workers\n\nFor debugging or environments without Worker support:\n\n```ts\nconst client = createClient({\n  local: {\n    model: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',\n    useWebWorker: false, // Run on main thread\n  },\n});\n```\n\n:::caution\nRunning inference on the main thread will freeze the UI during model loading and generation. Only disable workers for debugging.\n:::\n\n## Required HTTP Headers\n\nWebGPU in Workers requires `SharedArrayBuffer`, which needs these HTTP headers:\n\n```\nCross-Origin-Opener-Policy: same-origin\nCross-Origin-Embedder-Policy: require-corp\n```\n\n### Vite Configuration\n\n```ts\n// vite.config.ts\nexport default defineConfig({\n  plugins: [\n    {\n      name: 'configure-response-headers',\n      configureServer(server) {\n        server.middlewares.use((_req, res, next) => {\n          res.setHeader('Cross-Origin-Opener-Policy', 'same-origin');\n          res.setHeader('Cross-Origin-Embedder-Policy', 'require-corp');\n          next();\n        });\n      },\n    },\n  ],\n});\n```\n\n### Next.js Configuration\n\n```js\n// next.config.js\nmodule.exports = {\n  async headers() {\n    return [\n      {\n        source: '/(.*)',\n        headers: [\n          { key: 'Cross-Origin-Opener-Policy', value: 'same-origin' },\n          { key: 'Cross-Origin-Embedder-Policy', value: 'require-corp' },\n        ],\n      },\n    ];\n  },\n};\n```\n\n### Nginx\n\n```nginx\nadd_header Cross-Origin-Opener-Policy \"same-origin\" always;\nadd_header Cross-Origin-Embedder-Policy \"require-corp\" always;\n```\n\n## Worker Entry Point\n\nThe SDK provides a dedicated worker entry at `@webllm-io/sdk/worker`. If you need to customize the worker setup, you can reference this entry point directly.\n\n## Troubleshooting\n\n### \"SharedArrayBuffer is not defined\"\n\nThe COOP/COEP headers are missing or incorrect. Check your server configuration.\n\n### \"Failed to construct Worker\"\n\nEnsure your bundler is configured to handle worker imports. With Vite, workers are supported out of the box. For webpack, you may need `worker-loader` or the built-in worker support in webpack 5.\n\n### Performance\n\nWorker communication adds minimal overhead (typically &lt;1ms per message). The benefits of a non-blocking UI far outweigh this cost, especially during multi-second model loading phases.","src/content/docs/guides/web-worker.mdx","f5e9c9675864308b","guides/streaming",{"id":410,"data":412,"body":418,"filePath":419,"digest":420,"deferredRender":16},{"title":413,"description":414,"editUrl":16,"head":415,"template":18,"sidebar":416,"pagefind":16,"draft":20},"Streaming","Stream responses token-by-token for real-time user experiences",[],{"hidden":20,"attrs":417},{},"Streaming enables real-time token-by-token responses, providing immediate feedback and a better user experience for long-form content generation. WebLLM.io supports streaming for both local and cloud inference with identical APIs.\n\n## Basic Streaming\n\nEnable streaming by setting `stream: true`:\n\n```typescript\nimport { createClient } from '@webllm-io/sdk';\n\nconst client = createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\nconst stream = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Write a short story about AI' }\n  ],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);  // Print tokens as they arrive\n  }\n}\n```\n\n## Chunk Structure\n\nEach chunk follows the OpenAI Chat Completion Chunk format:\n\n```typescript\ninterface ChatCompletionChunk {\n  id: string;\n  object: 'chat.completion.chunk';\n  created: number;\n  model: string;\n  choices: Array\u003C{\n    index: number;\n    delta: {\n      role?: 'assistant';\n      content?: string;\n    };\n    finish_reason: string | null;\n  }>;\n}\n```\n\nKey fields:\n\n- **`chunk.choices[0].delta.content`**  The new token(s) generated\n- **`chunk.choices[0].delta.role`**  Role (only in the first chunk)\n- **`chunk.choices[0].finish_reason`**  Why generation stopped (`'stop'`, `'length'`, or `null`)\n- **`chunk.model`**  Model identifier (useful for detecting which backend responded)\n\n## Building the Full Response\n\nAccumulate chunks to construct the complete message:\n\n```typescript\nconst stream = await client.chat.completions.create({\n  messages: [\n    { role: 'user', content: 'Explain quantum entanglement' }\n  ],\n  stream: true\n});\n\nlet fullContent = '';\n\nfor await (const chunk of stream) {\n  const delta = chunk.choices[0]?.delta?.content;\n  if (delta) {\n    fullContent += delta;\n    console.log('Current content:', fullContent);\n  }\n\n  const finishReason = chunk.choices[0]?.finish_reason;\n  if (finishReason) {\n    console.log('Generation finished:', finishReason);\n  }\n}\n\nconsole.log('Final response:', fullContent);\n```\n\n## Streaming to UI\n\nUpdate a React component in real-time:\n\n```typescript\nimport { useState } from 'react';\nimport { createClient } from '@webllm-io/sdk';\n\nfunction ChatInterface() {\n  const [messages, setMessages] = useState([]);\n  const [isStreaming, setIsStreaming] = useState(false);\n\n  const client = createClient({\n    local: 'auto',\n    cloud: {\n      baseURL: 'https://api.openai.com/v1',\n      apiKey: import.meta.env.VITE_OPENAI_API_KEY,\n      model: 'gpt-4o-mini'\n    }\n  });\n\n  async function sendMessage(content) {\n    setIsStreaming(true);\n\n    // Add user message\n    const userMessage = { role: 'user', content };\n    setMessages(prev => [...prev, userMessage]);\n\n    // Stream assistant response\n    const stream = await client.chat.completions.create({\n      messages: [...messages, userMessage],\n      stream: true\n    });\n\n    let assistantContent = '';\n    setMessages(prev => [...prev, { role: 'assistant', content: '' }]);\n\n    for await (const chunk of stream) {\n      const delta = chunk.choices[0]?.delta?.content;\n      if (delta) {\n        assistantContent += delta;\n        setMessages(prev => {\n          const updated = [...prev];\n          updated[updated.length - 1].content = assistantContent;\n          return updated;\n        });\n      }\n    }\n\n    setIsStreaming(false);\n  }\n\n  return (\n    \u003Cdiv>\n      {messages.map((msg, i) => (\n        \u003Cdiv key={i} className={msg.role}>\n          {msg.content}\n        \u003C/div>\n      ))}\n      \u003Cbutton onClick={() => sendMessage('Hello!')} disabled={isStreaming}>\n        Send\n      \u003C/button>\n    \u003C/div>\n  );\n}\n```\n\n## Detecting the Backend\n\nCheck which backend is streaming responses:\n\n```typescript\nconst stream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  // Model name appears in the first chunk\n  if (chunk.model) {\n    console.log('Streaming from:', chunk.model);\n  }\n\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\nLocal models include `'MLC'` in the name (e.g., `'Llama-3.1-8B-Instruct-q4f16_1-MLC'`), while cloud models use the configured model ID (e.g., `'gpt-4o-mini'`).\n\n## Error Handling\n\nHandle errors during streaming:\n\n```typescript\ntry {\n  const stream = await client.chat.completions.create({\n    messages: [{ role: 'user', content: 'Hello!' }],\n    stream: true\n  });\n\n  for await (const chunk of stream) {\n    const content = chunk.choices[0]?.delta?.content;\n    if (content) {\n      process.stdout.write(content);\n    }\n  }\n} catch (error) {\n  console.error('Streaming failed:', error.message);\n  // Fall back to non-streaming or show error to user\n}\n```\n\n## Aborting Streaming\n\nCancel a streaming request mid-generation:\n\n```typescript\nconst controller = new AbortController();\n\nconst stream = client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Write a long essay' }],\n  stream: true,\n  signal: controller.signal\n});\n\n// Start consuming stream\n(async () => {\n  try {\n    for await (const chunk of stream) {\n      const content = chunk.choices[0]?.delta?.content;\n      if (content) {\n        process.stdout.write(content);\n      }\n    }\n  } catch (error) {\n    if (error.name === 'AbortError') {\n      console.log('\\nStream aborted by user');\n    }\n  }\n})();\n\n// Abort after 2 seconds\nsetTimeout(() => {\n  controller.abort();\n}, 2000);\n```\n\nSee the [Abort Requests](/guides/abort-requests) guide for more details.\n\n## Streaming vs. Non-Streaming\n\n**Use streaming when:**\n- Building conversational UIs (chatbots, assistants)\n- Generating long-form content (essays, articles, code)\n- User experience benefits from seeing incremental progress\n\n**Use non-streaming when:**\n- Processing responses programmatically (extracting JSON, parsing structured data)\n- Responses are short and fast\n- Simpler code is preferred\n\nExample of non-streaming:\n\n```typescript\nconst completion = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'What is 2+2?' }],\n  stream: false  // default\n});\n\nconsole.log(completion.choices[0].message.content);  // '4'\n```\n\n## Streaming with Other Parameters\n\nCombine streaming with other request parameters:\n\n```typescript\nconst stream = await client.chat.completions.create({\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'Explain neural networks' }\n  ],\n  stream: true,\n  temperature: 0.7,\n  max_tokens: 500,\n  provider: 'local'  // Force local streaming\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n## Local vs. Cloud Streaming\n\nStreaming works identically for both backends:\n\n```typescript\n// Hybrid client\nconst client = createClient({\n  local: 'auto',\n  cloud: {\n    baseURL: 'https://api.openai.com/v1',\n    apiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini'\n  }\n});\n\n// Local streaming (if model loaded)\nconst localStream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }],\n  stream: true,\n  provider: 'local'\n});\n\n// Cloud streaming\nconst cloudStream = await client.chat.completions.create({\n  messages: [{ role: 'user', content: 'Hello!' }],\n  stream: true,\n  provider: 'cloud'\n});\n\n// Both streams have identical chunk structure and API\n```\n\n## Next Steps\n\n- Learn about [Abort Requests](/guides/abort-requests) to cancel streaming\n- Explore [Hybrid Routing](/guides/hybrid-routing) for automatic backend selection\n- See [Structured Output](/guides/structured-output) for JSON responses","src/content/docs/guides/streaming.mdx","9ab41f94cae197be"]